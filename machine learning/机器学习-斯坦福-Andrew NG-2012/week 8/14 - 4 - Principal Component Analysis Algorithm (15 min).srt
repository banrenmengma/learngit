
1
00:00:00,034 --> 00:00:01,040
In this video I'd like

2
00:00:01,055 --> 00:00:03,002
to tell you about the principle

3
00:00:03,033 --> 00:00:04,057
components analysis algorithm.

4
00:00:05,059 --> 00:00:06,055
And by the end of this

5
00:00:06,071 --> 00:00:09,019
video you know to implement PCA for yourself.

6
00:00:10,016 --> 00:00:12,053
And use it reduce the dimension of your data.

7
00:00:13,009 --> 00:00:14,068
Before applying PCA, there is

8
00:00:14,080 --> 00:00:17,076
a data pre-processing step which you should always do.

9
00:00:18,051 --> 00:00:20,021
Given the trading sets of the

10
00:00:20,051 --> 00:00:22,028
examples is important to

11
00:00:22,060 --> 00:00:24,007
always perform mean normalization,

12
00:00:25,032 --> 00:00:26,014
and then depending on your data,

13
00:00:26,083 --> 00:00:28,053
maybe perform feature scaling as well.

14
00:00:29,062 --> 00:00:30,094
this is very similar to the

15
00:00:31,064 --> 00:00:33,025
mean normalization and feature scaling

16
00:00:34,007 --> 00:00:36,057
process that we have for supervised learning.

17
00:00:36,090 --> 00:00:38,024
In fact it's exactly the

18
00:00:38,039 --> 00:00:40,015
same procedure except that we're

19
00:00:40,031 --> 00:00:41,078
doing it now to our unlabeled

20
00:00:42,092 --> 00:00:43,067
data, X1 through Xm.

21
00:00:44,017 --> 00:00:45,053
So for mean normalization we

22
00:00:45,071 --> 00:00:47,007
first compute the mean of

23
00:00:47,039 --> 00:00:49,007
each feature and then

24
00:00:49,034 --> 00:00:50,089
we replace each feature, X,

25
00:00:51,014 --> 00:00:52,067
with X minus its mean,

26
00:00:52,081 --> 00:00:54,011
and so this makes each feature

27
00:00:54,052 --> 00:00:57,045
now have exactly zero mean

28
00:00:58,068 --> 00:01:00,068
The different features have very different scales.

29
00:01:01,053 --> 00:01:03,004
So for example, if x1

30
00:01:03,007 --> 00:01:04,006
is the size of a house, and

31
00:01:04,009 --> 00:01:05,039
x2 is the number of bedrooms, to

32
00:01:05,057 --> 00:01:07,037
use our earlier example, we

33
00:01:07,048 --> 00:01:08,068
then also scale each feature

34
00:01:09,012 --> 00:01:10,054
to have a comparable range of values.

35
00:01:10,098 --> 00:01:12,048
And so, similar to what

36
00:01:12,068 --> 00:01:13,085
we had with supervised learning,

37
00:01:14,006 --> 00:01:16,020
we would take x, i substitute

38
00:01:16,068 --> 00:01:17,062
j, that's the j feature

39
00:01:23,025 --> 00:01:25,053
and so we would

40
00:01:25,089 --> 00:01:27,060
subtract of the mean,

41
00:01:28,037 --> 00:01:29,051
now that's what we have on top, and then divide by sj.

42
00:01:29,060 --> 00:01:30,001
Here, sj is some measure of the beta values of feature j.  So, it could be the max minus

43
00:01:30,007 --> 00:01:31,031
min value, or more commonly,

44
00:01:31,089 --> 00:01:33,054
it is the standard deviation of

45
00:01:33,064 --> 00:01:35,051
feature j. Having done

46
00:01:36,023 --> 00:01:39,048
this sort of data pre-processing, here's what the PCA algorithm does.

47
00:01:40,062 --> 00:01:41,062
We saw from the previous video

48
00:01:41,095 --> 00:01:43,004
that what PCA does is, it

49
00:01:43,017 --> 00:01:44,051
tries to find a lower

50
00:01:44,079 --> 00:01:46,007
dimensional sub-space onto which to

51
00:01:46,017 --> 00:01:47,050
project the data, so as

52
00:01:47,065 --> 00:01:49,078
to minimize the squared projection

53
00:01:50,054 --> 00:01:51,065
errors, sum of the

54
00:01:51,073 --> 00:01:53,007
squared projection errors, as the

55
00:01:53,042 --> 00:01:54,079
square of the length of

56
00:01:54,087 --> 00:01:56,079
those blue lines that and so

57
00:01:57,010 --> 00:01:58,051
what we wanted to do specifically

58
00:01:59,020 --> 00:02:02,073
is find a vector, u1, which

59
00:02:03,028 --> 00:02:04,075
specifies that direction or

60
00:02:05,004 --> 00:02:06,062
in the 2D case we want

61
00:02:06,087 --> 00:02:08,075
to find two vectors, u1 and

62
00:02:10,063 --> 00:02:12,097
u2, to define this surface

63
00:02:13,059 --> 00:02:14,061
onto which to project the data.

64
00:02:16,062 --> 00:02:17,091
So, just as a

65
00:02:18,003 --> 00:02:19,015
quick reminder of what reducing

66
00:02:19,072 --> 00:02:20,081
the dimension of the data means,

67
00:02:21,049 --> 00:02:22,043
for this example on the

68
00:02:22,046 --> 00:02:23,056
left we were given

69
00:02:23,068 --> 00:02:26,000
the examples xI, which are in r2.

70
00:02:26,030 --> 00:02:28,038
And what we

71
00:02:28,065 --> 00:02:29,050
like to do is find

72
00:02:29,096 --> 00:02:32,040
a set of numbers zI in

73
00:02:33,000 --> 00:02:34,094
r push to represent our data.

74
00:02:36,000 --> 00:02:37,081
So that's what from reduction from 2D to 1D means.

75
00:02:39,002 --> 00:02:41,044
So specifically by projecting

76
00:02:42,071 --> 00:02:44,008
data onto this red line there.

77
00:02:44,080 --> 00:02:46,031
We need only one number to

78
00:02:46,044 --> 00:02:48,034
specify the position of the points on the line.

79
00:02:48,059 --> 00:02:49,037
So i'm going to call that number

80
00:02:50,069 --> 00:02:51,083
z or z1.

81
00:02:52,002 --> 00:02:54,084
Z here  [xx] real number, so that's like a one dimensional vector.

82
00:02:55,037 --> 00:02:56,065
So z1 just refers to

83
00:02:56,068 --> 00:02:58,008
the first component of this,

84
00:02:58,028 --> 00:03:00,043
you know, one by one matrix, or this one dimensional vector.

85
00:03:01,066 --> 00:03:03,016
And so we need only

86
00:03:03,049 --> 00:03:05,059
one number to specify the position of a point.

87
00:03:06,033 --> 00:03:07,093
So if this example

88
00:03:08,046 --> 00:03:09,050
here was my example

89
00:03:10,061 --> 00:03:13,015
X1, then maybe that gets mapped here.

90
00:03:13,090 --> 00:03:15,044
And if this example was X2

91
00:03:15,068 --> 00:03:17,025
maybe that example gets mapped

92
00:03:17,053 --> 00:03:18,078
And so this point

93
00:03:19,006 --> 00:03:20,040
here will be Z1

94
00:03:20,084 --> 00:03:21,091
and this point here will be

95
00:03:22,008 --> 00:03:24,024
Z2, and similarly we

96
00:03:24,062 --> 00:03:26,040
would have those other points

97
00:03:26,084 --> 00:03:30,022
for These, maybe X3,

98
00:03:30,050 --> 00:03:32,055
X4, X5 get mapped to Z1, Z2, Z3.

99
00:03:34,036 --> 00:03:35,093
So What PCA has

100
00:03:36,005 --> 00:03:36,083
to do is we need to

101
00:03:36,093 --> 00:03:38,091
come up with a way to compute two things.

102
00:03:39,031 --> 00:03:40,071
One is to compute these vectors,

103
00:03:41,083 --> 00:03:44,096
u1, and in this case u1 and u2.

104
00:03:45,022 --> 00:03:46,087
And the other is

105
00:03:47,012 --> 00:03:48,013
how do we compute these numbers,

106
00:03:49,036 --> 00:03:51,019
Z. So on the

107
00:03:51,043 --> 00:03:53,090
example on the left we're reducing the data from 2D to 1D.

108
00:03:55,028 --> 00:03:56,009
In the example on the right,

109
00:03:56,050 --> 00:03:58,009
we would be reducing data from

110
00:03:58,044 --> 00:04:00,059
3 dimensional as in

111
00:04:00,071 --> 00:04:04,084
r3, to zi, which is now two dimensional.

112
00:04:05,038 --> 00:04:07,078
So these z vectors would now be two dimensional.

113
00:04:08,044 --> 00:04:09,059
So it would be z1

114
00:04:10,015 --> 00:04:11,040
z2 like so, and so

115
00:04:11,063 --> 00:04:12,093
we need to give away to compute

116
00:04:13,066 --> 00:04:15,040
these new representations, the z1

117
00:04:15,056 --> 00:04:17,035
and z2 of the data as well.

118
00:04:18,027 --> 00:04:20,035
So how do you compute all of these quantities?

119
00:04:20,051 --> 00:04:21,051
It turns out that a mathematical

120
00:04:22,049 --> 00:04:23,066
derivation, also the mathematical

121
00:04:24,030 --> 00:04:26,001
proof, for what is

122
00:04:26,008 --> 00:04:27,097
the right value U1, U2, Z1,

123
00:04:28,029 --> 00:04:29,048
Z2, and so on.

124
00:04:29,068 --> 00:04:31,023
That mathematical proof is very

125
00:04:31,048 --> 00:04:32,088
complicated and beyond the

126
00:04:32,094 --> 00:04:34,062
scope of the course.

127
00:04:35,027 --> 00:04:37,029
But once you've done  [xx] it

128
00:04:37,058 --> 00:04:38,058
turns out that the procedure

129
00:04:39,035 --> 00:04:40,056
to actually find the value

130
00:04:41,019 --> 00:04:42,020
of u1 that you want

131
00:04:42,094 --> 00:04:43,094
is not that hard, even though

132
00:04:44,018 --> 00:04:45,063
so that the mathematical proof that

133
00:04:45,083 --> 00:04:46,093
this value is the correct

134
00:04:47,025 --> 00:04:48,044
value is someone more

135
00:04:48,069 --> 00:04:49,095
involved and more than i want to get into.

136
00:04:50,087 --> 00:04:52,006
But let me just describe the

137
00:04:52,048 --> 00:04:53,082
specific procedure that you

138
00:04:53,095 --> 00:04:55,025
have to implement in order

139
00:04:55,043 --> 00:04:56,044
to compute all of these

140
00:04:56,056 --> 00:04:57,083
things, the vectors, u1, u2,

141
00:04:58,091 --> 00:05:00,098
the vector z.  Here's the procedure.

142
00:05:02,006 --> 00:05:02,097
Let's say we want to reduce

143
00:05:03,017 --> 00:05:04,022
the data to n dimensions

144
00:05:04,083 --> 00:05:05,075
to k dimension What we're

145
00:05:06,075 --> 00:05:07,063
going to do is first

146
00:05:07,089 --> 00:05:09,039
compute something called the

147
00:05:09,082 --> 00:05:11,013
covariance matrix, and the covariance

148
00:05:11,069 --> 00:05:13,062
matrix is commonly denoted by

149
00:05:13,081 --> 00:05:15,005
this Greek alphabet which is

150
00:05:15,018 --> 00:05:16,087
the capital Greek alphabet sigma.

151
00:05:18,000 --> 00:05:19,020
It's a bit unfortunate that the

152
00:05:19,031 --> 00:05:21,007
Greek alphabet sigma looks exactly

153
00:05:21,075 --> 00:05:22,070
like the summation symbols.

154
00:05:23,020 --> 00:05:24,062
So this is the

155
00:05:24,069 --> 00:05:26,022
Greek alphabet Sigma is used

156
00:05:26,042 --> 00:05:29,054
to denote a matrix and this here is a summation symbol.

157
00:05:30,050 --> 00:05:32,032
So hopefully in these slides

158
00:05:32,068 --> 00:05:34,018
there won't be ambiguity about which

159
00:05:34,041 --> 00:05:36,033
is Sigma Matrix, the

160
00:05:36,051 --> 00:05:37,085
matrix, which is a

161
00:05:38,008 --> 00:05:39,062
summation symbol, and hopefully

162
00:05:39,093 --> 00:05:41,045
it will be clear from context when

163
00:05:41,081 --> 00:05:43,050
I'm using each one.

164
00:05:43,074 --> 00:05:44,079
How do you compute this matrix

165
00:05:45,052 --> 00:05:46,055
let's say we want to

166
00:05:47,013 --> 00:05:47,063
store it in an octave

167
00:05:48,012 --> 00:05:49,097
variable called sigma.

168
00:05:50,083 --> 00:05:51,088
What we need to do is

169
00:05:52,002 --> 00:05:53,066
compute something called the

170
00:05:54,012 --> 00:05:56,018
eigenvectors of the matrix sigma.

171
00:05:57,056 --> 00:05:58,044
And an octave, the way you

172
00:05:58,058 --> 00:05:59,081
do that is you use this

173
00:05:59,097 --> 00:06:01,001
command, u s v equals

174
00:06:01,035 --> 00:06:02,060
s v d of sigma.

175
00:06:03,064 --> 00:06:06,008
SVD, by the way, stands for singular value decomposition.

176
00:06:08,051 --> 00:06:10,058
This is a Much

177
00:06:10,079 --> 00:06:12,066
more advanced single value composition.

178
00:06:14,044 --> 00:06:15,056
It is much more advanced linear

179
00:06:15,080 --> 00:06:16,094
algebra than you actually need

180
00:06:16,094 --> 00:06:18,076
to know but now It turns out

181
00:06:18,094 --> 00:06:20,025
that when sigma is equal

182
00:06:20,048 --> 00:06:21,080
to matrix there is

183
00:06:21,087 --> 00:06:23,042
a few ways to compute these are

184
00:06:23,061 --> 00:06:25,081
high in vectors and If you

185
00:06:25,095 --> 00:06:27,035
are an expert in linear algebra

186
00:06:27,069 --> 00:06:28,061
and if you've heard of high in

187
00:06:28,086 --> 00:06:30,017
vectors before you may know

188
00:06:30,035 --> 00:06:31,066
that there is another octet function

189
00:06:31,099 --> 00:06:33,042
called I, which can

190
00:06:33,051 --> 00:06:35,002
also be used to compute the same thing.

191
00:06:35,094 --> 00:06:36,098
and It turns out that the

192
00:06:37,037 --> 00:06:39,018
SVD function and the

193
00:06:39,029 --> 00:06:40,031
I function it will give

194
00:06:40,037 --> 00:06:42,017
you the same vectors, although SVD

195
00:06:42,083 --> 00:06:44,020
is a little more numerically stable.

196
00:06:44,054 --> 00:06:45,088
So I tend to use SVD, although

197
00:06:46,013 --> 00:06:47,004
I have a few friends that use

198
00:06:47,027 --> 00:06:48,072
the I function to do

199
00:06:48,092 --> 00:06:50,005
this as wellbut when you

200
00:06:50,012 --> 00:06:51,026
apply this to a covariance matrix

201
00:06:51,075 --> 00:06:52,095
sigma it gives you the same thing.

202
00:06:53,087 --> 00:06:55,006
This is because the covariance matrix

203
00:06:55,050 --> 00:06:57,025
always satisfies a mathematical

204
00:06:57,093 --> 00:07:00,056
Property called symmetric positive definite

205
00:07:01,036 --> 00:07:02,013
You really don't need to know

206
00:07:02,027 --> 00:07:03,088
what that means, but the SVD

207
00:07:05,033 --> 00:07:07,008
and I-functions are different functions but

208
00:07:07,039 --> 00:07:08,067
when they are applied to a

209
00:07:08,077 --> 00:07:10,041
covariance matrix which can

210
00:07:10,055 --> 00:07:12,007
be proved to always satisfy this

211
00:07:13,018 --> 00:07:15,022
mathematical property; they'll always give you the same thing.

212
00:07:16,057 --> 00:07:19,018
Okay, that was probably much more linear algebra than you needed to know.

213
00:07:19,025 --> 00:07:22,037
In case none of that made sense, don't worry about it.

214
00:07:22,056 --> 00:07:23,049
All you need to know is that

215
00:07:24,012 --> 00:07:27,083
this system command you

216
00:07:28,013 --> 00:07:29,068
should implement in Octave.

217
00:07:30,007 --> 00:07:30,055
And if you're implementing this in a

218
00:07:30,070 --> 00:07:32,012
different language than Octave or MATLAB,

219
00:07:32,070 --> 00:07:33,079
what you should do is find

220
00:07:34,018 --> 00:07:35,086
the numerical linear algebra library

221
00:07:36,073 --> 00:07:37,095
that can compute the SVD

222
00:07:38,023 --> 00:07:40,045
or singular value decomposition, and

223
00:07:40,097 --> 00:07:42,068
there are many such libraries for

224
00:07:43,056 --> 00:07:45,006
probably all of the major programming languages.

225
00:07:45,030 --> 00:07:46,092
People can use that to

226
00:07:47,005 --> 00:07:48,092
compute the matrices u,

227
00:07:49,019 --> 00:07:52,076
s, and d of the covariance matrix sigma.

228
00:07:53,033 --> 00:07:54,049
So just to fill

229
00:07:54,062 --> 00:07:56,008
in some more details, this covariance

230
00:07:56,066 --> 00:07:58,007
matrix sigma will be

231
00:07:58,025 --> 00:08:01,048
an n by n matrix.

232
00:08:02,025 --> 00:08:03,024
And one way to see that

233
00:08:03,050 --> 00:08:04,022
is if you look at the definition

234
00:08:05,025 --> 00:08:06,027
this is an n by 1

235
00:08:06,066 --> 00:08:08,068
vector and this

236
00:08:08,092 --> 00:08:10,082
here I transpose is

237
00:08:11,000 --> 00:08:13,025
1 by N so the

238
00:08:13,037 --> 00:08:14,048
product of these two things

239
00:08:15,014 --> 00:08:15,080
is going to be an N

240
00:08:16,056 --> 00:08:17,052
by N matrix.

241
00:08:19,010 --> 00:08:22,012
1xN transfers, 1xN, so

242
00:08:22,027 --> 00:08:22,083
there's an NxN matrix and when

243
00:08:22,091 --> 00:08:23,070
we add up all of these you still

244
00:08:23,083 --> 00:08:26,013
have an NxN matrix.

245
00:08:27,060 --> 00:08:29,092
And what the SVD outputs three

246
00:08:30,050 --> 00:08:32,058
matrices, u, s, and

247
00:08:32,083 --> 00:08:35,007
v.  The thing you really need out of the SVD is the u matrix.

248
00:08:36,023 --> 00:08:40,015
The u matrix will also be a NxN matrix.

249
00:08:41,050 --> 00:08:42,027
And if we look at the

250
00:08:42,035 --> 00:08:43,025
columns of the U

251
00:08:43,048 --> 00:08:45,033
matrix it turns

252
00:08:45,062 --> 00:08:47,021
out that the columns

253
00:08:48,057 --> 00:08:50,017
of the U matrix will be

254
00:08:50,035 --> 00:08:53,086
exactly those vectors, u1,

255
00:08:54,025 --> 00:08:56,028
u2 and so on.

256
00:08:57,063 --> 00:08:59,033
So u, will be matrix.

257
00:09:00,090 --> 00:09:01,083
And if we want to reduce

258
00:09:02,023 --> 00:09:03,020
the data from n dimensions

259
00:09:03,079 --> 00:09:05,037
down to k dimensions, then what

260
00:09:05,049 --> 00:09:07,095
we need to do is take the first k vectors.

261
00:09:09,079 --> 00:09:12,066
that gives us u1 up

262
00:09:12,086 --> 00:09:14,070
to uK which gives

263
00:09:14,077 --> 00:09:16,092
us the K direction onto which

264
00:09:17,020 --> 00:09:19,076
we want to project the data.

265
00:09:20,009 --> 00:09:21,063
the rest of the procedure from

266
00:09:22,040 --> 00:09:24,016
this SVD numerical linear

267
00:09:24,049 --> 00:09:25,058
algebra routine we get this

268
00:09:25,084 --> 00:09:27,013
matrix u.  We'll call

269
00:09:27,052 --> 00:09:29,008
these columns u1-uN.

270
00:09:30,058 --> 00:09:31,062
So, just to wrap up the

271
00:09:31,083 --> 00:09:32,051
description of the rest of

272
00:09:32,053 --> 00:09:34,054
the procedure, from the SVD

273
00:09:35,032 --> 00:09:36,094
numerical linear algebra routine we

274
00:09:37,024 --> 00:09:38,064
get these matrices u, s,

275
00:09:38,083 --> 00:09:41,032
and d.  we're going

276
00:09:41,089 --> 00:09:44,046
to use the first K columns

277
00:09:45,004 --> 00:09:46,030
of this matrix to get u1-uK.

278
00:09:48,071 --> 00:09:49,046
Now the other thing we need

279
00:09:49,070 --> 00:09:53,073
to is take my original

280
00:09:54,011 --> 00:09:55,042
data set, X which is

281
00:09:55,062 --> 00:09:57,008
an RN And find a

282
00:09:57,025 --> 00:09:59,021
lower dimensional representation Z, which

283
00:09:59,041 --> 00:10:01,027
is a R K for this data.

284
00:10:01,057 --> 00:10:02,079
So the way we're

285
00:10:02,089 --> 00:10:03,092
going to do that is

286
00:10:04,017 --> 00:10:06,069
take the first K Columns of the U matrix.

287
00:10:08,033 --> 00:10:09,078
Construct this matrix.

288
00:10:11,005 --> 00:10:13,003
Stack up U1, U2 and

289
00:10:14,016 --> 00:10:16,069
so on up to U K in columns.

290
00:10:17,035 --> 00:10:19,012
It's really basically taking, you know,

291
00:10:19,027 --> 00:10:20,035
this part of the matrix, the

292
00:10:20,052 --> 00:10:22,025
first K columns of this matrix.

293
00:10:23,041 --> 00:10:25,037
And so this is

294
00:10:25,060 --> 00:10:26,091
going to be an N

295
00:10:27,020 --> 00:10:28,058
by K matrix.

296
00:10:29,050 --> 00:10:30,069
I'm going to give this matrix a name.

297
00:10:30,087 --> 00:10:32,020
I'm going to call this matrix

298
00:10:32,092 --> 00:10:35,075
U, subscript "reduce," sort

299
00:10:36,009 --> 00:10:38,062
of a reduced version of the U matrix maybe.

300
00:10:39,013 --> 00:10:41,025
I'm going to use it to reduce the dimension of my data.

301
00:10:43,003 --> 00:10:43,095
And the way I'm going to compute Z is going

302
00:10:44,025 --> 00:10:45,096
to let Z be equal to this

303
00:10:46,022 --> 00:10:49,057
U reduce matrix transpose times

304
00:10:50,000 --> 00:10:52,002
X. Or alternatively, you know,

305
00:10:52,050 --> 00:10:53,086
to write down what this transpose means.

306
00:10:54,062 --> 00:10:55,090
When I take this transpose of

307
00:10:56,000 --> 00:10:57,091
this U matrix, what I'm

308
00:10:58,000 --> 00:11:00,067
going to end up with is these vectors now in rows.

309
00:11:00,095 --> 00:11:04,053
I have U1 transpose down to UK transpose.

310
00:11:07,005 --> 00:11:08,086
Then take that times X,

311
00:11:09,070 --> 00:11:10,074
and that's how I get

312
00:11:10,091 --> 00:11:12,066
my vector Z. Just to

313
00:11:12,074 --> 00:11:14,027
make sure that these dimensions make sense,

314
00:11:15,037 --> 00:11:16,037
this matrix here is going

315
00:11:16,055 --> 00:11:17,045
to be k by n

316
00:11:18,026 --> 00:11:19,035
and x here is going

317
00:11:19,041 --> 00:11:20,052
to be n by 1

318
00:11:20,075 --> 00:11:21,080
and so the product

319
00:11:22,032 --> 00:11:24,033
here will be k by 1.

320
00:11:24,082 --> 00:11:27,091
And so z is k

321
00:11:28,078 --> 00:11:29,080
dimensional, is a k

322
00:11:30,000 --> 00:11:31,023
dimensional vector, which is exactly

323
00:11:32,000 --> 00:11:33,017
what we wanted.

324
00:11:33,054 --> 00:11:34,063
And of course these x's here right, can

325
00:11:34,088 --> 00:11:36,000
be Examples in our

326
00:11:36,010 --> 00:11:36,097
training set can be examples

327
00:11:37,053 --> 00:11:38,075
in our cross validation set, can be

328
00:11:38,098 --> 00:11:40,033
examples in our test set, and

329
00:11:40,050 --> 00:11:41,059
for example if you know,

330
00:11:41,092 --> 00:11:43,083
I wanted to take training example i,

331
00:11:44,025 --> 00:11:45,090
I can write this as xi

332
00:11:47,026 --> 00:11:48,042
XI and that's what will

333
00:11:48,050 --> 00:11:50,008
give me ZI over there.

334
00:11:50,094 --> 00:11:53,013
So, to summarize, here's the

335
00:11:53,046 --> 00:11:54,082
PCA algorithm on one slide.

336
00:11:56,025 --> 00:11:58,020
After mean normalization, to ensure

337
00:11:58,041 --> 00:11:59,023
that every feature is zero mean

338
00:11:59,061 --> 00:12:01,041
and optional feature scaling whichYou

339
00:12:02,027 --> 00:12:03,077
really should do feature scaling if

340
00:12:03,088 --> 00:12:05,082
your features take on very different ranges of values.

341
00:12:06,062 --> 00:12:08,061
After this pre-processing we compute

342
00:12:09,012 --> 00:12:12,000
the carrier matrix Sigma like

343
00:12:12,024 --> 00:12:14,007
so by the

344
00:12:14,021 --> 00:12:16,034
way if your data is

345
00:12:16,061 --> 00:12:17,077
given as a matrix

346
00:12:18,002 --> 00:12:18,096
like hits if you have your

347
00:12:19,023 --> 00:12:22,058
data Given in rows like this.

348
00:12:22,077 --> 00:12:24,037
If you have a matrix X

349
00:12:24,096 --> 00:12:26,019
which is your time trading sets

350
00:12:27,002 --> 00:12:28,083
written in rows where x1

351
00:12:29,021 --> 00:12:30,039
transpose down to x1 transpose,

352
00:12:31,052 --> 00:12:32,070
this covariance matrix sigma actually has

353
00:12:33,001 --> 00:12:36,003
a nice vectorizing implementation.

354
00:12:37,038 --> 00:12:38,098
You can implement in octave,

355
00:12:39,044 --> 00:12:41,012
you can even run sigma equals 1

356
00:12:41,066 --> 00:12:45,025
over m, times x,

357
00:12:45,054 --> 00:12:46,044
which is this matrix up here,

358
00:12:47,025 --> 00:12:50,076
transpose times x and

359
00:12:50,098 --> 00:12:53,032
this simple expression, that's

360
00:12:53,057 --> 00:12:55,007
the vectorize implementation of how

361
00:12:55,022 --> 00:12:56,090
to compute the matrix sigma.

362
00:12:58,001 --> 00:12:59,001
I'm not going to prove that today.

363
00:12:59,015 --> 00:13:00,060
This is the correct vectorization whether you

364
00:13:00,074 --> 00:13:02,046
want, you can either numerically test

365
00:13:02,087 --> 00:13:03,089
this on yourself by trying out an

366
00:13:03,098 --> 00:13:05,010
octave and making sure that

367
00:13:05,084 --> 00:13:06,088
both this and this implementations

368
00:13:06,091 --> 00:13:10,004
give the same answers or you Can try to prove it yourself mathematically.

369
00:13:11,025 --> 00:13:12,033
Either way but this is the

370
00:13:12,042 --> 00:13:14,058
correct vectorizing implementation, without compusingnext

371
00:13:16,048 --> 00:13:17,057
we can apply the SVD

372
00:13:17,091 --> 00:13:19,004
routine to get u, s,

373
00:13:19,025 --> 00:13:20,084
and d. And then we

374
00:13:21,010 --> 00:13:22,072
grab the first k

375
00:13:23,004 --> 00:13:24,045
columns of the u

376
00:13:24,065 --> 00:13:26,054
matrix you reduce and

377
00:13:26,064 --> 00:13:28,053
finally this defines how

378
00:13:28,074 --> 00:13:29,098
we go from a feature

379
00:13:30,028 --> 00:13:31,060
vector x to this

380
00:13:31,085 --> 00:13:34,034
reduce dimension representation z. And

381
00:13:34,053 --> 00:13:35,075
similar to k Means

382
00:13:36,009 --> 00:13:37,086
if you're apply PCA, they way

383
00:13:38,002 --> 00:13:40,029
you'd apply this is with vectors X and RN.

384
00:13:41,010 --> 00:13:43,099
So, this is not done with X-0 1.

385
00:13:44,020 --> 00:13:46,008
So that was

386
00:13:46,099 --> 00:13:48,067
the PCA algorithm.

387
00:13:50,012 --> 00:13:51,037
One thing I didn't do is

388
00:13:51,059 --> 00:13:53,019
give a mathematical proof that

389
00:13:53,051 --> 00:13:54,060
this There it actually give

390
00:13:54,097 --> 00:13:56,055
the projection of the data onto

391
00:13:57,023 --> 00:13:58,073
the K dimensional subspace onto the

392
00:13:58,087 --> 00:14:00,062
K dimensional surface that actually

393
00:14:02,016 --> 00:14:04,079
minimizes the square projection error Proof

394
00:14:05,011 --> 00:14:07,016
of that is beyond the scope of this course.

395
00:14:07,070 --> 00:14:09,011
Fortunately the PCA algorithm

396
00:14:09,047 --> 00:14:10,094
can be implemented in not

397
00:14:11,032 --> 00:14:12,050
too many lines of code.

398
00:14:13,019 --> 00:14:14,050
and if you implement this in

399
00:14:14,063 --> 00:14:16,012
octave or algorithm, you

400
00:14:16,051 --> 00:14:17,059
actually get a very effective

401
00:14:18,011 --> 00:14:19,071
dimensionality reduction algorithm.

402
00:14:22,042 --> 00:14:23,085
So, that was the PCA algorithm.

403
00:14:25,000 --> 00:14:26,028
One thing I didn't do was

404
00:14:26,084 --> 00:14:28,041
give a mathematical proof that

405
00:14:29,016 --> 00:14:30,036
the U1 and U2 and so

406
00:14:30,072 --> 00:14:31,062
on and the Z and so

407
00:14:31,076 --> 00:14:32,083
on you get out of this

408
00:14:32,098 --> 00:14:34,033
procedure is really the

409
00:14:34,067 --> 00:14:35,087
choices that would minimize

410
00:14:36,050 --> 00:14:37,079
these squared projection error.

411
00:14:38,013 --> 00:14:39,035
Right, remember we said What

412
00:14:39,061 --> 00:14:40,065
PCA tries to do is try

413
00:14:40,096 --> 00:14:42,016
to find a surface or line

414
00:14:42,057 --> 00:14:43,069
onto which to project the data

415
00:14:44,027 --> 00:14:46,034
so as to minimize to square projection error.

416
00:14:46,070 --> 00:14:48,061
So I didn't prove that this

417
00:14:49,013 --> 00:14:50,067
that, and the mathematical proof

418
00:14:50,097 --> 00:14:52,051
of that is beyond the scope of this course.

419
00:14:53,016 --> 00:14:55,054
But fortunately the PCA algorithm can

420
00:14:55,073 --> 00:14:58,088
be implemented in not too many lines of octave code.

421
00:14:59,035 --> 00:15:00,074
And if you implement this,

422
00:15:01,042 --> 00:15:02,055
this is actually what will

423
00:15:02,076 --> 00:15:03,073
work, or this will work well,

424
00:15:04,071 --> 00:15:05,094
and if you implement this algorithm,

425
00:15:06,050 --> 00:15:09,022
you get a very effective dimensionality reduction algorithm.

426
00:15:09,077 --> 00:15:10,064
That does do the right thing

427
00:15:11,004 --> 00:15:13,046
of minimizing this square projection error.
