
1
00:00:00,008 --> 00:00:01,013
In this video, I'd like

2
00:00:01,037 --> 00:00:03,012
to start adapting support vector

3
00:00:03,039 --> 00:00:06,028
machines in order to develop complex nonlinear classifiers.

4
00:00:07,062 --> 00:00:10,041
The main technique for doing that is something called kernels.

5
00:00:11,073 --> 00:00:13,068
Let's see what this kernels are and how to use them.

6
00:00:15,085 --> 00:00:16,092
If you have a training set that

7
00:00:17,003 --> 00:00:18,026
looks like this, and you

8
00:00:18,039 --> 00:00:20,000
want to find a

9
00:00:20,014 --> 00:00:21,067
nonlinear decision boundary to distinguish

10
00:00:22,026 --> 00:00:23,094
the positive and negative examples, maybe

11
00:00:24,035 --> 00:00:25,089
a decision boundary that looks like that.

12
00:00:27,003 --> 00:00:27,094
One way to do so is

13
00:00:28,023 --> 00:00:29,076
to come up with a set

14
00:00:29,096 --> 00:00:32,017
of complex polynomial features, right? So, set of

15
00:00:32,034 --> 00:00:33,042
features that looks like this,

16
00:00:34,014 --> 00:00:34,099
so that you end up

17
00:00:35,014 --> 00:00:37,011
with a hypothesis X that

18
00:00:38,004 --> 00:00:40,038
predicts 1 if you know

19
00:00:40,057 --> 00:00:41,078
that theta 0 and plus theta 1 X1

20
00:00:41,085 --> 00:00:45,000
plus dot dot dot all those polynomial features is

21
00:00:45,017 --> 00:00:47,040
greater than 0, and

22
00:00:47,053 --> 00:00:49,017
predict 0, otherwise.

23
00:00:51,007 --> 00:00:52,075
And another way

24
00:00:52,097 --> 00:00:54,032
of writing this, to introduce

25
00:00:54,084 --> 00:00:56,024
a level of new notation that

26
00:00:56,050 --> 00:00:57,085
I'll use later, is that

27
00:00:58,020 --> 00:00:59,036
we can think of a hypothesis

28
00:00:59,072 --> 00:01:01,060
as computing a decision boundary

29
00:01:02,011 --> 00:01:03,038
using this. So, theta

30
00:01:03,082 --> 00:01:04,087
0 plus theta 1 f1 plus

31
00:01:05,006 --> 00:01:06,012
theta 2, f2 plus theta

32
00:01:06,060 --> 00:01:08,073
3, f3 plus and so on.

33
00:01:09,059 --> 00:01:12,079
Where I'm going to

34
00:01:13,004 --> 00:01:14,006
use this new denotation

35
00:01:14,073 --> 00:01:15,093
f1, f2, f3 and so

36
00:01:16,026 --> 00:01:17,060
on to denote these new sort of features

37
00:01:19,034 --> 00:01:20,062
that I'm computing, so f1 is

38
00:01:21,037 --> 00:01:24,025
just X1, f2 is equal

39
00:01:24,059 --> 00:01:27,006
to X2, f3 is

40
00:01:27,014 --> 00:01:28,056
equal to this one

41
00:01:28,076 --> 00:01:29,079
here. So, X1X2. So,

42
00:01:29,090 --> 00:01:32,020
f4 is equal to

43
00:01:33,084 --> 00:01:35,059
X1 squared where f5 is

44
00:01:35,068 --> 00:01:37,073
to be x2 squared and so

45
00:01:38,051 --> 00:01:39,078
on and we seen previously that

46
00:01:40,034 --> 00:01:41,018
coming up with these high

47
00:01:41,037 --> 00:01:42,087
order polynomials is one

48
00:01:43,010 --> 00:01:44,039
way to come up with lots more features,

49
00:01:45,046 --> 00:01:47,006
the question is, is

50
00:01:47,025 --> 00:01:48,059
there a different choice of

51
00:01:48,067 --> 00:01:51,034
features or is there better sort of features than this high order

52
00:01:51,068 --> 00:01:53,051
polynomials because you know

53
00:01:53,082 --> 00:01:54,081
it's not clear that this high

54
00:01:55,012 --> 00:01:56,034
order polynomial is what we want,

55
00:01:56,085 --> 00:01:57,092
and what we talked about

56
00:01:58,017 --> 00:01:59,056
computer vision talk about when

57
00:01:59,078 --> 00:02:01,093
the input is an image with lots of pixels.

58
00:02:02,054 --> 00:02:04,067
We also saw how using high order polynomials

59
00:02:05,014 --> 00:02:06,035
becomes very computationally

60
00:02:07,031 --> 00:02:08,027
expensive because there are

61
00:02:08,028 --> 00:02:09,083
a lot of these higher order polynomial terms.

62
00:02:11,024 --> 00:02:12,028
So, is there a different or

63
00:02:12,043 --> 00:02:13,015
a better choice of the features

64
00:02:14,011 --> 00:02:15,009
that we can use to plug

65
00:02:15,040 --> 00:02:16,077
into this sort of

66
00:02:17,050 --> 00:02:19,019
hypothesis form.

67
00:02:19,041 --> 00:02:20,046
So, here is one idea for how to

68
00:02:20,058 --> 00:02:23,058
define new features f1, f2, f3.

69
00:02:24,096 --> 00:02:25,093
On this line I am

70
00:02:26,009 --> 00:02:27,059
going to define only three new

71
00:02:27,088 --> 00:02:28,077
features, but for real problems

72
00:02:29,050 --> 00:02:30,065
we can get to define a much larger number.

73
00:02:31,006 --> 00:02:32,006
But here's what I'm going to do

74
00:02:32,025 --> 00:02:33,040
in this phase

75
00:02:33,063 --> 00:02:34,097
of features X1, X2, and

76
00:02:35,040 --> 00:02:36,052
I'm going to leave X0

77
00:02:36,071 --> 00:02:37,080
out of this, the

78
00:02:38,006 --> 00:02:39,022
interceptor X0, but

79
00:02:39,033 --> 00:02:40,031
in this phase X1 X2, I'm going to just,

80
00:02:41,013 --> 00:02:42,034


81
00:02:42,055 --> 00:02:43,056
you know, manually pick a few points, and then

82
00:02:43,075 --> 00:02:45,021
call these points l1, we

83
00:02:45,044 --> 00:02:46,071
are going to pick

84
00:02:46,081 --> 00:02:49,056
a different point, let's call

85
00:02:50,008 --> 00:02:51,038
that l2 and let's pick

86
00:02:51,071 --> 00:02:52,087
the third one and call

87
00:02:53,016 --> 00:02:55,080
this one l3, and for

88
00:02:55,090 --> 00:02:56,083
now let's just say that I'm

89
00:02:56,093 --> 00:02:59,021
going to choose these three points manually.

90
00:02:59,087 --> 00:03:02,086
I'm going to call these three points line ups, so line up one, two, three.

91
00:03:03,071 --> 00:03:04,062
What I'm going to do is

92
00:03:04,078 --> 00:03:07,018
define my new features as follows, given

93
00:03:07,050 --> 00:03:10,006
an example X, let me

94
00:03:10,016 --> 00:03:13,012
define my first feature f1

95
00:03:13,033 --> 00:03:16,000
to be some

96
00:03:16,025 --> 00:03:18,096
measure of the similarity between

97
00:03:19,033 --> 00:03:21,046
my training example X and

98
00:03:21,068 --> 00:03:26,027
my first landmark and

99
00:03:26,052 --> 00:03:27,084
this specific formula that I'm

100
00:03:27,094 --> 00:03:29,059
going to use to measure similarity is

101
00:03:30,015 --> 00:03:31,083
going to be this is E to

102
00:03:31,093 --> 00:03:34,021
the minus the length of

103
00:03:34,046 --> 00:03:37,087
X minus l1, squared, divided

104
00:03:38,031 --> 00:03:39,061
by two sigma squared.

105
00:03:40,072 --> 00:03:41,063
So, depending on whether or not

106
00:03:41,078 --> 00:03:43,041
you watched the previous optional video,

107
00:03:44,038 --> 00:03:48,013
this notation, you know, this is

108
00:03:48,046 --> 00:03:49,034
the length of the vector

109
00:03:49,068 --> 00:03:51,025
W. And so, this thing

110
00:03:51,046 --> 00:03:53,075
here, this X

111
00:03:54,002 --> 00:03:55,099
minus l1, this is

112
00:03:56,009 --> 00:03:57,043
actually just the euclidean distance

113
00:03:58,061 --> 00:03:59,094
squared, is the euclidean

114
00:04:00,040 --> 00:04:03,024
distance between the point x and the landmark l1.

115
00:04:03,053 --> 00:04:04,061
We will see more about this later.

116
00:04:06,043 --> 00:04:07,099
But that's my first feature, and

117
00:04:08,012 --> 00:04:09,061
my second feature f2 is

118
00:04:09,075 --> 00:04:11,075
going to be, you know,

119
00:04:12,037 --> 00:04:14,003
similarity function that measures

120
00:04:14,040 --> 00:04:17,031
how similar X is to l2 and the game is going to be defined as

121
00:04:17,037 --> 00:04:19,036
the following function.

122
00:04:19,060 --> 00:04:21,019


123
00:04:21,036 --> 00:04:24,025


124
00:04:25,097 --> 00:04:27,031
This is E to the minus of the square of the euclidean distance

125
00:04:28,014 --> 00:04:29,005
between X and the second

126
00:04:29,081 --> 00:04:31,031
landmark, that is what the enumerator is and

127
00:04:31,050 --> 00:04:32,066
then divided by 2 sigma squared

128
00:04:33,051 --> 00:04:35,027
and similarly f3 is, you know,

129
00:04:35,085 --> 00:04:39,048
similarity between X

130
00:04:39,083 --> 00:04:41,086
and l3, which is

131
00:04:41,098 --> 00:04:44,050
equal to, again, similar formula.

132
00:04:46,055 --> 00:04:48,006
And what this similarity

133
00:04:48,082 --> 00:04:50,043
function is, the mathematical term

134
00:04:50,073 --> 00:04:52,002
for this, is that this is

135
00:04:52,016 --> 00:04:54,038
going to be a kernel function.

136
00:04:55,033 --> 00:04:56,081
And the specific kernel I'm using

137
00:04:57,013 --> 00:04:59,056
here, this is actually called a Gaussian kernel.

138
00:05:00,062 --> 00:05:01,092
And so this formula, this particular

139
00:05:02,050 --> 00:05:04,099
choice of similarity function is called a Gaussian kernel.

140
00:05:05,076 --> 00:05:07,022
But the way the terminology goes is that, you know, in

141
00:05:07,036 --> 00:05:09,011
the abstract these different

142
00:05:09,060 --> 00:05:11,026
similarity functions are called kernels and

143
00:05:11,060 --> 00:05:12,067
we can have different similarity functions

144
00:05:13,075 --> 00:05:16,041
and the specific example I'm giving here is called the Gaussian kernel.

145
00:05:17,011 --> 00:05:18,039
We'll see other examples of other kernels.

146
00:05:18,083 --> 00:05:21,010
But for now just think of these as similarity functions.

147
00:05:22,047 --> 00:05:24,010
And so, instead of writing similarity between

148
00:05:24,050 --> 00:05:26,026
X and l, sometimes we

149
00:05:26,048 --> 00:05:28,037
also write this a kernel denoted

150
00:05:29,006 --> 00:05:32,036
you know, lower case k between x and one of my landmarks all right.

151
00:05:34,012 --> 00:05:36,012
So let's see what a

152
00:05:36,064 --> 00:05:38,048
criminals actually do and

153
00:05:38,081 --> 00:05:40,063
why these sorts of similarity

154
00:05:41,027 --> 00:05:44,054
functions, why these expressions might make sense.

155
00:05:46,068 --> 00:05:48,001
So let's take my first landmark. My

156
00:05:48,032 --> 00:05:49,023
landmark l1, which is

157
00:05:49,035 --> 00:05:51,037
one of those points I chose on my figure just now.

158
00:05:53,000 --> 00:05:54,016
So the similarity of the kernel between x and l1 is given by this expression.

159
00:05:57,052 --> 00:05:58,060
Just to make sure, you know, we

160
00:05:58,068 --> 00:05:59,060
are on the same page about what

161
00:05:59,077 --> 00:06:01,086
the numerator term is, the

162
00:06:01,095 --> 00:06:03,013
numerator can also be

163
00:06:03,032 --> 00:06:04,062
written as a sum from

164
00:06:04,087 --> 00:06:06,047
J equals 1 through N on sort of the distance.

165
00:06:07,000 --> 00:06:08,069
So this is the component wise distance

166
00:06:09,026 --> 00:06:10,089
between the vector X and

167
00:06:11,006 --> 00:06:12,005
the vector l. And again

168
00:06:12,037 --> 00:06:14,045
for the purpose of these

169
00:06:14,072 --> 00:06:16,018
slides I'm ignoring X0.

170
00:06:16,068 --> 00:06:17,091
So just ignoring the intercept

171
00:06:18,022 --> 00:06:19,095
term X0, which is always equal to 1.

172
00:06:21,043 --> 00:06:22,047
So, you know, this is

173
00:06:22,062 --> 00:06:25,077
how you compute the kernel with similarity between X and a landmark.

174
00:06:27,026 --> 00:06:28,019
So let's see what this function does.

175
00:06:29,011 --> 00:06:31,087
Suppose X is close to one of the landmarks.

176
00:06:33,031 --> 00:06:34,091
Then this euclidean distance

177
00:06:35,036 --> 00:06:36,068
formula and the numerator will

178
00:06:36,099 --> 00:06:38,076
be close to 0, right.

179
00:06:38,088 --> 00:06:40,006
So, that is this term

180
00:06:40,057 --> 00:06:41,087
here, the distance was great,

181
00:06:42,017 --> 00:06:43,012
the distance using X and 0

182
00:06:43,024 --> 00:06:45,012
will be close to zero, and so

183
00:06:46,038 --> 00:06:47,043
f1, this is a simple

184
00:06:47,070 --> 00:06:50,010
feature, will be approximately E

185
00:06:50,029 --> 00:06:52,075
to the minus 0 and

186
00:06:52,080 --> 00:06:54,064
then the numerator squared over 2 is equal to squared

187
00:06:55,064 --> 00:06:56,067
so that E to the

188
00:06:56,076 --> 00:06:58,006
0, E to minus 0,

189
00:06:58,037 --> 00:06:59,081
E to 0 is going to be close to one.

190
00:07:01,063 --> 00:07:03,048
And I'll put the approximation symbol here

191
00:07:03,069 --> 00:07:05,043
because the distance may

192
00:07:05,052 --> 00:07:06,093
not be exactly 0, but

193
00:07:07,012 --> 00:07:08,004
if X is closer to landmark

194
00:07:08,033 --> 00:07:09,018
this term will be close

195
00:07:09,043 --> 00:07:12,006
to 0 and so f1 would be close 1.

196
00:07:13,039 --> 00:07:15,022
Conversely, if X is

197
00:07:15,051 --> 00:07:17,035
far from 01 then this

198
00:07:17,055 --> 00:07:18,093
first feature f1 will

199
00:07:19,081 --> 00:07:21,018
be E to the minus

200
00:07:21,054 --> 00:07:24,004
of some large number squared,

201
00:07:24,095 --> 00:07:25,098
divided divided by two sigma

202
00:07:26,025 --> 00:07:27,068
squared and E to

203
00:07:27,081 --> 00:07:28,080
the minus of a large number

204
00:07:29,062 --> 00:07:31,044
is going to be close to 0.

205
00:07:33,031 --> 00:07:34,061
So what these

206
00:07:34,075 --> 00:07:36,007
features do is they measure how

207
00:07:36,029 --> 00:07:37,050
similar X is from one

208
00:07:37,067 --> 00:07:39,016
of your landmarks and the feature

209
00:07:39,052 --> 00:07:40,029
f is going to be close

210
00:07:40,054 --> 00:07:42,036
to one when X is

211
00:07:42,054 --> 00:07:43,081
close to your landmark and is

212
00:07:44,001 --> 00:07:45,031
going to be 0 or close

213
00:07:45,037 --> 00:07:46,051
to zero when X is

214
00:07:46,079 --> 00:07:48,085
far from your landmark.

215
00:07:49,031 --> 00:07:49,098
Each of these landmarks.

216
00:07:50,058 --> 00:07:51,062
On the previous line, I drew

217
00:07:52,025 --> 00:07:54,025
three landmarks, l1, l2,l3.

218
00:07:56,018 --> 00:08:00,002
Each of these landmarks, defines a new feature

219
00:08:00,066 --> 00:08:02,026
f1, f2 and f3.

220
00:08:02,068 --> 00:08:03,066
That is, given the the

221
00:08:03,070 --> 00:08:05,016
training example X, we can

222
00:08:05,037 --> 00:08:06,075
now compute three new

223
00:08:06,093 --> 00:08:08,072
features: f1, f2, and

224
00:08:09,051 --> 00:08:11,000
f3, given, you know, the three

225
00:08:11,033 --> 00:08:13,052
landmarks that I wrote just now.

226
00:08:13,075 --> 00:08:15,002
But first, let's look

227
00:08:15,024 --> 00:08:16,044
at this exponentiation function, let's look

228
00:08:16,070 --> 00:08:18,018
at this similarity function and plot

229
00:08:18,056 --> 00:08:20,079
in some figures and just, you know, understand

230
00:08:21,023 --> 00:08:22,045
better what this really looks like.

231
00:08:23,050 --> 00:08:26,031
For this example, let's say I have two features X1 and X2.

232
00:08:26,056 --> 00:08:27,043
And let's say my first

233
00:08:27,081 --> 00:08:29,029
landmark, l1 is at

234
00:08:29,051 --> 00:08:32,054
a location, 3 5. So

235
00:08:33,064 --> 00:08:35,075
and let's say I set sigma squared equals one for now.

236
00:08:36,050 --> 00:08:37,054
If I plot what this feature

237
00:08:37,088 --> 00:08:40,041
looks like, what I get is this figure.

238
00:08:41,021 --> 00:08:42,050
So the vertical axis, the height

239
00:08:42,075 --> 00:08:44,002
of the surface is the value

240
00:08:45,024 --> 00:08:46,027
of f1 and down here

241
00:08:46,062 --> 00:08:48,049
on the horizontal axis are, if

242
00:08:48,071 --> 00:08:50,058
I have some training example, and there

243
00:08:51,065 --> 00:08:53,004
is x1 and there is x2.

244
00:08:53,032 --> 00:08:54,094
Given a certain training example, the

245
00:08:55,012 --> 00:08:56,088
training example here which shows

246
00:08:56,098 --> 00:08:58,013
the value of x1 and x2

247
00:08:58,013 --> 00:08:59,038
at a height above the surface,

248
00:08:59,095 --> 00:09:02,022
shows the corresponding value of

249
00:09:02,040 --> 00:09:03,083
f1 and down below this is

250
00:09:03,096 --> 00:09:04,088
the same figure I had showed,

251
00:09:05,003 --> 00:09:06,060
using a quantifiable plot, with

252
00:09:06,080 --> 00:09:08,032
x1 on horizontal

253
00:09:09,009 --> 00:09:10,034
axis, x2 on horizontal

254
00:09:10,082 --> 00:09:12,050
axis and so, this

255
00:09:12,082 --> 00:09:13,070
figure on the bottom is just

256
00:09:13,094 --> 00:09:15,044
a contour plot of the 3D surface.

257
00:09:16,053 --> 00:09:17,079
You notice that when

258
00:09:18,002 --> 00:09:19,053
X is equal to

259
00:09:19,082 --> 00:09:24,013
3 5 exactly, then we

260
00:09:24,037 --> 00:09:25,067
the f1 takes on the

261
00:09:25,075 --> 00:09:26,099
value 1, because that's at

262
00:09:27,016 --> 00:09:29,039
the maximum and X

263
00:09:29,086 --> 00:09:31,014
moves away as X goes

264
00:09:31,067 --> 00:09:33,064
further away then this

265
00:09:33,086 --> 00:09:35,026
feature takes on values

266
00:09:36,046 --> 00:09:37,015
that are close to 0.

267
00:09:38,075 --> 00:09:40,012
And so, this is really a feature,

268
00:09:40,039 --> 00:09:42,010
f1 measures, you know, how

269
00:09:42,039 --> 00:09:43,067
close X is to the first

270
00:09:44,003 --> 00:09:46,004
landmark and if

271
00:09:46,051 --> 00:09:47,061
varies between 0 and one

272
00:09:47,078 --> 00:09:48,094
depending on how close X

273
00:09:49,015 --> 00:09:50,064
is to the first landmark l1.

274
00:09:52,036 --> 00:09:53,071
Now the other was due on

275
00:09:53,091 --> 00:09:55,052
this slide is show the effects

276
00:09:56,009 --> 00:09:59,074
of varying this parameter sigma squared.

277
00:10:00,003 --> 00:10:01,076
So, sigma squared is the parameter of the

278
00:10:02,052 --> 00:10:04,012
Gaussian kernel and as you vary it, you get slightly different effects.

279
00:10:05,014 --> 00:10:06,037
Let's set sigma squared to be

280
00:10:06,064 --> 00:10:07,057
equal to 0.5 and see

281
00:10:07,071 --> 00:10:09,085
what we get. We set sigma square to 0.5,

282
00:10:10,009 --> 00:10:11,016
what you find is that the

283
00:10:11,042 --> 00:10:12,066
kernel looks similar, except for the

284
00:10:12,073 --> 00:10:14,020
width of the bump becomes narrower.

285
00:10:14,078 --> 00:10:16,039
The contours shrink a bit too.

286
00:10:17,012 --> 00:10:18,036
So if sigma squared equals to 0.5

287
00:10:18,074 --> 00:10:19,082
then as you start

288
00:10:20,025 --> 00:10:21,064
from X equals 3

289
00:10:21,090 --> 00:10:23,013
5 and as you move away,

290
00:10:24,075 --> 00:10:26,037
then the feature f1

291
00:10:27,004 --> 00:10:28,051
falls to zero much more

292
00:10:28,073 --> 00:10:30,083
rapidly and conversely,

293
00:10:32,009 --> 00:10:33,092
if you has increase since

294
00:10:34,066 --> 00:10:36,027
where three in that

295
00:10:36,050 --> 00:10:37,070
case and as I

296
00:10:37,079 --> 00:10:39,009
move away from, you know l. So

297
00:10:39,062 --> 00:10:40,076
this point here is really

298
00:10:41,011 --> 00:10:42,040
l, right, that's l1 is at

299
00:10:42,061 --> 00:10:45,021
location 3 5, right. So it's shown up here.

300
00:10:48,019 --> 00:10:49,048
And if sigma squared is

301
00:10:49,065 --> 00:10:50,046
large, then as you move

302
00:10:50,069 --> 00:10:54,003
away from l1, the

303
00:10:54,032 --> 00:10:56,016
value of the feature falls

304
00:10:56,074 --> 00:10:57,066
away much more slowly.

305
00:11:03,059 --> 00:11:05,020
So, given this definition of

306
00:11:05,028 --> 00:11:06,073
the features, let's see what

307
00:11:06,096 --> 00:11:08,041
source of hypothesis we can learn.

308
00:11:09,054 --> 00:11:11,036
Given the training example X, we

309
00:11:11,048 --> 00:11:12,092
are going to compute these features

310
00:11:14,066 --> 00:11:16,036
f1, f2, f3 and a

311
00:11:17,054 --> 00:11:18,098
hypothesis is going to

312
00:11:19,003 --> 00:11:20,050
predict one when theta 0

313
00:11:20,075 --> 00:11:22,004
plus theta 1 f1 plus theta 2 f2,

314
00:11:22,033 --> 00:11:26,021
and so on is greater than or equal to 0.

315
00:11:26,025 --> 00:11:27,010
For this particular example, let's say

316
00:11:27,028 --> 00:11:28,046
that I've already found a learning

317
00:11:28,062 --> 00:11:29,051
algorithm and let's say that, you know,

318
00:11:30,019 --> 00:11:31,022
somehow I ended up with

319
00:11:31,089 --> 00:11:32,087
these values of the parameter.

320
00:11:33,050 --> 00:11:34,060
So if theta 0 equals

321
00:11:34,083 --> 00:11:36,000
minus 0.5, theta 1 equals

322
00:11:36,038 --> 00:11:37,077
1, theta 2 equals

323
00:11:38,017 --> 00:11:39,057
1, and theta 3

324
00:11:40,037 --> 00:11:42,048
equals 0 And what

325
00:11:42,072 --> 00:11:44,052
I want to do is consider what

326
00:11:44,066 --> 00:11:46,010
happens if we have a

327
00:11:46,020 --> 00:11:48,005
training example that takes

328
00:11:49,025 --> 00:11:51,071
has location at this

329
00:11:52,050 --> 00:11:55,004
magenta dot, right where I just drew this dot over here.

330
00:11:55,037 --> 00:11:56,017
So let's say I have a training

331
00:11:56,028 --> 00:11:58,069
example X, what would my hypothesis predict?

332
00:11:59,000 --> 00:12:01,042
Well, If I look at this formula.

333
00:12:04,058 --> 00:12:05,088
Because my training example X

334
00:12:06,004 --> 00:12:07,082
is close to l1, we have

335
00:12:08,023 --> 00:12:10,019
that f1 is going

336
00:12:10,025 --> 00:12:11,083
to be close to 1 the because

337
00:12:12,025 --> 00:12:13,020
my training example X is

338
00:12:13,036 --> 00:12:15,004
far from l2 and l3 I

339
00:12:15,036 --> 00:12:16,087
have that, you know, f2 would be close to

340
00:12:17,059 --> 00:12:20,050
0 and f3 will be close to 0.

341
00:12:21,054 --> 00:12:22,070
So, if I look at

342
00:12:22,087 --> 00:12:23,097
that formula, I have theta

343
00:12:24,023 --> 00:12:25,066
0 plus theta 1

344
00:12:26,060 --> 00:12:29,097
times 1 plus theta 2 times some value.

345
00:12:30,050 --> 00:12:32,038
Not exactly 0, but let's say close to 0.

346
00:12:33,013 --> 00:12:36,039
Then plus theta 3 times something close to 0.

347
00:12:37,048 --> 00:12:39,080
And this is going to be equal to plugging in these values now.

348
00:12:41,004 --> 00:12:43,047
So, that gives minus 0.5

349
00:12:44,015 --> 00:12:46,082
plus 1 times 1 which is 1, and so on.

350
00:12:46,096 --> 00:12:47,074
Which is equal to 0.5 which is greater than or equal to 0.

351
00:12:48,000 --> 00:12:50,082
So, at this point,

352
00:12:51,015 --> 00:12:54,027
we're going to predict Y equals

353
00:12:54,074 --> 00:12:57,032
1, because that's greater than or equal to zero.

354
00:12:58,090 --> 00:12:59,095
Now let's take a different point.

355
00:13:00,079 --> 00:13:02,010
Now lets' say I take a

356
00:13:02,013 --> 00:13:03,005
different point, I'm going to

357
00:13:03,025 --> 00:13:04,037
draw this one in a different

358
00:13:04,076 --> 00:13:07,008
color, in cyan say, for

359
00:13:07,025 --> 00:13:08,047
a point out there, if that

360
00:13:08,071 --> 00:13:10,058
were my training example X, then

361
00:13:11,026 --> 00:13:12,019
if you make a similar computation,

362
00:13:12,095 --> 00:13:14,038
you find that f1, f2,

363
00:13:15,041 --> 00:13:16,085
Ff3 are all going to be close to 0.

364
00:13:18,015 --> 00:13:19,090
And so, we have theta

365
00:13:20,024 --> 00:13:23,094
0 plus theta 1, f1,

366
00:13:24,023 --> 00:13:26,000
plus so on and this

367
00:13:26,020 --> 00:13:27,083
will be about equal to

368
00:13:28,001 --> 00:13:30,080
minus 0.5, because theta

369
00:13:31,016 --> 00:13:32,011
0 is minus 0.5 and

370
00:13:32,019 --> 00:13:33,091
f1, f2, f3 are all zero.

371
00:13:34,090 --> 00:13:37,050
So this will be minus 0.5, this is less than zero.

372
00:13:37,086 --> 00:13:38,090
And so, at this

373
00:13:39,009 --> 00:13:40,022
point out there, we're going to

374
00:13:40,047 --> 00:13:42,000
predict Y equals zero.

375
00:13:44,019 --> 00:13:45,010
And if you do

376
00:13:45,026 --> 00:13:46,023
this yourself for a range

377
00:13:46,037 --> 00:13:47,046
of different points, be sure to

378
00:13:47,066 --> 00:13:48,065
convince yourself that if you

379
00:13:48,073 --> 00:13:50,034
have a training example that's

380
00:13:50,088 --> 00:13:52,038
close to L2, say,

381
00:13:52,097 --> 00:13:55,073
then at this point we'll also predict Y equals one.

382
00:13:56,079 --> 00:13:58,011
And in fact, what you end

383
00:13:58,024 --> 00:13:59,029
up doing is, you know,

384
00:13:59,035 --> 00:14:00,091
if you look around this boundary, this

385
00:14:01,013 --> 00:14:02,029
space, what we'll find is that

386
00:14:02,082 --> 00:14:03,089
for points near l1

387
00:14:04,009 --> 00:14:05,055
and l2 we end up predicting positive.

388
00:14:06,054 --> 00:14:07,077
And for points far away from

389
00:14:08,004 --> 00:14:09,025
l1 and l2, that's for

390
00:14:09,047 --> 00:14:12,022
points far away from these two

391
00:14:12,048 --> 00:14:13,077
landmarks, we end up predicting

392
00:14:14,038 --> 00:14:15,055
that the class is equal to 0.

393
00:14:16,050 --> 00:14:17,037
As so, what we end up doing,is

394
00:14:17,088 --> 00:14:20,026
that the decision boundary of

395
00:14:20,039 --> 00:14:22,011
this hypothesis would end

396
00:14:22,027 --> 00:14:24,021
up looking something like this where

397
00:14:24,037 --> 00:14:25,062
inside this red decision boundary

398
00:14:26,058 --> 00:14:28,024
would predict Y equals

399
00:14:28,062 --> 00:14:30,025
1 and outside we predict

400
00:14:32,057 --> 00:14:32,057
Y equals 0.

401
00:14:33,001 --> 00:14:34,076
And so this is

402
00:14:34,085 --> 00:14:36,000
how with this definition

403
00:14:36,087 --> 00:14:38,055
of the landmarks and of the kernel function.

404
00:14:39,037 --> 00:14:40,094
We can learn pretty complex non-linear

405
00:14:41,041 --> 00:14:42,079
decision boundary, like what I

406
00:14:42,092 --> 00:14:44,014
just drew where we predict

407
00:14:44,055 --> 00:14:46,099
positive when we're close to either one of the two landmarks.

408
00:14:47,057 --> 00:14:48,087
And we predict negative when we're

409
00:14:49,025 --> 00:14:50,067
very far away from any

410
00:14:50,095 --> 00:14:52,099
of the landmarks.

411
00:14:53,044 --> 00:14:55,000
And so this is part of

412
00:14:55,004 --> 00:14:57,029
the idea of kernels of and

413
00:14:57,060 --> 00:14:58,062
how we use them with the

414
00:14:58,076 --> 00:14:59,080
support vector machine, which is that

415
00:14:59,099 --> 00:15:01,072
we define these extra features using

416
00:15:02,003 --> 00:15:03,089
landmarks and similarity functions

417
00:15:04,076 --> 00:15:06,073
to learn more complex nonlinear classifiers.

418
00:15:08,021 --> 00:15:09,028
So hopefully that gives you

419
00:15:09,038 --> 00:15:10,040
a sense of the idea of

420
00:15:10,059 --> 00:15:11,067
kernels and how we could

421
00:15:11,088 --> 00:15:14,011
use it to define new features for the Support Vector Machine.

422
00:15:15,050 --> 00:15:17,066
But there are a couple of questions that we haven't answered yet.

423
00:15:18,000 --> 00:15:19,054
One is, how do we get these landmarks?

424
00:15:20,012 --> 00:15:20,092
How do we choose these landmarks?

425
00:15:21,004 --> 00:15:22,090
And another is, what

426
00:15:23,009 --> 00:15:24,050
other similarity functions, if any,

427
00:15:24,075 --> 00:15:25,067
can we use other than the

428
00:15:25,077 --> 00:15:29,000
one we talked about, which is called the Gaussian kernel.

429
00:15:29,019 --> 00:15:29,097
In the next video we give

430
00:15:29,099 --> 00:15:31,028
answers to these questions and put

431
00:15:31,049 --> 00:15:33,014
everything together to show how

432
00:15:33,074 --> 00:15:35,005
support vector machines with kernels

433
00:15:35,072 --> 00:15:36,096
can be a powerful way

434
00:15:37,020 --> 00:15:38,061
to learn complex nonlinear functions.
