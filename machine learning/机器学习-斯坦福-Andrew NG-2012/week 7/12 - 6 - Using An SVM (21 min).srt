
1
00:00:00,014 --> 00:00:01,031
So far we've been talking about

2
00:00:01,063 --> 00:00:03,029
SVMs in a fairly abstract level.

3
00:00:03,098 --> 00:00:05,003
In this video I'd like to

4
00:00:05,020 --> 00:00:06,046
talk about what you actually need

5
00:00:06,074 --> 00:00:09,041
to do in order to run or to use an SVM.

6
00:00:11,032 --> 00:00:12,030
The support vector machine algorithm

7
00:00:12,084 --> 00:00:14,086
poses a particular optimization problem.

8
00:00:15,052 --> 00:00:16,094
But as I briefly mentioned in

9
00:00:17,012 --> 00:00:18,014
an earlier video, I really

10
00:00:18,037 --> 00:00:20,057
do not recommend writing your

11
00:00:20,062 --> 00:00:22,080
own software to solve for the parameter's theta yourself.

12
00:00:23,094 --> 00:00:26,010
So just as today, very

13
00:00:26,042 --> 00:00:27,073
few of us, or maybe almost essentially

14
00:00:28,008 --> 00:00:29,039
none of us would think of

15
00:00:29,053 --> 00:00:31,067
writing code ourselves to invert a matrix

16
00:00:31,094 --> 00:00:33,093
or take a square root of a number, and so on.

17
00:00:34,018 --> 00:00:36,057
We just, you know, call some library function to do that.

18
00:00:36,070 --> 00:00:38,009
In the same way, the

19
00:00:38,085 --> 00:00:40,031
software for solving the SVM

20
00:00:40,061 --> 00:00:42,020
optimization problem is very

21
00:00:42,043 --> 00:00:43,088
complex, and there have

22
00:00:43,099 --> 00:00:44,096
been researchers that have been

23
00:00:45,010 --> 00:00:47,056
doing essentially numerical optimization research for many years.

24
00:00:47,085 --> 00:00:48,096
So you come up with good

25
00:00:49,014 --> 00:00:50,054
software libraries and good software

26
00:00:50,092 --> 00:00:52,027
packages to do this.

27
00:00:52,046 --> 00:00:53,047
And then strongly recommend just using

28
00:00:53,085 --> 00:00:55,025
one of the highly optimized software

29
00:00:55,071 --> 00:00:57,078
libraries rather than trying to implement something yourself.

30
00:00:58,072 --> 00:01:00,067
And there are lots of good software libraries out there.

31
00:01:00,096 --> 00:01:02,006
The two that I happen to

32
00:01:02,021 --> 00:01:03,021
use the most often are the

33
00:01:03,039 --> 00:01:05,000
linear SVM but there are really

34
00:01:05,040 --> 00:01:06,085
lots of good software libraries for

35
00:01:07,003 --> 00:01:08,043
doing this that you know, you can

36
00:01:08,059 --> 00:01:10,018
link to many of the

37
00:01:10,045 --> 00:01:11,085
major programming languages that you

38
00:01:11,095 --> 00:01:14,040
may be using to code up learning algorithm.

39
00:01:15,028 --> 00:01:16,045
Even though you shouldn't be writing

40
00:01:16,073 --> 00:01:18,032
your own SVM optimization software,

41
00:01:19,012 --> 00:01:20,068
there are a few things you need to do, though.

42
00:01:21,042 --> 00:01:23,012
First is to come up

43
00:01:23,012 --> 00:01:24,023
with with some choice of the

44
00:01:24,031 --> 00:01:25,064
parameter's C. We talked a

45
00:01:25,093 --> 00:01:26,093
little bit of the bias/variance properties of

46
00:01:27,004 --> 00:01:28,084
this in the earlier video.

47
00:01:30,029 --> 00:01:31,048
Second, you also need to

48
00:01:31,062 --> 00:01:33,004
choose the kernel or the

49
00:01:33,040 --> 00:01:34,087
similarity function that you want to use.

50
00:01:35,073 --> 00:01:37,007
So one choice might

51
00:01:37,028 --> 00:01:38,098
be if we decide not to use any kernel.

52
00:01:40,056 --> 00:01:41,051
And the idea of no kernel

53
00:01:41,090 --> 00:01:43,059
is also called a linear kernel.

54
00:01:44,012 --> 00:01:45,031
So if someone says, I use

55
00:01:45,053 --> 00:01:46,076
an SVM with a linear kernel,

56
00:01:47,018 --> 00:01:48,032
what that means is you know, they use

57
00:01:48,048 --> 00:01:50,068
an SVM without using without

58
00:01:51,001 --> 00:01:52,025
using a kernel and it

59
00:01:52,035 --> 00:01:53,040
was a version of the SVM

60
00:01:54,012 --> 00:01:55,087
that just uses theta transpose X, right,

61
00:01:56,014 --> 00:01:57,062
that predicts 1 theta 0

62
00:01:57,084 --> 00:01:59,042
plus theta 1 X1

63
00:01:59,073 --> 00:02:01,000
plus so on plus theta

64
00:02:01,068 --> 00:02:04,015
N, X N is greater than equals 0.

65
00:02:05,051 --> 00:02:06,082
This term linear kernel, you

66
00:02:06,095 --> 00:02:08,025
can think of this as you know this

67
00:02:08,047 --> 00:02:09,028
is the version of the SVM

68
00:02:10,034 --> 00:02:12,031
that just gives you a standard linear classifier.

69
00:02:13,093 --> 00:02:14,069
So that would be one

70
00:02:15,003 --> 00:02:16,015
reasonable choice for some problems,

71
00:02:17,012 --> 00:02:18,008
and you know, there would be many software

72
00:02:18,046 --> 00:02:20,090
libraries, like linear, was

73
00:02:21,021 --> 00:02:22,031
one example, out of many,

74
00:02:22,084 --> 00:02:23,087
one example of a software library

75
00:02:24,056 --> 00:02:25,062
that can train an SVM

76
00:02:25,097 --> 00:02:27,040
without using a kernel, also

77
00:02:27,075 --> 00:02:29,046
called a linear kernel.

78
00:02:29,084 --> 00:02:31,034
So, why would you want to do this?

79
00:02:31,040 --> 00:02:32,081
If you have a large number of

80
00:02:33,015 --> 00:02:34,028
features, if N is

81
00:02:34,043 --> 00:02:37,080
large, and M the

82
00:02:37,099 --> 00:02:39,059
number of training examples is

83
00:02:39,066 --> 00:02:41,005
small, then you know

84
00:02:41,022 --> 00:02:42,030
you have a huge number of

85
00:02:42,036 --> 00:02:43,062
features that if X, this is

86
00:02:43,071 --> 00:02:45,084
an X is an Rn, Rn +1.

87
00:02:46,000 --> 00:02:46,093
So if you have a

88
00:02:47,008 --> 00:02:48,069
huge number of features already, with

89
00:02:48,080 --> 00:02:50,053
a small training set, you know, maybe you

90
00:02:50,061 --> 00:02:51,043
want to just fit a linear

91
00:02:51,071 --> 00:02:52,088
decision boundary and not try

92
00:02:53,006 --> 00:02:54,041
to fit a very complicated nonlinear

93
00:02:54,086 --> 00:02:56,097
function, because might not have enough data.

94
00:02:57,056 --> 00:02:59,033
And you might risk overfitting, if

95
00:02:59,046 --> 00:03:00,053
you're trying to fit a very complicated function

96
00:03:01,053 --> 00:03:03,021
in a very high dimensional feature space,

97
00:03:03,097 --> 00:03:04,099
but if your training set sample

98
00:03:05,003 --> 00:03:07,012
is small. So this

99
00:03:07,034 --> 00:03:08,059
would be one reasonable setting where

100
00:03:08,074 --> 00:03:09,094
you might decide to just

101
00:03:10,069 --> 00:03:11,096
not use a kernel, or

102
00:03:12,025 --> 00:03:15,058
equivalents to use what's called a linear kernel.

103
00:03:15,074 --> 00:03:16,074
A second choice for the kernel that

104
00:03:16,081 --> 00:03:18,000
you might make, is this Gaussian

105
00:03:18,037 --> 00:03:19,091
kernel, and this is what we had previously.

106
00:03:21,027 --> 00:03:22,034
And if you do this, then the

107
00:03:22,043 --> 00:03:23,012
other choice you need to make

108
00:03:23,041 --> 00:03:25,097
is to choose this parameter sigma squared

109
00:03:26,084 --> 00:03:29,080
when we also talk a little bit about the bias variance tradeoffs

110
00:03:30,081 --> 00:03:32,036
of how, if sigma squared is

111
00:03:32,059 --> 00:03:33,088
large, then you tend

112
00:03:34,015 --> 00:03:35,058
to have a higher bias, lower

113
00:03:35,077 --> 00:03:37,065
variance classifier, but if

114
00:03:37,080 --> 00:03:39,069
sigma squared is small, then you

115
00:03:40,006 --> 00:03:42,036
have a higher variance, lower bias classifier.

116
00:03:43,093 --> 00:03:45,034
So when would you choose a Gaussian kernel?

117
00:03:46,021 --> 00:03:48,005
Well, if your omission

118
00:03:48,031 --> 00:03:49,053
of features X, I mean

119
00:03:49,081 --> 00:03:51,037
Rn, and if N

120
00:03:51,056 --> 00:03:53,088
is small, and, ideally, you know,

121
00:03:55,065 --> 00:03:57,011
if n is large, right,

122
00:03:58,046 --> 00:04:00,016
so that's if, you know, we have

123
00:04:00,055 --> 00:04:02,034
say, a two-dimensional training set,

124
00:04:03,012 --> 00:04:04,087
like the example I drew earlier.

125
00:04:05,046 --> 00:04:08,031
So n is equal to 2, but we have a pretty large training set.

126
00:04:08,068 --> 00:04:09,077
So, you know, I've drawn in a

127
00:04:09,094 --> 00:04:10,088
fairly large number of training examples,

128
00:04:11,065 --> 00:04:12,040
then maybe you want to use

129
00:04:12,053 --> 00:04:14,040
a kernel to fit a more

130
00:04:14,090 --> 00:04:16,025
complex nonlinear decision boundary,

131
00:04:16,064 --> 00:04:18,075
and the Gaussian kernel would be a fine way to do this.

132
00:04:19,048 --> 00:04:20,061
I'll say more towards the end

133
00:04:20,072 --> 00:04:22,056
of the video, a little bit

134
00:04:22,066 --> 00:04:23,075
more about when you might choose a

135
00:04:23,097 --> 00:04:26,031
linear kernel, a Gaussian kernel and so on.

136
00:04:27,086 --> 00:04:29,074
But if concretely, if you

137
00:04:30,004 --> 00:04:31,020
decide to use a Gaussian

138
00:04:31,072 --> 00:04:33,091
kernel, then here's what you need to do.

139
00:04:35,037 --> 00:04:36,055
Depending on what support vector machine

140
00:04:37,027 --> 00:04:38,099
software package you use, it

141
00:04:39,010 --> 00:04:40,095
may ask you to implement a

142
00:04:41,006 --> 00:04:42,019
kernel function, or to implement

143
00:04:43,006 --> 00:04:43,087
the similarity function.

144
00:04:45,001 --> 00:04:46,075
So if you're using an

145
00:04:47,000 --> 00:04:49,081
octave or MATLAB implementation of

146
00:04:50,000 --> 00:04:50,072
an SVM, it may ask you

147
00:04:50,081 --> 00:04:52,056
to provide a function to

148
00:04:52,068 --> 00:04:54,068
compute a particular feature of the kernel.

149
00:04:55,011 --> 00:04:56,048
So this is really computing f

150
00:04:56,076 --> 00:04:57,088
subscript i for one

151
00:04:58,022 --> 00:04:59,056
particular value of i, where

152
00:05:00,056 --> 00:05:02,031
f here is just a

153
00:05:02,032 --> 00:05:03,056
single real number, so maybe

154
00:05:03,083 --> 00:05:05,006
I should move this better written

155
00:05:05,025 --> 00:05:07,023
f(i), but what you

156
00:05:07,050 --> 00:05:08,012
need to do is to write a kernel

157
00:05:08,048 --> 00:05:09,052
function that takes this input, you know,

158
00:05:10,061 --> 00:05:11,091
a training example or a

159
00:05:12,001 --> 00:05:13,013
test example whatever it takes

160
00:05:13,027 --> 00:05:14,063
in some vector X and takes

161
00:05:14,099 --> 00:05:16,022
as input one of the

162
00:05:16,037 --> 00:05:18,026
landmarks and but

163
00:05:18,087 --> 00:05:20,075
only I've come down X1 and

164
00:05:20,094 --> 00:05:21,081
X2 here, because the

165
00:05:21,089 --> 00:05:23,075
landmarks are really training examples as well.

166
00:05:24,047 --> 00:05:26,016
But what you

167
00:05:26,039 --> 00:05:27,049
need to do is write software that

168
00:05:27,067 --> 00:05:28,095
takes this input, you know, X1, X2

169
00:05:29,014 --> 00:05:30,031
and computes this sort

170
00:05:30,057 --> 00:05:31,094
of similarity function between them

171
00:05:32,052 --> 00:05:33,047
and return a real number.

172
00:05:36,018 --> 00:05:37,043
And so what some support vector machine

173
00:05:37,057 --> 00:05:39,004
packages do is expect

174
00:05:39,050 --> 00:05:40,086
you to provide this kernel function

175
00:05:41,041 --> 00:05:44,057
that take this input you know, X1, X2 and returns a real number.

176
00:05:45,057 --> 00:05:46,045
And then it will take it from there

177
00:05:46,085 --> 00:05:49,006
and it will automatically generate all the features, and

178
00:05:49,041 --> 00:05:51,048
so automatically take X and

179
00:05:51,060 --> 00:05:53,037
map it to f1,

180
00:05:53,042 --> 00:05:54,042
f2, down to f(m) using

181
00:05:54,075 --> 00:05:56,019
this function that you write, and

182
00:05:56,031 --> 00:05:57,018
generate all the features and

183
00:05:57,064 --> 00:05:59,007
train the support vector machine from there.

184
00:05:59,087 --> 00:06:00,080
But sometimes you do need to

185
00:06:00,087 --> 00:06:04,070
provide this function yourself.

186
00:06:05,068 --> 00:06:06,076
Other if you are using the Gaussian kernel, some SVM implementations will also include the Gaussian kernel

187
00:06:06,098 --> 00:06:09,094
and a

188
00:06:10,004 --> 00:06:10,099
few other kernels as well, since

189
00:06:11,023 --> 00:06:13,057
the Gaussian kernel is probably the most common kernel.

190
00:06:14,087 --> 00:06:16,029
Gaussian and linear kernels are

191
00:06:16,037 --> 00:06:18,020
really the two most popular kernels by far.

192
00:06:19,012 --> 00:06:20,023
Just one implementational note.

193
00:06:20,075 --> 00:06:21,081
If you have features of very

194
00:06:22,007 --> 00:06:23,062
different scales, it is important

195
00:06:24,069 --> 00:06:26,026
to perform feature scaling before

196
00:06:26,060 --> 00:06:27,077
using the Gaussian kernel.

197
00:06:28,057 --> 00:06:29,018
And here's why.

198
00:06:30,014 --> 00:06:31,060
If you imagine the computing

199
00:06:32,029 --> 00:06:33,056
the norm between X and

200
00:06:33,079 --> 00:06:34,088
l, right, so this term here,

201
00:06:35,038 --> 00:06:37,014
and the numerator term over there.

202
00:06:38,030 --> 00:06:39,077
What this is doing, the norm

203
00:06:40,006 --> 00:06:40,093
between X and l, that's really

204
00:06:41,012 --> 00:06:42,013
saying, you know, let's compute the vector

205
00:06:42,044 --> 00:06:43,029
V, which is equal to

206
00:06:43,041 --> 00:06:44,098
X minus l. And then

207
00:06:45,025 --> 00:06:47,093
let's compute the norm does

208
00:06:48,012 --> 00:06:49,007
vector V, which is the

209
00:06:49,017 --> 00:06:50,050
difference between X. So the

210
00:06:50,057 --> 00:06:51,050
norm of V is really

211
00:06:53,036 --> 00:06:54,013
equal to V1 squared

212
00:06:54,025 --> 00:06:55,061
plus V2 squared plus

213
00:06:55,082 --> 00:06:58,029
dot dot dot, plus Vn squared.

214
00:06:58,089 --> 00:07:00,031
Because here X is in

215
00:07:01,006 --> 00:07:02,019
Rn, or Rn

216
00:07:02,029 --> 00:07:05,018
plus 1, but I'm going to ignore, you know, X0.

217
00:07:06,054 --> 00:07:08,042
So, let's pretend X is

218
00:07:08,050 --> 00:07:10,080
an Rn, square on

219
00:07:10,094 --> 00:07:12,031
the left side is what makes this correct.

220
00:07:12,056 --> 00:07:14,008
So this is equal

221
00:07:14,039 --> 00:07:16,012
to that, right?

222
00:07:17,020 --> 00:07:18,070
And so written differently, this is

223
00:07:18,085 --> 00:07:20,010
going to be X1 minus l1

224
00:07:20,029 --> 00:07:22,060
squared, plus x2

225
00:07:22,091 --> 00:07:24,058
minus l2 squared, plus

226
00:07:24,091 --> 00:07:26,057
dot dot dot plus Xn minus

227
00:07:27,012 --> 00:07:28,054
ln squared.

228
00:07:29,072 --> 00:07:30,079
And now if your features

229
00:07:31,085 --> 00:07:33,045
take on very different ranges of value.

230
00:07:33,093 --> 00:07:35,014
So take a housing

231
00:07:35,036 --> 00:07:37,018
prediction, for example, if

232
00:07:38,001 --> 00:07:40,049
your data is some data about houses.

233
00:07:41,042 --> 00:07:43,000
And if X is in the

234
00:07:43,013 --> 00:07:44,066
range of thousands of square

235
00:07:44,094 --> 00:07:47,018
feet, for the

236
00:07:48,000 --> 00:07:48,083
first feature, X1.

237
00:07:49,069 --> 00:07:51,062
But if your second feature, X2 is the number of bedrooms.

238
00:07:52,054 --> 00:07:53,061
So if this is in the

239
00:07:53,073 --> 00:07:56,072
range of one to five bedrooms, then

240
00:07:57,081 --> 00:07:59,031
X1 minus l1 is going to be huge.

241
00:07:59,077 --> 00:08:00,081
This could be like a thousand squared,

242
00:08:01,000 --> 00:08:02,087
whereas X2 minus l2

243
00:08:03,019 --> 00:08:04,062
is going to be much smaller and if

244
00:08:04,075 --> 00:08:06,080
that's the case, then in this term,

245
00:08:08,031 --> 00:08:09,066
those distances will be almost

246
00:08:10,006 --> 00:08:12,006
essentially dominated by the

247
00:08:12,056 --> 00:08:13,027
sizes of the houses

248
00:08:14,038 --> 00:08:15,075
and the number of bathrooms would be largely ignored.

249
00:08:16,094 --> 00:08:18,006
As so as, to avoid this in

250
00:08:18,023 --> 00:08:19,006
order to make a machine work

251
00:08:19,036 --> 00:08:21,088
well, do perform future scaling.

252
00:08:23,042 --> 00:08:24,082
And that will sure that the SVM

253
00:08:25,081 --> 00:08:27,001
gives, you know, comparable amount of attention

254
00:08:27,094 --> 00:08:28,087
to all of your different features,

255
00:08:29,018 --> 00:08:30,044
and not just to in

256
00:08:30,060 --> 00:08:31,087
this example to size of

257
00:08:32,014 --> 00:08:33,044
houses were big movement here the features.

258
00:08:34,070 --> 00:08:35,080
When you try a support vector

259
00:08:36,011 --> 00:08:38,075
machines chances are by

260
00:08:38,097 --> 00:08:40,000
far the two most common

261
00:08:40,046 --> 00:08:41,075
kernels you use will

262
00:08:41,085 --> 00:08:43,012
be the linear kernel, meaning no

263
00:08:43,032 --> 00:08:45,060
kernel, or the Gaussian kernel that we talked about.

264
00:08:46,051 --> 00:08:47,038
And just one note of warning

265
00:08:47,089 --> 00:08:49,007
which is that not all similarity

266
00:08:49,058 --> 00:08:50,059
functions you might come up

267
00:08:50,076 --> 00:08:52,051
with are valid kernels.

268
00:08:53,045 --> 00:08:54,084
And the Gaussian kernel and the linear

269
00:08:55,009 --> 00:08:56,040
kernel and other kernels that you

270
00:08:56,071 --> 00:08:57,085
sometimes others will use, all

271
00:08:58,002 --> 00:08:59,084
of them need to satisfy a technical condition.

272
00:09:00,037 --> 00:09:02,050
It's called Mercer's Theorem and

273
00:09:02,062 --> 00:09:03,055
the reason you need to this

274
00:09:03,071 --> 00:09:05,042
is because support vector machine

275
00:09:06,037 --> 00:09:08,013
algorithms or implementations of the

276
00:09:08,048 --> 00:09:09,055
SVM have lots of clever

277
00:09:10,004 --> 00:09:11,037
numerical optimization tricks.

278
00:09:12,011 --> 00:09:13,026
In order to solve for the

279
00:09:13,034 --> 00:09:15,064
parameter's theta efficiently and

280
00:09:16,059 --> 00:09:18,084
in the original design envisaged,

281
00:09:19,047 --> 00:09:21,000
those are decision made to restrict

282
00:09:21,053 --> 00:09:22,089
our attention only to kernels

283
00:09:23,050 --> 00:09:25,086
that satisfy this technical condition called Mercer's Theorem.

284
00:09:26,027 --> 00:09:27,036
And what that does is, that

285
00:09:27,057 --> 00:09:28,053
makes sure that all of these

286
00:09:28,082 --> 00:09:30,026
SVM packages, all of these SVM

287
00:09:30,050 --> 00:09:32,021
software packages can use the

288
00:09:32,030 --> 00:09:34,074
large class of optimizations and

289
00:09:35,027 --> 00:09:37,047
get the parameter theta very quickly.

290
00:09:39,032 --> 00:09:40,034
So, what most people end up doing

291
00:09:40,084 --> 00:09:42,047
is using either the linear

292
00:09:42,061 --> 00:09:44,021
or Gaussian kernel, but there

293
00:09:44,042 --> 00:09:45,061
are a few other kernels that also

294
00:09:45,094 --> 00:09:47,046
satisfy Mercer's theorem and

295
00:09:47,055 --> 00:09:48,069
that you may run across other

296
00:09:48,085 --> 00:09:50,004
people using, although I personally

297
00:09:50,087 --> 00:09:53,077
end up using other kernels you know, very, very rarely, if at all.

298
00:09:54,015 --> 00:09:56,099
Just to mention some of the other kernels that you may run across.

299
00:09:57,099 --> 00:10:00,029
One is the polynomial kernel.

300
00:10:01,057 --> 00:10:03,035
And for that the similarity between

301
00:10:03,079 --> 00:10:05,051
X and l is

302
00:10:05,073 --> 00:10:06,075
defined as, there are

303
00:10:06,083 --> 00:10:07,087
a lot of options, you can

304
00:10:08,063 --> 00:10:10,037
take X transpose l squared.

305
00:10:10,096 --> 00:10:13,040
So, here's one measure of how similar X and l are.

306
00:10:13,061 --> 00:10:14,092
If X and l are very close with

307
00:10:15,050 --> 00:10:18,025
each other, then the inner product will tend to be large.

308
00:10:20,020 --> 00:10:21,087
And so, you know, this is a slightly

309
00:10:23,008 --> 00:10:23,051
unusual kernel.

310
00:10:24,000 --> 00:10:25,012
That is not used that often, but

311
00:10:26,049 --> 00:10:29,019
you may run across some people using it.

312
00:10:30,004 --> 00:10:31,080
This is one version of a polynomial kernel.

313
00:10:32,033 --> 00:10:35,009
Another is X transpose l cubed.

314
00:10:36,069 --> 00:10:38,077
These are all examples of the polynomial kernel.

315
00:10:39,003 --> 00:10:41,026
X transpose l plus 1 cubed.

316
00:10:42,055 --> 00:10:43,062
X transpose l plus maybe

317
00:10:43,090 --> 00:10:44,092
a number different then one 5

318
00:10:44,097 --> 00:10:46,067
and, you know, to the power of 4 and

319
00:10:47,070 --> 00:10:49,084
so the polynomial kernel actually has two parameters.

320
00:10:50,061 --> 00:10:53,001
One is, what number do you add over here?

321
00:10:53,051 --> 00:10:53,091
It could be 0.

322
00:10:54,042 --> 00:10:58,065
This is really plus 0 over there, as well as what's the degree of the polynomial over there.

323
00:10:58,067 --> 00:11:01,066
So the degree power and these numbers.

324
00:11:02,025 --> 00:11:04,013
And the more general form of the

325
00:11:04,027 --> 00:11:05,052
polynomial kernel is X

326
00:11:05,072 --> 00:11:07,062
transpose l, plus some

327
00:11:07,094 --> 00:11:11,050
constant and then

328
00:11:11,079 --> 00:11:14,085
to some degree in the

329
00:11:15,005 --> 00:11:16,072
X1 and so both

330
00:11:16,094 --> 00:11:19,064
of these are parameters for the polynomial kernel.

331
00:11:20,050 --> 00:11:22,082
So the polynomial kernel almost always

332
00:11:23,035 --> 00:11:24,044
or usually performs worse.

333
00:11:24,082 --> 00:11:25,095
And the Gaussian kernel does not

334
00:11:26,026 --> 00:11:28,037
use that much, but this is just something that you may run across.

335
00:11:29,032 --> 00:11:30,048
Usually it is used only for

336
00:11:30,075 --> 00:11:31,071
data where X and l

337
00:11:32,000 --> 00:11:33,017
are all strictly non negative,

338
00:11:33,074 --> 00:11:34,072
and so that ensures that these

339
00:11:34,090 --> 00:11:36,071
inner products are never negative.

340
00:11:37,085 --> 00:11:40,000
And this captures the intuition that

341
00:11:40,038 --> 00:11:41,034
X and l are very similar

342
00:11:41,053 --> 00:11:44,011
to each other, then maybe the inter product between them will be large.

343
00:11:44,041 --> 00:11:45,059
They have some other properties as well

344
00:11:46,025 --> 00:11:48,008
but people tend not to use it much.

345
00:11:49,012 --> 00:11:50,014
And then, depending on what you're

346
00:11:50,025 --> 00:11:51,021
doing, there are other, sort of more

347
00:11:52,033 --> 00:11:54,095
esoteric kernels as well, that you may come across.

348
00:11:55,066 --> 00:11:57,017
You know, there's a string kernel, this

349
00:11:57,034 --> 00:11:58,042
is sometimes used if your

350
00:11:58,054 --> 00:12:01,035
input data is text strings or other types of strings.

351
00:12:02,026 --> 00:12:02,094
There are things like the

352
00:12:03,025 --> 00:12:06,000
chi-square kernel, the histogram intersection kernel, and so on.

353
00:12:06,069 --> 00:12:08,041
There are sort of more esoteric kernels that

354
00:12:08,065 --> 00:12:09,084
you can use to measure similarity

355
00:12:10,075 --> 00:12:12,002
between different objects.

356
00:12:12,065 --> 00:12:13,079
So for example, if you're trying to

357
00:12:14,037 --> 00:12:15,084
do some sort of text classification

358
00:12:16,016 --> 00:12:17,005
problem, where the input

359
00:12:17,020 --> 00:12:19,029
x is a string then

360
00:12:19,049 --> 00:12:20,049
maybe we want to find the

361
00:12:20,054 --> 00:12:22,004
similarity between two strings

362
00:12:22,042 --> 00:12:24,024
using the string kernel, but I

363
00:12:24,051 --> 00:12:26,044
personally you know end up very rarely,

364
00:12:26,099 --> 00:12:29,034
if at all, using these more esoteric kernels.
I

365
00:12:29,087 --> 00:12:30,097
think I might have use the chi-square

366
00:12:31,016 --> 00:12:32,026
kernel, may be once in

367
00:12:32,034 --> 00:12:33,066
my life and the histogram kernel,

368
00:12:34,024 --> 00:12:35,058
may be once or twice in my life. I've

369
00:12:35,062 --> 00:12:38,050
actually never used the string kernel myself. But in

370
00:12:39,035 --> 00:12:41,055
case you've run across this in other applications. You know, if

371
00:12:42,070 --> 00:12:43,063
you do a quick web

372
00:12:43,086 --> 00:12:44,085
search we do a quick Google

373
00:12:45,003 --> 00:12:46,000
search or quick Bing search

374
00:12:46,059 --> 00:12:48,024
you should have found definitions that these are the kernels as well.
So

375
00:12:51,048 --> 00:12:55,067
just two last details I want to talk about in this video. One in multiclass classification. So, you

376
00:12:56,037 --> 00:12:59,050
have four classes or more generally

377
00:12:59,079 --> 00:13:01,087
3 classes output some appropriate

378
00:13:02,052 --> 00:13:06,086
decision bounday between your multiple classes. Most SVM, many SVM

379
00:13:07,022 --> 00:13:08,075
packages already have built-in

380
00:13:09,002 --> 00:13:10,042
multiclass classification functionality. So

381
00:13:11,010 --> 00:13:12,005
if your using a pattern like

382
00:13:12,026 --> 00:13:13,032
that, you just use the

383
00:13:13,053 --> 00:13:15,037
both that functionality and that

384
00:13:15,049 --> 00:13:16,094
should work fine. Otherwise,

385
00:13:17,078 --> 00:13:18,078
one way to do this

386
00:13:19,000 --> 00:13:19,087
is to use the one

387
00:13:20,000 --> 00:13:21,027
versus all method that we

388
00:13:21,037 --> 00:13:23,069
talked about when we are developing logistic regression. So

389
00:13:24,067 --> 00:13:25,040
what you do is you trade

390
00:13:26,015 --> 00:13:27,054
kSVM's if you have

391
00:13:27,070 --> 00:13:29,019
k classes, one to distinguish

392
00:13:29,089 --> 00:13:31,005
each of the classes from the rest.

393
00:13:31,085 --> 00:13:32,092
And this would give you k parameter

394
00:13:33,051 --> 00:13:34,052
vectors, so this will

395
00:13:34,067 --> 00:13:36,021
give you, upi lmpw. theta 1, which

396
00:13:36,052 --> 00:13:38,016
is trying to distinguish class y equals

397
00:13:38,062 --> 00:13:39,098
one from all of

398
00:13:40,012 --> 00:13:41,034
the other classes, then you

399
00:13:41,041 --> 00:13:42,090
get the second parameter, vector

400
00:13:42,097 --> 00:13:43,090
theta 2, which is what

401
00:13:44,001 --> 00:13:45,041
you get when you, you know, have

402
00:13:45,072 --> 00:13:47,008
y equals 2 as the positive class

403
00:13:47,046 --> 00:13:48,067
and all the others as negative class

404
00:13:49,025 --> 00:13:50,054
and so on up to

405
00:13:50,079 --> 00:13:52,039
a parameter vector theta k,

406
00:13:52,075 --> 00:13:54,051
which is the parameter vector for

407
00:13:54,060 --> 00:13:56,076
distinguishing the final class

408
00:13:57,036 --> 00:13:59,037
key from anything else, and

409
00:13:59,049 --> 00:14:00,059
then lastly, this is exactly

410
00:14:01,026 --> 00:14:02,003
the same as the one versus

411
00:14:02,041 --> 00:14:04,023
all method we have for logistic regression.

412
00:14:04,075 --> 00:14:05,090
Where we you just predict the class

413
00:14:06,038 --> 00:14:07,069
i with the largest theta

414
00:14:08,002 --> 00:14:11,084
transpose X.  So let's multiclass classification designate.

415
00:14:12,044 --> 00:14:13,075
For the more common cases

416
00:14:14,029 --> 00:14:15,009
that there is a good

417
00:14:15,017 --> 00:14:16,046
chance that whatever software package

418
00:14:16,077 --> 00:14:18,000
you use, you know, there will be

419
00:14:18,034 --> 00:14:19,064
a reasonable chance that are already

420
00:14:19,091 --> 00:14:21,074
have built in multiclass classification functionality,

421
00:14:21,091 --> 00:14:24,040
and so you don't need to worry about this result.

422
00:14:25,027 --> 00:14:27,000
Finally, we developed support vector

423
00:14:27,021 --> 00:14:28,064
machines starting off with logistic

424
00:14:29,009 --> 00:14:31,050
regression and then modifying the cost function a little bit.

425
00:14:31,090 --> 00:14:34,089
The last thing we want to do in this video is, just say a little bit about.

426
00:14:35,054 --> 00:14:36,057
when you will use one of

427
00:14:36,065 --> 00:14:38,084
these two algorithms, so let's

428
00:14:39,008 --> 00:14:40,000
say n is the number

429
00:14:40,015 --> 00:14:42,000
of features and m is the number of training examples.

430
00:14:43,019 --> 00:14:45,025
So, when should we use one algorithm versus the other?

431
00:14:47,012 --> 00:14:48,042
Well, if n is larger

432
00:14:48,098 --> 00:14:50,013
relative to your training set

433
00:14:50,036 --> 00:14:51,038
size, so for example,

434
00:14:52,080 --> 00:14:53,099
if you take a business

435
00:14:54,025 --> 00:14:55,017
with a number of features this is

436
00:14:55,033 --> 00:14:56,087
much larger than m and this

437
00:14:57,012 --> 00:14:58,021
might be, for example, if you

438
00:14:58,032 --> 00:15:00,059
have a text classification problem, where

439
00:15:01,054 --> 00:15:02,042
you know, the dimension of the feature

440
00:15:02,070 --> 00:15:04,015
vector is I don't know, maybe, 10 thousand.

441
00:15:05,037 --> 00:15:06,035
And if your training

442
00:15:06,072 --> 00:15:08,028
set size is maybe 10

443
00:15:08,050 --> 00:15:10,025
you know, maybe, up to 1000.

444
00:15:10,050 --> 00:15:12,013
So, imagine a spam

445
00:15:12,032 --> 00:15:14,025
classification problem, where email

446
00:15:14,050 --> 00:15:15,084
spam, where you have 10,000

447
00:15:16,014 --> 00:15:18,000
features corresponding to 10,000 words

448
00:15:18,019 --> 00:15:19,054
but you have, you know, maybe 10

449
00:15:19,077 --> 00:15:21,014
training examples or maybe up to 1,000 examples.

450
00:15:22,045 --> 00:15:23,075
So if n is large relative to

451
00:15:23,088 --> 00:15:25,009
m, then what I

452
00:15:25,025 --> 00:15:26,048
would usually do is use logistic

453
00:15:26,085 --> 00:15:27,099
regression or use it

454
00:15:28,010 --> 00:15:29,002
as the m without a kernel or

455
00:15:29,046 --> 00:15:30,078
use it with a linear kernel.

456
00:15:31,062 --> 00:15:32,042
Because, if you have so many

457
00:15:32,058 --> 00:15:33,083
features with smaller training sets, you know,

458
00:15:34,052 --> 00:15:35,087
a linear function will probably

459
00:15:36,033 --> 00:15:37,037
do fine, and you don't have

460
00:15:37,063 --> 00:15:38,078
really enough data to

461
00:15:38,090 --> 00:15:40,075
fit a very complicated nonlinear function.

462
00:15:41,034 --> 00:15:42,040
Now if is n is

463
00:15:42,051 --> 00:15:44,001
small and m is

464
00:15:44,035 --> 00:15:45,088
intermediate what I mean

465
00:15:45,094 --> 00:15:47,045
by this is n is

466
00:15:48,003 --> 00:15:50,035
maybe anywhere from 1 - 1000, 1 would be very small.

467
00:15:50,052 --> 00:15:51,047
But maybe up to 1000

468
00:15:51,070 --> 00:15:54,026
features and if

469
00:15:54,059 --> 00:15:56,017
the number of training

470
00:15:56,033 --> 00:15:57,070
examples is maybe anywhere from

471
00:15:58,021 --> 00:16:00,075
10, you know, 10 to maybe up to 10,000 examples.

472
00:16:01,035 --> 00:16:03,015
Maybe up to 50,000 examples.

473
00:16:03,062 --> 00:16:06,049
If m is pretty big like maybe 10,000 but not a million.

474
00:16:06,075 --> 00:16:08,010
Right? So if m is an

475
00:16:08,029 --> 00:16:09,095
intermediate size then often

476
00:16:10,078 --> 00:16:12,098
an SVM with a linear kernel will work well.

477
00:16:13,052 --> 00:16:14,058
We talked about this early as

478
00:16:14,071 --> 00:16:15,079
well, with the one concrete example,

479
00:16:16,035 --> 00:16:17,010
this would be if you have

480
00:16:17,051 --> 00:16:19,072
a two dimensional training set. So, if n

481
00:16:19,089 --> 00:16:21,000
is equal to 2 where you

482
00:16:21,032 --> 00:16:23,071
have, you know, drawing in a pretty large number of training examples.

483
00:16:24,071 --> 00:16:25,086
So Gaussian kernel will do

484
00:16:26,012 --> 00:16:28,015
a pretty good job separating positive and negative classes.

485
00:16:29,076 --> 00:16:30,088
One third setting that's of

486
00:16:30,098 --> 00:16:32,041
interest is if n is

487
00:16:32,051 --> 00:16:34,026
small but m is large.

488
00:16:34,088 --> 00:16:36,055
So if n is you know, again maybe

489
00:16:37,038 --> 00:16:39,027
1 to 1000, could be larger.

490
00:16:40,020 --> 00:16:42,075
But if m was, maybe

491
00:16:43,032 --> 00:16:46,039
50,000 and greater to millions.

492
00:16:47,051 --> 00:16:50,026
So, 50,000, a 100,000, million, trillion.

493
00:16:51,028 --> 00:16:54,001
You have very very large training set sizes, right.

494
00:16:55,024 --> 00:16:56,015
So if this is the case,

495
00:16:56,037 --> 00:16:57,062
then a SVM of the

496
00:16:57,089 --> 00:16:59,085
Gaussian Kernel will be somewhat slow to run.

497
00:17:00,015 --> 00:17:02,029
Today's SVM packages, if you're

498
00:17:02,040 --> 00:17:04,090
using a Gaussian Kernel, tend to struggle a bit.

499
00:17:05,004 --> 00:17:06,025
If you have, you know, maybe 50

500
00:17:06,058 --> 00:17:07,052
thousands okay, but if you

501
00:17:07,061 --> 00:17:10,025
have a million training examples, maybe

502
00:17:10,045 --> 00:17:11,095
or even a 100,000 with a

503
00:17:12,017 --> 00:17:13,073
massive value of m. Today's

504
00:17:14,018 --> 00:17:15,058
SVM packages are very good,

505
00:17:15,086 --> 00:17:17,009
but they can still struggle

506
00:17:17,059 --> 00:17:18,040
a little bit when you have a

507
00:17:19,000 --> 00:17:20,094
massive, massive trainings that size when using a Gaussian Kernel.

508
00:17:22,004 --> 00:17:23,015
So in that case, what I

509
00:17:23,034 --> 00:17:24,096
would usually do is try to just

510
00:17:25,032 --> 00:17:26,066
manually create have more

511
00:17:26,079 --> 00:17:28,059
features and then use

512
00:17:28,093 --> 00:17:30,033
logistic regression or an SVM

513
00:17:30,063 --> 00:17:32,005
without the Kernel.

514
00:17:33,014 --> 00:17:34,002
And in case you look at this

515
00:17:34,023 --> 00:17:35,090
slide and you see logistic regression

516
00:17:36,046 --> 00:17:37,075
or SVM without a kernel.

517
00:17:38,050 --> 00:17:39,089
In both of these places, I

518
00:17:39,098 --> 00:17:41,075
kind of paired them together. There's

519
00:17:42,005 --> 00:17:43,004
a reason for that, is that

520
00:17:43,090 --> 00:17:45,064
logistic regression and SVM without

521
00:17:46,000 --> 00:17:47,013
the kernel, those are really pretty

522
00:17:47,034 --> 00:17:49,045
similar algorithms and, you know, either

523
00:17:49,068 --> 00:17:51,017
logistic regression or SVM

524
00:17:51,050 --> 00:17:53,023
without a kernel will usually do

525
00:17:53,038 --> 00:17:54,077
pretty similar things and give

526
00:17:54,090 --> 00:17:56,069
pretty similar performance, but depending

527
00:17:57,005 --> 00:18:00,033
on your implementational details, one may be more efficient than the other.

528
00:18:00,093 --> 00:18:02,022
But, where one of

529
00:18:02,030 --> 00:18:03,052
these algorithms applies, logistic

530
00:18:03,074 --> 00:18:05,019
regression where SVM without a

531
00:18:05,042 --> 00:18:05,083
kernel, the other one is to likely

532
00:18:06,065 --> 00:18:07,059
to work pretty well as well.

533
00:18:08,053 --> 00:18:09,066
But along with the power of

534
00:18:09,072 --> 00:18:11,060
the SVM is when you

535
00:18:11,080 --> 00:18:14,009
use different kernels to learn

536
00:18:14,043 --> 00:18:15,085
complex nonlinear functions.

537
00:18:16,068 --> 00:18:20,029
And this regime, you know, when you

538
00:18:20,054 --> 00:18:22,052
have maybe up to 10,000 examples, maybe up to 50,000.

539
00:18:22,060 --> 00:18:25,000
And your number of features,

540
00:18:26,057 --> 00:18:27,053
this is reasonably large.

541
00:18:27,083 --> 00:18:29,023
That's a very common regime

542
00:18:29,067 --> 00:18:30,091
and maybe that's a regime

543
00:18:31,043 --> 00:18:33,082
where a support vector machine with a kernel kernel will shine.

544
00:18:34,031 --> 00:18:35,064
You can do things that are much

545
00:18:35,085 --> 00:18:39,084
harder to do that will need logistic regression.

546
00:18:40,009 --> 00:18:40,093
And finally, where do neural networks fit in?

547
00:18:41,011 --> 00:18:42,023
Well for all of these

548
00:18:42,044 --> 00:18:43,089
problems, for all of

549
00:18:43,096 --> 00:18:46,030
these different regimes, a well

550
00:18:46,063 --> 00:18:49,010
designed neural network is likely to work well as well.

551
00:18:50,031 --> 00:18:51,070
The one disadvantage, or the one

552
00:18:51,082 --> 00:18:52,098
reason that might not sometimes use

553
00:18:53,022 --> 00:18:54,069
the neural network is that,

554
00:18:54,092 --> 00:18:56,007
for some of these problems, the

555
00:18:56,018 --> 00:18:57,064
neural network might be slow to train.

556
00:18:58,025 --> 00:18:59,007
But if you have a very good

557
00:18:59,034 --> 00:19:01,019
SVM implementation package, that

558
00:19:01,040 --> 00:19:04,011
could run faster, quite a bit faster than your neural network.

559
00:19:05,013 --> 00:19:06,013
And, although we didn't show this

560
00:19:06,034 --> 00:19:07,051
earlier, it turns out that

561
00:19:07,063 --> 00:19:09,079
the optimization problem that the

562
00:19:10,006 --> 00:19:11,011
SVM has is a convex

563
00:19:12,031 --> 00:19:13,082
optimization problem and so the

564
00:19:14,041 --> 00:19:15,079
good SVM optimization software

565
00:19:16,016 --> 00:19:17,086
packages will always find

566
00:19:18,024 --> 00:19:21,036
the global minimum or something close to it.

567
00:19:21,072 --> 00:19:24,009
And so for the SVM you don't need to worry about local optima.

568
00:19:25,027 --> 00:19:26,044
In practice local optima aren't

569
00:19:26,057 --> 00:19:27,092
a huge problem for neural networks

570
00:19:28,008 --> 00:19:29,011
but they all solve, so this

571
00:19:29,030 --> 00:19:31,051
is one less thing to worry about if you're using an SVM.

572
00:19:33,034 --> 00:19:34,055
And depending on your problem, the neural

573
00:19:34,091 --> 00:19:37,004
network may be slower, especially

574
00:19:37,057 --> 00:19:41,001
in this sort of regime than the SVM.

575
00:19:41,042 --> 00:19:42,020
In case the guidelines they gave

576
00:19:42,051 --> 00:19:43,050
here, seem a little bit vague

577
00:19:43,085 --> 00:19:44,059
and if you're looking at some problems, you know,

578
00:19:46,093 --> 00:19:48,004
the guidelines are a bit

579
00:19:48,017 --> 00:19:49,019
vague, I'm still not entirely

580
00:19:49,056 --> 00:19:50,073
sure, should I use this

581
00:19:50,077 --> 00:19:52,069
algorithm or that algorithm, that's actually okay.

582
00:19:52,095 --> 00:19:54,009
When I face a machine learning

583
00:19:54,032 --> 00:19:55,056
problem, you know, sometimes its actually

584
00:19:55,073 --> 00:19:57,000
just not clear whether that's the

585
00:19:57,015 --> 00:19:58,070
best algorithm to use, but as

586
00:19:59,053 --> 00:20:00,058
you saw in the earlier videos, really,

587
00:20:01,020 --> 00:20:02,047
you know, the algorithm does

588
00:20:02,070 --> 00:20:03,092
matter, but what often matters

589
00:20:04,025 --> 00:20:06,040
even more is things like, how much data do you have.

590
00:20:07,008 --> 00:20:08,027
And how skilled are you, how

591
00:20:08,045 --> 00:20:09,050
good are you at doing error

592
00:20:09,075 --> 00:20:11,045
analysis and debugging learning

593
00:20:11,066 --> 00:20:13,008
algorithms, figuring out how

594
00:20:13,022 --> 00:20:15,011
to design new features and

595
00:20:15,027 --> 00:20:17,053
figuring out what other features to give you learning algorithms and so on.

596
00:20:17,096 --> 00:20:19,010
And often those things will matter

597
00:20:19,066 --> 00:20:20,070
more than what you are

598
00:20:20,083 --> 00:20:22,036
using logistic regression or an SVM.

599
00:20:23,027 --> 00:20:24,065
But having said that,

600
00:20:25,000 --> 00:20:26,018
the SVM is still widely

601
00:20:26,063 --> 00:20:27,089
perceived as one of

602
00:20:27,095 --> 00:20:29,059
the most powerful learning algorithms, and

603
00:20:29,074 --> 00:20:31,056
there is this regime of when there's

604
00:20:31,078 --> 00:20:34,033
a very effective way to learn complex non linear functions.

605
00:20:35,015 --> 00:20:36,083
And so I actually, together with

606
00:20:37,003 --> 00:20:38,093
logistic regressions, neural networks, SVM's,

607
00:20:39,008 --> 00:20:40,063
using those to speed

608
00:20:40,075 --> 00:20:42,017
learning algorithms you're I think

609
00:20:42,044 --> 00:20:43,060
very well positioned to build

610
00:20:44,011 --> 00:20:45,011
state of the art you know,

611
00:20:45,030 --> 00:20:46,071
machine learning systems for a wide

612
00:20:46,096 --> 00:20:49,010
region for applications and this

613
00:20:49,032 --> 00:20:52,046
is another very powerful tool to have in your arsenal.

614
00:20:53,016 --> 00:20:54,026
One that is used all

615
00:20:54,046 --> 00:20:55,084
over the place in Silicon Valley,

616
00:20:56,039 --> 00:20:58,002
or in industry and in

617
00:20:58,030 --> 00:20:59,085
the Academia, to build many

618
00:21:00,011 --> 00:21:01,068
high performance machine learning system.
