
1
00:00:00,302 --> 00:00:01,883
In this video, we'll talk about

2
00:00:01,883 --> 00:00:03,948
the normal equation, which for

3
00:00:03,948 --> 00:00:05,660
some linear regression problems, will

4
00:00:05,660 --> 00:00:06,981
give us a much better way

5
00:00:06,981 --> 00:00:09,115
to solve for the optimal value

6
00:00:09,115 --> 00:00:10,879
of the parameters theta.

7
00:00:10,879 --> 00:00:13,096
Concretely, so far the

8
00:00:13,096 --> 00:00:14,399
algorithm that we've been using

9
00:00:14,399 --> 00:00:16,042
for linear regression is gradient

10
00:00:16,042 --> 00:00:17,823
descent where in order

11
00:00:17,823 --> 00:00:19,410
to minimize the cost function

12
00:00:19,410 --> 00:00:21,354
J of Theta, we would take

13
00:00:21,354 --> 00:00:23,792
this iterative algorithm that takes

14
00:00:23,792 --> 00:00:26,410
many steps, multiple iterations of

15
00:00:26,410 --> 00:00:28,259
gradient descent to converge

16
00:00:28,259 --> 00:00:30,396
to the global minimum.

17
00:00:30,396 --> 00:00:32,563
In contrast, the normal equation

18
00:00:32,563 --> 00:00:34,413
would give us a method to

19
00:00:34,413 --> 00:00:36,986
solve for theta analytically, so

20
00:00:36,986 --> 00:00:38,761
that rather than needing to run

21
00:00:38,761 --> 00:00:40,594
this iterative algorithm, we can

22
00:00:40,594 --> 00:00:41,365
instead just solve for the

23
00:00:41,365 --> 00:00:42,791
optimal value for theta

24
00:00:42,791 --> 00:00:44,403
all at one go, so that in

25
00:00:44,403 --> 00:00:46,096
basically one step you get

26
00:00:46,096 --> 00:00:48,136
to the optimal value right there.

27
00:00:49,136 --> 00:00:51,947
It turns out the normal equation

28
00:00:52,209 --> 00:00:54,442
that has some advantages and

29
00:00:54,442 --> 00:00:56,024
some disadvantages, but before

30
00:00:56,024 --> 00:00:57,817
we get to that and talk about

31
00:00:57,903 --> 00:00:59,426
when you should use it, let's

32
00:00:59,426 --> 00:01:02,539
get some intuition about what this method does.

33
00:01:02,539 --> 00:01:04,633
For this week's planetary example, let's

34
00:01:04,633 --> 00:01:06,120
imagine, let's take a

35
00:01:06,120 --> 00:01:07,505
very simplified cost function

36
00:01:07,505 --> 00:01:09,291
J of Theta, that's just the

37
00:01:09,291 --> 00:01:11,958
function of a real number Theta.

38
00:01:11,958 --> 00:01:13,642
So, for now, imagine that Theta

39
00:01:13,842 --> 00:01:16,615
is just a scalar value or that Theta is just a row value.

40
00:01:16,769 --> 00:01:18,918
It's just a number, rather than a vector.

41
00:01:19,171 --> 00:01:24,595
Imagine that we have a cost function J
that's a quadratic function of this real value

42
00:01:25,028 --> 00:01:27,420
parameter Theta, so J of Theta
looks like that.

43
00:01:27,851 --> 00:01:30,336
Well, how do you minimize
a quadratic function?

44
00:01:30,720 --> 00:01:32,745
For those of you that know
a little bit of calculus,

45
00:01:32,858 --> 00:01:34,965
you may know that the way to

46
00:01:34,965 --> 00:01:36,628
minimize a function is to

47
00:01:36,628 --> 00:01:38,991
take derivatives and to

48
00:01:38,991 --> 00:01:41,707
set derivatives equal to zero.

49
00:01:41,707 --> 00:01:44,721
So, you take the derivative of J
with respect to the parameter of Theta.

50
00:01:44,797 --> 00:01:46,847
You get some formula
which I am not going to derive,

51
00:01:46,847 --> 00:01:49,161
you set that derivative

52
00:01:49,161 --> 00:01:50,782
equal to zero, and this

53
00:01:50,782 --> 00:01:53,503
allows you to solve for

54
00:01:53,503 --> 00:01:57,866
the value of Theda that
minimizes J of Theta.

55
00:01:57,866 --> 00:01:59,096
That was a simpler case

56
00:01:59,096 --> 00:02:01,716
of when data was just real number.

57
00:02:01,716 --> 00:02:04,272
In the problem that we are
interested in, Theta is

58
00:02:04,929 --> 00:02:06,559
no longer just a real number,

59
00:02:06,559 --> 00:02:07,847
but, instead, is this

60
00:02:07,847 --> 00:02:11,986
n+1-dimensional parameter vector, and,

61
00:02:11,986 --> 00:02:13,809
a cost function J is

62
00:02:13,809 --> 00:02:15,742
a function of this vector

63
00:02:15,742 --> 00:02:17,501
value or Theta 0 through

64
00:02:17,501 --> 00:02:18,924
Theta m. And, a cost

65
00:02:18,924 --> 00:02:21,957
function looks like this,
some square cost function on the right.

66
00:02:22,373 --> 00:02:25,712
How do we minimize this cost function J?

67
00:02:25,712 --> 00:02:27,163
Calculus actually tells us

68
00:02:27,163 --> 00:02:29,377
that, if you, that

69
00:02:29,377 --> 00:02:30,709
one way to do so, is

70
00:02:30,709 --> 00:02:38,604
to take the partial derivative of J, with respect to every parameter of Theta J in turn, and then, to set

71
00:02:38,604 --> 00:02:40,271
all of these to 0.

72
00:02:40,271 --> 00:02:41,394
If you do that, and you

73
00:02:41,394 --> 00:02:42,718
solve for the values of

74
00:02:42,718 --> 00:02:44,000
Theta 0, Theta 1,

75
00:02:44,000 --> 00:02:45,973
up to Theta N, then,

76
00:02:45,973 --> 00:02:47,217
this would give you that values

77
00:02:47,217 --> 00:02:48,765
of Theta to minimize the cost

78
00:02:48,765 --> 00:02:50,878
function J.  Where, if

79
00:02:50,878 --> 00:02:52,176
you actually work through the

80
00:02:52,176 --> 00:02:53,597
calculus and work through

81
00:02:53,597 --> 00:02:55,194
the solution to the parameters

82
00:02:55,194 --> 00:02:57,316
Theta 0 through Theta N, the

83
00:02:57,316 --> 00:03:00,520
derivation ends up being somewhat involved.

84
00:03:00,520 --> 00:03:01,625
And, what I am going

85
00:03:01,625 --> 00:03:03,113
to do in this video,

86
00:03:03,113 --> 00:03:04,852
is actually to not go

87
00:03:04,852 --> 00:03:06,297
through the derivation, which is kind

88
00:03:06,297 --> 00:03:07,657
of long and kind of involved, but

89
00:03:07,657 --> 00:03:08,962
what I want to do is just

90
00:03:08,962 --> 00:03:10,545
tell you what you need to know

91
00:03:10,545 --> 00:03:12,619
in order to implement this process

92
00:03:12,619 --> 00:03:14,138
so you can solve for the

93
00:03:14,138 --> 00:03:15,511
values of the thetas that

94
00:03:15,511 --> 00:03:16,892
corresponds to where the

95
00:03:16,892 --> 00:03:19,273
partial derivatives is equal to zero.

96
00:03:19,273 --> 00:03:21,733
Or alternatively, or equivalently,

97
00:03:21,733 --> 00:03:23,357
the values of Theta is that

98
00:03:23,357 --> 00:03:25,901
minimize the cost function J of Theta.

99
00:03:25,901 --> 00:03:27,283
I realize that some of

100
00:03:27,283 --> 00:03:28,846
the comments I made that made

101
00:03:28,846 --> 00:03:29,914
more sense only to those

102
00:03:29,914 --> 00:03:31,896
of you that are normally familiar with calculus.

103
00:03:31,896 --> 00:03:33,065
So, but if you don't

104
00:03:33,065 --> 00:03:34,487
know, if you're less familiar

105
00:03:34,487 --> 00:03:36,354
with calculus, don't worry about it.

106
00:03:36,354 --> 00:03:37,404
I'm just going to tell you what

107
00:03:37,404 --> 00:03:38,374
you need to know in order to

108
00:03:38,374 --> 00:03:41,358
implement this algorithm and get it to work.

109
00:03:41,358 --> 00:03:42,585
For the example that I

110
00:03:42,585 --> 00:03:43,737
want to use as a running

111
00:03:43,737 --> 00:03:46,339
example let's say that

112
00:03:46,339 --> 00:03:49,056
I have m = 4 training examples.

113
00:03:50,409 --> 00:03:52,881
In order to implement this normal

114
00:03:52,881 --> 00:03:56,515
equation at big, what I'm going to do
is the following.

115
00:03:56,515 --> 00:03:57,640
I'm going to take my

116
00:03:57,640 --> 00:04:00,375
data set, so here are my four training examples.

117
00:04:00,375 --> 00:04:01,844
In this case let's assume that,

118
00:04:01,844 --> 00:04:06,073
you know, these four examples is all the data I have.

119
00:04:06,073 --> 00:04:07,890
What I am going to do is take

120
00:04:07,890 --> 00:04:09,007
my data set and add

121
00:04:09,007 --> 00:04:11,289
an extra column that corresponds

122
00:04:11,289 --> 00:04:14,579
to my extra feature, x0,

123
00:04:14,579 --> 00:04:15,967
that is always takes

124
00:04:15,967 --> 00:04:17,527
on this value of 1.

125
00:04:17,527 --> 00:04:18,681
What I'm going to do is

126
00:04:18,681 --> 00:04:19,943
I'm then going to construct

127
00:04:19,943 --> 00:04:22,638
a matrix called X that's

128
00:04:22,638 --> 00:04:24,632
a matrix are basically contains all

129
00:04:24,632 --> 00:04:26,100
of the features from my

130
00:04:26,100 --> 00:04:28,140
training data, so completely

131
00:04:28,140 --> 00:04:31,528
here is my here are

132
00:04:31,528 --> 00:04:33,743
all my features and we're

133
00:04:33,743 --> 00:04:34,797
going to take all those numbers and

134
00:04:34,797 --> 00:04:37,777
put them into this matrix "X", okay?

135
00:04:37,777 --> 00:04:39,179
So just, you know, copy

136
00:04:39,179 --> 00:04:41,233
the data over one column

137
00:04:41,233 --> 00:04:45,962
at a time and then I am going to do
something similar for y's.

138
00:04:45,962 --> 00:04:47,087
I am going to take the

139
00:04:47,087 --> 00:04:47,952
values that I'm trying to

140
00:04:47,952 --> 00:04:49,360
predict and construct now

141
00:04:49,360 --> 00:04:52,894
a vector, like so

142
00:04:52,894 --> 00:04:55,440
and call that a vector y.

143
00:04:55,440 --> 00:04:58,038
So X is going to be a

144
00:04:59,653 --> 00:05:05,688
m by (n+1) - dimensional matrix, and

145
00:05:05,688 --> 00:05:07,490
Y is going to be

146
00:05:07,490 --> 00:05:14,421
a m-dimensional vector

147
00:05:14,421 --> 00:05:16,624
where m is the number of
training examples

148
00:05:16,984 --> 00:05:18,688
and n is, n is

149
00:05:18,688 --> 00:05:20,713
a number of features, n+1, because of

150
00:05:20,713 --> 00:05:24,825
this extra feature X0 that I had.

151
00:05:24,825 --> 00:05:26,350
Finally if you take

152
00:05:26,350 --> 00:05:27,489
your matrix X and you take

153
00:05:27,489 --> 00:05:28,595
your vector Y, and if you

154
00:05:28,595 --> 00:05:31,065
just compute this, and set

155
00:05:31,065 --> 00:05:32,419
theta to be equal to

156
00:05:32,419 --> 00:05:34,440
X transpose X inverse times

157
00:05:34,440 --> 00:05:36,516
X transpose Y, this would

158
00:05:36,516 --> 00:05:38,583
give you the value of theta

159
00:05:38,583 --> 00:05:42,559
that minimizes your cost function.

160
00:05:42,559 --> 00:05:43,436
There was a lot

161
00:05:43,436 --> 00:05:44,416
that happened on the slides and

162
00:05:44,416 --> 00:05:47,514
I work through it using
one specific example of one dataset.

163
00:05:47,514 --> 00:05:49,241
Let me just write this

164
00:05:49,333 --> 00:05:50,770
out in a slightly more general form

165
00:05:50,955 --> 00:05:53,418
and then let me just,
and later on in

166
00:05:53,621 --> 00:05:56,531
this video let me explain
this equation a little bit more.

167
00:05:57,581 --> 00:06:00,687
It is not yet entirely clear how to do this.

168
00:06:00,687 --> 00:06:02,129
In a general case, let us

169
00:06:02,129 --> 00:06:04,124
say we have M training examples

170
00:06:04,124 --> 00:06:05,697
so X1, Y1 up to

171
00:06:05,697 --> 00:06:09,319
Xn, Yn and n features.

172
00:06:09,319 --> 00:06:10,811
So, each of the training example

173
00:06:10,811 --> 00:06:12,926
x(i) may looks like a vector

174
00:06:12,926 --> 00:06:16,297
like this, that is a n+1 dimensional feature vector.

175
00:06:16,943 --> 00:06:18,350
The way I'm going to construct the

176
00:06:18,350 --> 00:06:20,674
matrix "X", this is

177
00:06:20,674 --> 00:06:24,827
also called the design matrix

178
00:06:24,827 --> 00:06:26,712
is as follows.

179
00:06:26,712 --> 00:06:28,640
Each training example gives

180
00:06:28,640 --> 00:06:30,549
me a feature vector like this.

181
00:06:30,549 --> 00:06:34,491
say, sort of n+1 dimensional vector.

182
00:06:34,491 --> 00:06:36,190
The way I am going to construct my

183
00:06:36,359 --> 00:06:39,734
design matrix x is only construct the matrix like this.

184
00:06:39,734 --> 00:06:40,834
and what I'm going to

185
00:06:40,834 --> 00:06:42,109
do is take the first

186
00:06:42,109 --> 00:06:43,711
training example, so that's

187
00:06:43,711 --> 00:06:46,350
a vector, take its transpose

188
00:06:46,350 --> 00:06:48,692
so it ends up being this,

189
00:06:48,692 --> 00:06:50,250
you know, long flat thing and

190
00:06:50,250 --> 00:06:55,153
make x1 transpose the first row of my design matrix.

191
00:06:55,153 --> 00:06:56,225
Then I am going to take my

192
00:06:56,225 --> 00:06:58,682
second training example, x2, take

193
00:06:58,682 --> 00:07:00,437
the transpose of that and

194
00:07:00,437 --> 00:07:01,838
put that as the second row

195
00:07:01,838 --> 00:07:04,068
of x and so on,

196
00:07:04,068 --> 00:07:07,206
down until my last training example.

197
00:07:07,206 --> 00:07:09,279
Take the transpose of that,

198
00:07:09,279 --> 00:07:10,850
and that's my last row of

199
00:07:10,850 --> 00:07:12,665
my matrix X. And, so,

200
00:07:12,665 --> 00:07:14,418
that makes my matrix X, an

201
00:07:14,418 --> 00:07:17,129
M by N +1

202
00:07:17,129 --> 00:07:19,836
dimensional matrix.

203
00:07:19,836 --> 00:07:21,953
As a concrete example, let's

204
00:07:21,953 --> 00:07:23,505
say I have only one

205
00:07:23,505 --> 00:07:24,670
feature, really, only one

206
00:07:24,670 --> 00:07:26,631
feature other than X zero,

207
00:07:26,631 --> 00:07:28,165
which is always equal to 1.

208
00:07:28,165 --> 00:07:30,376
So if my feature vectors

209
00:07:30,376 --> 00:07:32,186
X-i are equal to this

210
00:07:32,186 --> 00:07:33,878
1, which is X-0, then

211
00:07:33,878 --> 00:07:35,912
some real feature, like maybe the

212
00:07:35,912 --> 00:07:37,662
size of the house, then my

213
00:07:37,662 --> 00:07:40,947
design matrix, X, would be equal to this.

214
00:07:40,947 --> 00:07:42,589
For the first row, I'm going

215
00:07:42,589 --> 00:07:46,071
to basically take this and take its transpose.

216
00:07:46,071 --> 00:07:51,644
So, I'm going to end up with 1, and then X-1-1.

217
00:07:51,644 --> 00:07:53,309
For the second row, we're going to end

218
00:07:53,309 --> 00:07:56,077
up with 1 and then

219
00:07:56,077 --> 00:07:58,046
X-1-2 and so

220
00:07:58,046 --> 00:07:59,046
on down to 1, and

221
00:07:59,046 --> 00:08:01,420
then X-1-M.

222
00:08:01,420 --> 00:08:03,084
And thus, this will be

223
00:08:03,084 --> 00:08:07,776
a m by 2-dimensional matrix.

224
00:08:07,776 --> 00:08:08,821
So, that's how to construct

225
00:08:08,821 --> 00:08:11,251
the matrix X. And, the

226
00:08:11,251 --> 00:08:13,886
vector Y--sometimes I might

227
00:08:13,886 --> 00:08:15,487
write an arrow on top to

228
00:08:15,487 --> 00:08:16,541
denote that it is a vector,

229
00:08:16,541 --> 00:08:19,871
but very often I'll just write this as Y, either way.

230
00:08:19,871 --> 00:08:21,182
The vector Y is obtained by

231
00:08:21,182 --> 00:08:23,275
taking all all the labels,

232
00:08:23,275 --> 00:08:25,098
all the correct prices of

233
00:08:25,098 --> 00:08:27,076
houses in my training set, and

234
00:08:27,076 --> 00:08:28,963
just stacking them up into

235
00:08:28,963 --> 00:08:32,011
an M-dimensional vector, and

236
00:08:32,011 --> 00:08:34,511
that's Y.  Finally, having

237
00:08:34,511 --> 00:08:36,724
constructed the matrix X

238
00:08:36,724 --> 00:08:38,184
and the vector Y, we then

239
00:08:38,184 --> 00:08:40,887
just compute theta as X'(1/X)

240
00:08:40,887 --> 00:08:47,243
x X'Y. I just

241
00:08:47,243 --> 00:08:49,356
want to make

242
00:08:49,356 --> 00:08:51,348
I just want to make sure that
this equation makes sense to you

243
00:08:51,348 --> 00:08:52,242
and that you know how to implement it.

244
00:08:52,242 --> 00:08:55,221
So, you know, concretely, what is this X'(1/X)?

245
00:08:55,221 --> 00:08:57,903
Well, X'(1/X) is the

246
00:08:57,903 --> 00:09:02,101
inverse of the matrix X'X.

247
00:09:02,101 --> 00:09:04,498
Concretely, if you were

248
00:09:04,498 --> 00:09:08,055
to say set A to

249
00:09:08,055 --> 00:09:11,120
be equal to X' x

250
00:09:11,120 --> 00:09:12,542
X, so X' is a

251
00:09:12,542 --> 00:09:14,063
matrix, X' x X

252
00:09:14,063 --> 00:09:15,305
gives you another matrix, and we

253
00:09:15,305 --> 00:09:17,560
call that matrix A. Then, you

254
00:09:17,560 --> 00:09:19,968
know, X'(1/X) is just

255
00:09:19,968 --> 00:09:22,352
you take this matrix A and you invert it, right!

256
00:09:23,245 --> 00:09:24,417
This gives, let's say 1/A.

257
00:09:26,025 --> 00:09:28,919
And so that's how you compute this thing.

258
00:09:28,919 --> 00:09:31,451
You compute X'X and then you compute its inverse.

259
00:09:31,451 --> 00:09:34,296
We haven't yet talked about Octave.

260
00:09:34,296 --> 00:09:35,941
We'll do so in the later

261
00:09:35,941 --> 00:09:37,211
set of videos, but in the

262
00:09:37,211 --> 00:09:39,073
Octave programming language or a

263
00:09:39,073 --> 00:09:40,652
similar view, and also the

264
00:09:40,652 --> 00:09:42,957
matlab programming language is very similar.

265
00:09:42,957 --> 00:09:46,937
The command to compute this quantity,

266
00:09:47,384 --> 00:09:50,326
X transpose X inverse times

267
00:09:50,326 --> 00:09:52,537
X transpose Y, is as follows.

268
00:09:52,537 --> 00:09:54,903
In Octave X prime is

269
00:09:54,903 --> 00:09:58,354
the notation that you use to denote X transpose.

270
00:09:58,354 --> 00:10:00,737
And so, this expression that's

271
00:10:00,737 --> 00:10:03,588
boxed in red, that's computing

272
00:10:03,588 --> 00:10:06,633
X transpose times X.

273
00:10:06,633 --> 00:10:08,551
pinv is a function for

274
00:10:08,551 --> 00:10:09,701
computing the inverse of

275
00:10:09,701 --> 00:10:11,818
a matrix, so this computes

276
00:10:11,818 --> 00:10:14,656
X transpose X inverse,

277
00:10:14,656 --> 00:10:16,453
and then you multiply that by

278
00:10:16,453 --> 00:10:18,267
X transpose, and you multiply

279
00:10:18,267 --> 00:10:19,712
that by Y. So you

280
00:10:19,712 --> 00:10:22,325
end computing that formula

281
00:10:22,325 --> 00:10:24,369
which I didn't prove,

282
00:10:24,369 --> 00:10:25,994
but it is possible to

283
00:10:25,994 --> 00:10:27,382
show mathematically even though I'm

284
00:10:27,382 --> 00:10:28,537
not going to do so

285
00:10:28,537 --> 00:10:31,071
here, that this formula gives you

286
00:10:31,071 --> 00:10:32,316
the optimal value of theta

287
00:10:32,316 --> 00:10:34,865
in the sense that if you set theta equal

288
00:10:34,865 --> 00:10:36,512
to this, that's the value

289
00:10:36,512 --> 00:10:38,000
of theta that minimizes the

290
00:10:38,000 --> 00:10:40,169
cost function J of theta

291
00:10:40,169 --> 00:10:41,993
for the new regression.

292
00:10:41,993 --> 00:10:44,530
One last detail in the earlier video.

293
00:10:44,530 --> 00:10:46,131
I talked about the feature

294
00:10:46,131 --> 00:10:47,061
skill and the idea of

295
00:10:47,061 --> 00:10:48,878
getting features to be

296
00:10:48,878 --> 00:10:50,726
on similar ranges of

297
00:10:50,726 --> 00:10:54,900
Scales of similar ranges of values of each other.

298
00:10:54,900 --> 00:10:56,872
If you are using this normal

299
00:10:56,872 --> 00:10:59,843
equation method then feature

300
00:10:59,843 --> 00:11:02,315
scaling isn't actually necessary

301
00:11:02,315 --> 00:11:04,361
and is actually okay if,

302
00:11:04,361 --> 00:11:06,094
say, some feature X one

303
00:11:06,094 --> 00:11:07,552
is between zero and one,

304
00:11:07,552 --> 00:11:08,846
and some feature X two is

305
00:11:08,846 --> 00:11:10,550
between ranges from zero to

306
00:11:10,550 --> 00:11:12,019
one thousand and some feature

307
00:11:12,019 --> 00:11:14,159
x three ranges from zero

308
00:11:14,159 --> 00:11:15,822
to ten to the

309
00:11:15,822 --> 00:11:17,263
minus five and if

310
00:11:17,263 --> 00:11:18,321
you are using the normal equation method

311
00:11:18,321 --> 00:11:20,296
this is okay and there is

312
00:11:20,296 --> 00:11:21,550
no need to do features

313
00:11:21,550 --> 00:11:22,740
scaling, although of course

314
00:11:22,740 --> 00:11:25,667
if you are using gradient descent,

315
00:11:25,667 --> 00:11:27,814
then, features scaling is still important.

316
00:11:28,030 --> 00:11:31,020
Finally, where should you use the gradient descent

317
00:11:31,020 --> 00:11:33,273
and when should you use the normal equation method.

318
00:11:33,273 --> 00:11:35,800
Here are some of the their advantages and disadvantages.

319
00:11:35,800 --> 00:11:38,305
Let's say you have m training

320
00:11:38,305 --> 00:11:40,918
examples and n features.

321
00:11:40,918 --> 00:11:42,854
One disadvantage of gradient descent

322
00:11:42,854 --> 00:11:46,015
is that, you need to choose the learning rate Alpha.

323
00:11:46,015 --> 00:11:47,374
And, often, this means running

324
00:11:47,374 --> 00:11:49,128
it few times with different learning

325
00:11:49,128 --> 00:11:51,154
rate alphas and then seeing what works best.

326
00:11:51,154 --> 00:11:54,274
And so that is sort of extra work and extra hassle.

327
00:11:54,274 --> 00:11:55,976
Another disadvantage with gradient descent

328
00:11:55,976 --> 00:11:57,841
is it needs many more iterations.

329
00:11:57,841 --> 00:11:59,346
So, depending on the details,

330
00:11:59,346 --> 00:12:00,839
that could make it slower, although

331
00:12:00,839 --> 00:12:04,391
there's more to the story as we'll see in a second.

332
00:12:04,391 --> 00:12:07,544
As for the normal equation, you don't need to
choose any learning rate alpha.

333
00:12:07,821 --> 00:12:11,208
So that, you know, makes it really convenient,
makes it simple to implement.

334
00:12:11,208 --> 00:12:13,888
You just run it and it usually just works.

335
00:12:13,888 --> 00:12:15,061
And you don't need to

336
00:12:15,061 --> 00:12:16,129
iterate, so, you don't need

337
00:12:16,129 --> 00:12:17,456
to plot J of Theta or

338
00:12:17,456 --> 00:12:20,497
check the convergence or take all those extra steps.

339
00:12:20,497 --> 00:12:21,931
So far, the balance seems to

340
00:12:21,931 --> 00:12:23,846
favor normal the normal equation.

341
00:12:24,826 --> 00:12:27,085
Here are some disadvantages of

342
00:12:27,612 --> 00:12:29,435
the normal equation, and some advantages of gradient descent.

343
00:12:29,681 --> 00:12:31,447
Gradient descent works pretty well,

344
00:12:31,928 --> 00:12:34,698
even when you have a very large number of features.

345
00:12:34,698 --> 00:12:36,168
So, even if you

346
00:12:36,168 --> 00:12:37,812
have millions of features you

347
00:12:37,812 --> 00:12:40,865
can run gradient descent and it will be reasonably efficient.

348
00:12:40,865 --> 00:12:43,381
It will do something reasonable.

349
00:12:43,381 --> 00:12:46,566
In contrast to normal equation, In, in

350
00:12:46,566 --> 00:12:48,014
order to solve for the parameters

351
00:12:48,014 --> 00:12:50,394
data, we need to solve for this term.

352
00:12:50,394 --> 00:12:53,058
We need to compute this term, X transpose, X inverse.

353
00:12:53,058 --> 00:12:56,328
This matrix X transpose X.

354
00:12:56,328 --> 00:13:00,206
That's an n by n matrix,
if you have n features.

355
00:13:00,770 --> 00:13:02,947
Because, if you look

356
00:13:02,947 --> 00:13:03,917
at the dimensions of

357
00:13:03,917 --> 00:13:05,529
X transpose the dimension of

358
00:13:05,529 --> 00:13:07,024
X, you multiply, figure out what

359
00:13:07,024 --> 00:13:08,749
the dimension of the product

360
00:13:08,749 --> 00:13:10,983
is, the matrix X transpose

361
00:13:10,983 --> 00:13:13,727
X is an n by n matrix where

362
00:13:13,727 --> 00:13:15,853
n is the number of features, and

363
00:13:15,853 --> 00:13:18,641
for almost computed implementations

364
00:13:18,641 --> 00:13:20,990
the cost of inverting

365
00:13:20,990 --> 00:13:23,087
the matrix, rose roughly as

366
00:13:23,087 --> 00:13:25,707
the cube of the dimension of the matrix.

367
00:13:25,707 --> 00:13:28,180
So, computing this inverse costs,

368
00:13:28,180 --> 00:13:29,964
roughly order, and cube time.

369
00:13:29,964 --> 00:13:31,213
Sometimes, it's slightly faster than

370
00:13:31,213 --> 00:13:35,050
N cube but, it's, you know, close enough
for our purposes.

371
00:13:35,489 --> 00:13:36,605
So if n the number of features
is very large,

372
00:13:37,643 --> 00:13:39,025
then computing this

373
00:13:39,025 --> 00:13:40,570
quantity can be slow and

374
00:13:40,570 --> 00:13:44,289
the normal equation method can actually be much slower.

375
00:13:44,289 --> 00:13:45,491
So if n is

376
00:13:45,491 --> 00:13:47,622
large then I might

377
00:13:47,622 --> 00:13:49,490
usually use gradient descent because

378
00:13:49,490 --> 00:13:51,872
we don't want to pay this all in q time.

379
00:13:51,872 --> 00:13:53,525
But, if n is relatively small,

380
00:13:53,525 --> 00:13:57,395
then the normal equation might give you a better way to solve the parameters.

381
00:13:57,395 --> 00:13:59,080
What does small and large mean?

382
00:13:59,080 --> 00:14:00,741
Well, if n is on

383
00:14:00,741 --> 00:14:02,130
the order of a hundred, then

384
00:14:02,130 --> 00:14:03,822
inverting a hundred-by-hundred matrix is

385
00:14:03,822 --> 00:14:06,539
no problem by modern computing standards.

386
00:14:06,539 --> 00:14:10,966
If n is a thousand, I would still use
the normal equation method.

387
00:14:10,966 --> 00:14:12,583
Inverting a thousand-by-thousand matrix is

388
00:14:12,583 --> 00:14:15,408
actually really fast on a modern computer.

389
00:14:15,408 --> 00:14:18,406
If n is ten thousand, then I might start to wonder.

390
00:14:18,406 --> 00:14:20,618
Inverting a ten-thousand-  by-ten-thousand matrix

391
00:14:20,618 --> 00:14:22,208
starts to get kind of slow,

392
00:14:22,208 --> 00:14:23,471
and I might then start to

393
00:14:23,471 --> 00:14:25,007
maybe lean in the

394
00:14:25,007 --> 00:14:27,007
direction of gradient descent, but maybe not quite.

395
00:14:27,114 --> 00:14:28,672
n equals ten thousand, you can

396
00:14:28,672 --> 00:14:31,148
sort of convert a ten-thousand-by-ten-thousand matrix.

397
00:14:31,148 --> 00:14:34,345
But if it gets much bigger than that, then,
I would probably use gradient descent.

398
00:14:34,345 --> 00:14:35,834
So, if n equals ten

399
00:14:35,834 --> 00:14:36,920
to the sixth with a million

400
00:14:36,920 --> 00:14:38,963
features, then inverting a

401
00:14:38,963 --> 00:14:41,565
million-by-million matrix is going

402
00:14:41,565 --> 00:14:42,631
to be very expensive, and

403
00:14:42,631 --> 00:14:46,163
I would definitely favor gradient descent if you have that many features.

404
00:14:46,163 --> 00:14:47,859
So exactly how large

405
00:14:47,859 --> 00:14:49,282
set of features has to be

406
00:14:49,282 --> 00:14:52,655
before you convert a gradient descent,
it's hard to give a strict number.

407
00:14:52,655 --> 00:14:53,855
But, for me, it is usually

408
00:14:53,855 --> 00:14:55,501
around ten thousand that I might

409
00:14:55,501 --> 00:14:58,258
start to consider switching over

410
00:14:58,335 --> 00:15:00,663
to gradient descents or maybe,

411
00:15:00,663 --> 00:15:04,324
some other algorithms that we'll talk about later in this class.

412
00:15:04,324 --> 00:15:05,765
To summarize, so long

413
00:15:05,765 --> 00:15:06,999
as the number of features is

414
00:15:06,999 --> 00:15:08,475
not too large, the normal equation

415
00:15:08,475 --> 00:15:12,229
gives us a great alternative method
to solve for the parameter theta.

416
00:15:12,583 --> 00:15:13,983
Concretely, so long as

417
00:15:13,983 --> 00:15:15,749
the number of features is less

418
00:15:15,749 --> 00:15:17,472
than 1000, you know, I would

419
00:15:17,472 --> 00:15:18,881
use, I would usually is used

420
00:15:18,881 --> 00:15:21,955
in normal equation method rather than, gradient descent.

421
00:15:21,955 --> 00:15:23,549
To preview some ideas that

422
00:15:23,549 --> 00:15:24,493
we'll talk about later in this

423
00:15:24,493 --> 00:15:26,235
course, as we get

424
00:15:26,235 --> 00:15:27,912
to the more complex learning algorithm, for

425
00:15:27,912 --> 00:15:29,617
example, when we talk about

426
00:15:29,617 --> 00:15:32,188
classification algorithm, like a logistic regression algorithm,

427
00:15:32,834 --> 00:15:34,319
We'll see that those algorithm

428
00:15:34,319 --> 00:15:35,467
actually...

429
00:15:35,467 --> 00:15:37,592
The normal  equation method
actually do not work

430
00:15:37,592 --> 00:15:39,388
for those more sophisticated

431
00:15:39,388 --> 00:15:41,190
learning algorithms, and, we

432
00:15:41,190 --> 00:15:43,916
will have to resort to gradient descent
for those algorithms.

433
00:15:43,916 --> 00:15:46,682
So, gradient descent is a very useful algorithm to know.

434
00:15:46,682 --> 00:15:48,859
The linear regression will have

435
00:15:48,982 --> 00:15:50,017
a large number of features and

436
00:15:50,017 --> 00:15:52,373
for some of the other algorithms

437
00:15:52,373 --> 00:15:53,893
that we'll see in

438
00:15:53,893 --> 00:15:55,438
this course, because, for them, the normal

439
00:15:55,438 --> 00:15:58,747
equation method just doesn't apply and doesn't work.

440
00:15:58,747 --> 00:16:00,537
But for this specific model of

441
00:16:00,537 --> 00:16:02,904
linear regression, the normal equation

442
00:16:02,904 --> 00:16:05,827
can give you a alternative

443
00:16:07,219 --> 00:16:08,612
that can be much faster, than gradient descent.

444
00:16:09,604 --> 00:16:11,920
So, depending on the detail of your algortithm,

445
00:16:12,007 --> 00:16:14,164
depending of the detail of the problems and

446
00:16:14,164 --> 00:16:15,550
how many features that you have,

447
00:16:15,550 --> 00:16:19,550
both of these algorithms are
well worth knowing about.
