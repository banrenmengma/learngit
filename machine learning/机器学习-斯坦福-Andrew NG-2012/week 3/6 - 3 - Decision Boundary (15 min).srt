
1
00:00:00,098 --> 00:00:02,006
In the last video, we talked

2
00:00:02,037 --> 00:00:05,050
about the hypothesis representation for logistic progression.

3
00:00:06,070 --> 00:00:07,061
What I'd like to do now is

4
00:00:07,092 --> 00:00:09,022
tell you about something called the

5
00:00:09,036 --> 00:00:11,016
decision boundary, and this

6
00:00:11,038 --> 00:00:12,031
will give us a better sense

7
00:00:12,086 --> 00:00:14,031
of what the logistic regression

8
00:00:15,002 --> 00:00:16,046
hypothesis function is computing.

9
00:00:17,078 --> 00:00:19,085
To recap, this is

10
00:00:19,098 --> 00:00:21,007
what we wrote out last time,

11
00:00:21,028 --> 00:00:22,053
where we said that the

12
00:00:22,062 --> 00:00:24,044
hypothesis is represented as

13
00:00:24,092 --> 00:00:25,094
H of X equals G of

14
00:00:26,010 --> 00:00:28,001
theta transpose X, where G

15
00:00:28,035 --> 00:00:29,060
is this function called the

16
00:00:29,085 --> 00:00:31,055
sigmoid function, which looks like this.

17
00:00:32,075 --> 00:00:34,027
So, it slowly increases from zero

18
00:00:35,010 --> 00:00:37,011
to one, asymptoting at one.

19
00:00:38,089 --> 00:00:39,092
What I want to do now is

20
00:00:40,035 --> 00:00:42,015
try to understand better when

21
00:00:42,043 --> 00:00:43,086
this hypothesis will make

22
00:00:44,007 --> 00:00:45,021
predictions that Y is

23
00:00:45,032 --> 00:00:46,088
equal to one versus when it

24
00:00:47,000 --> 00:00:48,010
might make predictions that Y

25
00:00:48,032 --> 00:00:50,010
is equal to zero and understand

26
00:00:50,063 --> 00:00:51,089
better what the hypothesis function

27
00:00:52,034 --> 00:00:55,032
looks like, particularly when we have more than one feature.

28
00:00:56,064 --> 00:00:58,088
Concretely, this hypothesis is

29
00:00:59,003 --> 00:01:00,071
out putting estimates of the

30
00:01:00,081 --> 00:01:01,095
probability that Y is

31
00:01:02,006 --> 00:01:03,074
equal to one given X is prime.

32
00:01:05,053 --> 00:01:06,050
So if we wanted to

33
00:01:06,078 --> 00:01:07,092
predict is Y equal to

34
00:01:08,015 --> 00:01:09,014
one or is Y equal

35
00:01:09,043 --> 00:01:11,031
to zero here's something we might do.

36
00:01:12,023 --> 00:01:14,043
When ever the hypothesis its that

37
00:01:14,068 --> 00:01:15,092
the problem with y being one

38
00:01:16,031 --> 00:01:17,034
is greater than or equal

39
00:01:17,056 --> 00:01:19,010
to 0.5 so this means

40
00:01:19,034 --> 00:01:20,093
that it is more likely to

41
00:01:21,001 --> 00:01:22,007
be y equals one than y

42
00:01:22,023 --> 00:01:25,054
equals zero then let's predict Y equals one.

43
00:01:26,040 --> 00:01:27,045
And otherwise, if the probability

44
00:01:27,095 --> 00:01:30,003
of, the estimated probability of

45
00:01:30,018 --> 00:01:31,056
Y being one is less

46
00:01:31,082 --> 00:01:34,020
than 0.5, then let's predict Y equals zero.

47
00:01:35,000 --> 00:01:35,096
And I chose a greater

48
00:01:36,026 --> 00:01:38,062
than or equal to here and less than here.

49
00:01:39,067 --> 00:01:40,060
If H of X is equal

50
00:01:40,098 --> 00:01:42,070
to 0.5 exactly, then

51
00:01:43,006 --> 00:01:44,067
we could predict positive or

52
00:01:44,067 --> 00:01:45,062
negative vector but a put a

53
00:01:45,075 --> 00:01:46,087
great deal on to here

54
00:01:47,045 --> 00:01:48,062
so we default maybe to predicting

55
00:01:49,020 --> 00:01:51,009
a positive if your

56
00:01:51,045 --> 00:01:52,053
vector is 0.5 but that's

57
00:01:52,079 --> 00:01:54,065
a detail that really doesn't matter that much.

58
00:01:56,068 --> 00:01:57,065
What I want to do is understand

59
00:01:58,014 --> 00:01:59,015
better when it is

60
00:01:59,025 --> 00:02:01,025
exactly that H of

61
00:02:01,034 --> 00:02:02,059
X will be greater or equal

62
00:02:02,096 --> 00:02:04,042
to 0.5, so that

63
00:02:04,065 --> 00:02:07,004
we end up predicting Y is equal to one.

64
00:02:09,053 --> 00:02:10,090
If we look at this plot

65
00:02:11,053 --> 00:02:13,046
of the sigmoid function, we'll notice

66
00:02:14,018 --> 00:02:15,050
that the sigmoid function, G

67
00:02:17,009 --> 00:02:18,071
of Z, is greater than

68
00:02:18,097 --> 00:02:20,044
or equal to 0.5

69
00:02:21,003 --> 00:02:23,021
whenever Z is

70
00:02:24,030 --> 00:02:25,040
greater than or equal to zero.

71
00:02:25,097 --> 00:02:28,003
So is in this half of

72
00:02:28,015 --> 00:02:29,061
the figure that, G takes

73
00:02:29,090 --> 00:02:32,003
on values that are 0.5 and higher.

74
00:02:32,050 --> 00:02:33,099
This is not clear, that's the 0.5.

75
00:02:34,046 --> 00:02:35,084
So when Z is

76
00:02:35,096 --> 00:02:37,086
positive, G of Z,

77
00:02:38,034 --> 00:02:40,099
the sigmoid function, is greater than or equal to 0.5.

78
00:02:41,090 --> 00:02:43,094
Since the hypothesis for

79
00:02:44,021 --> 00:02:46,009
logistic regression is H of

80
00:02:46,038 --> 00:02:48,031
X equals G of theta

81
00:02:48,047 --> 00:02:50,078
transpose X. This is

82
00:02:50,094 --> 00:02:51,084
therefore going to be greater

83
00:02:52,018 --> 00:02:53,080
than or equal to 0.5

84
00:02:54,033 --> 00:02:57,075
whenever theta transpose

85
00:02:58,034 --> 00:03:00,015
X is greater than or equal to zero.

86
00:03:01,059 --> 00:03:03,003
So what was shown, right,

87
00:03:03,034 --> 00:03:05,021
because here theta transpose X

88
00:03:05,080 --> 00:03:06,078
takes the row of Z.

89
00:03:08,012 --> 00:03:09,041
So what we're shown is that

90
00:03:09,053 --> 00:03:10,096
our hypothesis is going

91
00:03:11,006 --> 00:03:12,053
to predict Y equals one

92
00:03:13,019 --> 00:03:14,099
whenever theta transpose X

93
00:03:15,038 --> 00:03:16,093
is greater than or equal to 0.

94
00:03:17,087 --> 00:03:19,093
Let's now consider the other

95
00:03:20,000 --> 00:03:21,075
case of when a hypothesis

96
00:03:22,031 --> 00:03:24,028
will predict Y is equal to 0.

97
00:03:25,003 --> 00:03:27,000
Well, by similar argument, H

98
00:03:27,021 --> 00:03:28,068
of X is going to be

99
00:03:28,096 --> 00:03:30,034
less than 0.5 whenever G

100
00:03:30,072 --> 00:03:31,099
of Z is less than

101
00:03:32,018 --> 00:03:34,044
0.5 because the range

102
00:03:34,071 --> 00:03:36,033
of values of Z that

103
00:03:36,047 --> 00:03:37,088
calls Z to take on

104
00:03:38,002 --> 00:03:40,097
values less that 0.5, well that's when Z is negative.

105
00:03:42,055 --> 00:03:44,043
So when G of Z is less than 0.5.

106
00:03:44,090 --> 00:03:46,068
Our hypothesis will predict

107
00:03:46,086 --> 00:03:48,040
that Y is equal to zero, and

108
00:03:48,087 --> 00:03:50,034
by similar argument to what

109
00:03:50,050 --> 00:03:52,046
we had earlier, H of

110
00:03:52,056 --> 00:03:53,077
X is equal G of

111
00:03:54,031 --> 00:03:56,063
theta transpose X. And

112
00:03:56,091 --> 00:03:58,037
so, we'll predict Y equals

113
00:03:58,066 --> 00:04:00,025
zero whenever this quantity

114
00:04:00,097 --> 00:04:03,043
theta transpose X is less than zero.

115
00:04:04,093 --> 00:04:06,040
To summarize what we just

116
00:04:06,046 --> 00:04:08,006
worked out, we saw that if

117
00:04:08,034 --> 00:04:09,066
we decide to predict whether

118
00:04:09,090 --> 00:04:10,080
Y is equal to one or

119
00:04:11,006 --> 00:04:12,015
Y is equal to zero,

120
00:04:12,040 --> 00:04:13,074
depending on whether the estimated

121
00:04:14,019 --> 00:04:15,050
probability is greater than

122
00:04:15,075 --> 00:04:17,023
or equal 0.5, or whether

123
00:04:17,081 --> 00:04:19,011
it's less than 0.5, then

124
00:04:19,060 --> 00:04:20,072
that's the same as saying that

125
00:04:20,087 --> 00:04:22,072
will predict Y equals 1

126
00:04:22,087 --> 00:04:24,037
whenever theta transpose axis greater

127
00:04:25,000 --> 00:04:25,091
than or equal to 0,

128
00:04:25,097 --> 00:04:27,060
and we will predict Y is

129
00:04:27,075 --> 00:04:29,041
equal to zero whenever theta transpose X

130
00:04:29,099 --> 00:04:31,000
is less than zero.

131
00:04:32,093 --> 00:04:33,089
Let's use this to better

132
00:04:34,017 --> 00:04:36,006
understand how the hypothesis

133
00:04:36,088 --> 00:04:38,086
of logistic regression makes those predictions.

134
00:04:40,004 --> 00:04:41,031
Now, let's suppose we have

135
00:04:41,051 --> 00:04:42,075
a training set like that shown

136
00:04:43,006 --> 00:04:44,075
on the slide, and suppose

137
00:04:45,011 --> 00:04:47,013
our hypothesis is H of

138
00:04:47,023 --> 00:04:48,033
X equals G of theta

139
00:04:48,066 --> 00:04:50,006
zero, plus theta one X1

140
00:04:50,025 --> 00:04:51,067
plus theta two X2.

141
00:04:52,085 --> 00:04:54,026
We haven't talked yet about how

142
00:04:54,050 --> 00:04:55,091
to fit the parameters of this model.

143
00:04:56,070 --> 00:04:58,033
We'll talk about that in the next video.

144
00:04:59,031 --> 00:05:00,097
But suppose that variable procedure

145
00:05:01,068 --> 00:05:03,036
to be specified, we end

146
00:05:03,054 --> 00:05:05,043
up choosing the following values for the parameters.

147
00:05:06,018 --> 00:05:07,039
Let's say we choose theta zero

148
00:05:07,083 --> 00:05:09,035
equals three, theta one

149
00:05:09,069 --> 00:05:12,002
equals one, theta two equals one.

150
00:05:13,054 --> 00:05:14,080
So this means that my parameter

151
00:05:15,038 --> 00:05:16,050
vector is going to be

152
00:05:17,022 --> 00:05:20,061
theta equals minus 311.

153
00:05:24,013 --> 00:05:26,063
So, we're given this

154
00:05:27,006 --> 00:05:28,089
choice of my hypothesis parameters,

155
00:05:30,011 --> 00:05:31,088
let's try to figure out where

156
00:05:32,027 --> 00:05:33,063
a hypothesis will end up

157
00:05:33,075 --> 00:05:35,033
predicting y equals 1 and where it

158
00:05:35,043 --> 00:05:37,004
will end up predicting y equals 0.

159
00:05:39,006 --> 00:05:40,043
Using the formulas that we

160
00:05:40,062 --> 00:05:42,047
worked on the previous slide, we know

161
00:05:42,089 --> 00:05:44,037
that Y equals 1 is

162
00:05:44,050 --> 00:05:45,068
more likely, that is the

163
00:05:45,076 --> 00:05:46,081
probability that Y equals

164
00:05:47,039 --> 00:05:48,079
1 is greater than 0.5

165
00:05:48,094 --> 00:05:50,080
or greater than or equal to 0.5.

166
00:05:51,056 --> 00:05:54,000
Whenever theta transpose x

167
00:05:55,024 --> 00:05:55,094
is greater than zero.

168
00:05:57,023 --> 00:05:58,056
And this formula that I

169
00:05:58,068 --> 00:06:00,060
just underlined minus three

170
00:06:00,085 --> 00:06:02,031
plus X1 plus X2 is,

171
00:06:03,000 --> 00:06:04,056
of course, theta transpose

172
00:06:05,022 --> 00:06:06,062
X when theta is equal

173
00:06:07,000 --> 00:06:09,006
to this value of the parameters

174
00:06:09,075 --> 00:06:10,063
that we just chose.

175
00:06:12,041 --> 00:06:14,043
So, for any example, for

176
00:06:14,062 --> 00:06:16,014
any example with features X1

177
00:06:16,039 --> 00:06:18,069
and X2 that satisfy this

178
00:06:19,030 --> 00:06:21,007
equation that minus 3

179
00:06:21,017 --> 00:06:22,044
plus X1 plus X2

180
00:06:23,052 --> 00:06:24,056
is greater than or equal to 0.

181
00:06:24,067 --> 00:06:26,029
Our hypothesis will think

182
00:06:27,000 --> 00:06:28,001
that Y equals 1 is

183
00:06:28,006 --> 00:06:30,031
more likely, or will predict that Y is equal to one.

184
00:06:32,043 --> 00:06:34,008
We can also take minus three

185
00:06:34,049 --> 00:06:35,025
and bring this to the right

186
00:06:35,075 --> 00:06:37,055
and rewrite this as X1

187
00:06:37,074 --> 00:06:41,001
plus X2 is greater than or equal to three.

188
00:06:41,041 --> 00:06:43,019
And so, equivalently, we found

189
00:06:43,058 --> 00:06:44,098
that this hypothesis will predict

190
00:06:45,081 --> 00:06:47,010
Y equals one whenever X1

191
00:06:47,050 --> 00:06:49,087
plus X2 is greater than or equal to three.

192
00:06:51,087 --> 00:06:53,037
Let's see what that means on the figure.

193
00:06:54,087 --> 00:06:56,004
If I write down the equation,

194
00:06:57,017 --> 00:06:59,031
X1 plus X2 equals three,

195
00:07:00,023 --> 00:07:02,050
this defines the equation of a straight line.

196
00:07:03,036 --> 00:07:04,063
And if I draw what that straight

197
00:07:05,000 --> 00:07:07,048
line looks like, it gives

198
00:07:07,073 --> 00:07:09,024
me the following line which passes

199
00:07:10,008 --> 00:07:11,031
through 3 and 3 on

200
00:07:11,056 --> 00:07:13,010
the X1 and the X2 axis.

201
00:07:16,024 --> 00:07:17,025
So the part of the input space,

202
00:07:17,026 --> 00:07:18,051
the part of the

203
00:07:18,076 --> 00:07:20,080
X1, X2 plane that corresponds

204
00:07:21,055 --> 00:07:24,018
to when X1 plus X2 is greater than or equal to three.

205
00:07:24,087 --> 00:07:26,098
That's going to be this very top plane.

206
00:07:27,020 --> 00:07:29,014
That is everything to the

207
00:07:29,037 --> 00:07:30,043
up, and everything to the upper

208
00:07:30,064 --> 00:07:32,093
right portion of this magenta line that I just drew.

209
00:07:34,008 --> 00:07:35,039
And so, the region where our

210
00:07:35,061 --> 00:07:36,088
hypothesis will predict Y

211
00:07:37,006 --> 00:07:38,013
equals 1 is this

212
00:07:38,032 --> 00:07:39,089
region, you know, is

213
00:07:40,000 --> 00:07:41,031
really this huge region, this

214
00:07:41,062 --> 00:07:43,067
half-space over to the upper right.

215
00:07:44,038 --> 00:07:45,016
And let me just write that down.

216
00:07:45,043 --> 00:07:46,098
I'm gonna call this the Y

217
00:07:47,032 --> 00:07:50,011
equals one region, and in

218
00:07:50,025 --> 00:07:52,017
contrast the region where

219
00:07:54,029 --> 00:07:55,089
X1 plus X2 is

220
00:07:56,050 --> 00:07:58,023
less than three, that's when

221
00:07:58,062 --> 00:08:00,008
we will predict that Y,

222
00:08:00,011 --> 00:08:01,085
Y is equal to zero, and

223
00:08:01,097 --> 00:08:03,048
that corresponds to this region.

224
00:08:04,070 --> 00:08:05,085
You know, itt's really a half-plane, but

225
00:08:06,007 --> 00:08:07,073
that region on the left is

226
00:08:08,049 --> 00:08:11,029
the region where our hypothesis predict Y equals 0.

227
00:08:11,074 --> 00:08:13,017
I want to give

228
00:08:13,043 --> 00:08:15,066
this line, this magenta line that I drew a name.

229
00:08:16,045 --> 00:08:18,092
This line there is called

230
00:08:19,043 --> 00:08:21,087
the decision boundary.

231
00:08:24,057 --> 00:08:26,047
And concretely, this straight line

232
00:08:27,006 --> 00:08:28,032
X1 plus X equals 3.

233
00:08:28,047 --> 00:08:30,074
That corresponds to the set of points.

234
00:08:31,016 --> 00:08:32,035
So that corresponds to the region

235
00:08:33,030 --> 00:08:34,033
where H of X is equal

236
00:08:34,057 --> 00:08:36,076
to 0.5 exactly and

237
00:08:36,097 --> 00:08:38,047
the decision boundary, that is

238
00:08:38,075 --> 00:08:40,062
this straight line, that's the

239
00:08:40,072 --> 00:08:42,027
line that separates the region

240
00:08:42,075 --> 00:08:44,028
where the hypothesis predicts Y equals

241
00:08:44,064 --> 00:08:45,067
one from the region

242
00:08:46,042 --> 00:08:49,004
where the hypothesis predicts that Y is equal to 0.

243
00:08:49,074 --> 00:08:50,087
And just to be clear.

244
00:08:51,038 --> 00:08:53,023
The decision boundary is a

245
00:08:53,034 --> 00:08:54,069
property of the hypothesis

246
00:08:57,042 --> 00:09:00,028
including the parameters theta 0, theta 1, theta 2.

247
00:09:00,072 --> 00:09:02,097
And in the figure I drew a training set.

248
00:09:03,024 --> 00:09:05,044
I drew a data set in order to help the visualization.

249
00:09:06,048 --> 00:09:07,046
But even if we take

250
00:09:07,072 --> 00:09:09,020
away the data set, you know

251
00:09:09,027 --> 00:09:11,000
decision boundary and a

252
00:09:11,007 --> 00:09:12,012
region where we predict Y

253
00:09:12,029 --> 00:09:13,065
equals 1 versus Y equals zero.

254
00:09:14,032 --> 00:09:15,038
That's a property of the

255
00:09:15,046 --> 00:09:16,070
hypothesis and of the

256
00:09:16,078 --> 00:09:18,022
parameters of the hypothesis, and

257
00:09:18,082 --> 00:09:20,044
not a property of the data set.

258
00:09:22,013 --> 00:09:23,033
Later on, of course, we'll talk

259
00:09:23,055 --> 00:09:24,055
about how to fit the

260
00:09:24,064 --> 00:09:26,046
parameters and there we'll

261
00:09:26,062 --> 00:09:27,092
end up using the training set,

262
00:09:28,022 --> 00:09:31,003
or using our data, to determine the value of the parameters.

263
00:09:32,052 --> 00:09:33,095
But once we have particular values

264
00:09:34,051 --> 00:09:36,092
for the parameters: theta 0, theta 1, theta 2.

265
00:09:37,028 --> 00:09:39,002
Then that completely defines

266
00:09:39,059 --> 00:09:41,046
the decision boundary and we

267
00:09:41,066 --> 00:09:42,061
don't actually need to plot

268
00:09:43,011 --> 00:09:44,040
a training set in order

269
00:09:44,085 --> 00:09:46,014
to plot the decision boundary.

270
00:09:49,062 --> 00:09:50,035
Let's now look at a more

271
00:09:50,057 --> 00:09:52,025
complex example where, as

272
00:09:52,041 --> 00:09:53,087
usual, I have crosses to

273
00:09:54,003 --> 00:09:55,063
denote my positive examples and

274
00:09:55,091 --> 00:09:57,025
O's to denote my negative examples.

275
00:09:58,088 --> 00:10:00,004
Given a training set like this,

276
00:10:00,071 --> 00:10:01,095
how can I get logistic regression

277
00:10:02,089 --> 00:10:04,032
to fit this sort of data?

278
00:10:05,052 --> 00:10:06,083
Earlier, when we were talking about

279
00:10:07,012 --> 00:10:08,069
polynomial regression or when

280
00:10:09,003 --> 00:10:10,066
we're linear regression, we talked

281
00:10:10,096 --> 00:10:12,012
about how we can add extra

282
00:10:12,052 --> 00:10:14,050
higher order polynomial terms to the features.

283
00:10:15,051 --> 00:10:17,051
And we can do the same for logistic regression.

284
00:10:18,090 --> 00:10:21,062
Concretely, let's say my hypothesis looks like this.

285
00:10:22,022 --> 00:10:23,037
Where I've added two extra

286
00:10:23,070 --> 00:10:26,062
features, X1 squared and X2 squared, to my features.

287
00:10:27,069 --> 00:10:28,095
So that I now have 5 parameters,

288
00:10:29,078 --> 00:10:31,023
theta 0 through theta 4.

289
00:10:32,077 --> 00:10:34,046
As before, we'll defer to

290
00:10:34,085 --> 00:10:36,065
the next video our discussion

291
00:10:37,041 --> 00:10:38,091
on how to automatically choose

292
00:10:39,021 --> 00:10:41,085
values for the parameters theta 0 through theta 4.

293
00:10:42,050 --> 00:10:43,064
But let's say that

294
00:10:44,024 --> 00:10:46,002
very procedure to be specified,

295
00:10:46,066 --> 00:10:48,088
I end up choosing theta 0

296
00:10:49,021 --> 00:10:50,067
equals minus 1, theta 1

297
00:10:51,028 --> 00:10:52,046
equals 0, theta 2

298
00:10:52,091 --> 00:10:55,008
equals 0, theta 3 equals

299
00:10:55,064 --> 00:10:57,087
1, and theta 4 equals 1.

300
00:10:58,000 --> 00:10:59,089
What this means

301
00:11:00,021 --> 00:11:01,052
is that with this particular choice

302
00:11:02,014 --> 00:11:03,072
of parameters, my parameter

303
00:11:04,053 --> 00:11:07,012
vector theta looks like minus 1, 0, 0, 1, 1.

304
00:11:10,054 --> 00:11:12,011
Following our earlier discussion, this

305
00:11:12,035 --> 00:11:14,007
means that my hypothesis will predict

306
00:11:14,037 --> 00:11:16,010
that Y is equal to 1

307
00:11:16,037 --> 00:11:17,088
whenever minus 1 plus X1

308
00:11:18,024 --> 00:11:20,075
squared plus X2 squared is greater than or equal to 0.

309
00:11:21,002 --> 00:11:23,000
This is whenever theta transpose

310
00:11:24,012 --> 00:11:25,066
times my theta transpose

311
00:11:26,035 --> 00:11:28,062
my features is greater than or equal to 0.

312
00:11:30,005 --> 00:11:31,026
And if I take minus

313
00:11:31,069 --> 00:11:32,069
1 and just bring this to

314
00:11:32,089 --> 00:11:34,037
the right, I'm saying that

315
00:11:34,077 --> 00:11:36,035
my hypothesis will predict that

316
00:11:36,062 --> 00:11:38,010
Y is equal to 1

317
00:11:38,012 --> 00:11:40,027
whenever X1 squared plus

318
00:11:40,071 --> 00:11:43,050
X2 squared is greater than or equal to 1.

319
00:11:43,058 --> 00:11:46,099
So, what does decision boundary look like?

320
00:11:47,094 --> 00:11:49,065
Well, if you were to plot the

321
00:11:49,077 --> 00:11:51,049
curve for X1 squared plus

322
00:11:51,089 --> 00:11:53,045
X2 squared equals 1.

323
00:11:53,064 --> 00:11:55,007
Some of you will

324
00:11:55,050 --> 00:11:58,011
that is the equation for

325
00:11:58,028 --> 00:12:00,045
a circle of radius

326
00:12:01,028 --> 00:12:03,000
1 centered around the origin.

327
00:12:04,014 --> 00:12:06,028
So, that is my decision boundary.

328
00:12:10,040 --> 00:12:12,019
And everything outside the

329
00:12:12,025 --> 00:12:13,029
circle I'm going to predict

330
00:12:14,017 --> 00:12:15,022
as Y equals 1.

331
00:12:15,038 --> 00:12:17,046
So out here is, you know, my

332
00:12:17,070 --> 00:12:19,002
Y equals 1 region.

333
00:12:19,036 --> 00:12:21,097
I'm going to predict Y equals 1 out here.

334
00:12:22,066 --> 00:12:24,011
And inside the circle is where

335
00:12:24,030 --> 00:12:26,053
I'll predict Y is equal to 0.

336
00:12:27,078 --> 00:12:29,086
So, by adding these more

337
00:12:30,002 --> 00:12:32,084
complex or these polynomial terms to my features as well.

338
00:12:33,015 --> 00:12:34,061
I can get more complex decision

339
00:12:35,001 --> 00:12:36,014
boundaries that don't just

340
00:12:36,050 --> 00:12:38,096
try to separate the positive and negative examples of a straight line.

341
00:12:39,054 --> 00:12:40,040
I can get in this example

342
00:12:41,029 --> 00:12:42,091
a decision boundary that's a circle.

343
00:12:44,019 --> 00:12:45,040
Once again the decision boundary

344
00:12:46,000 --> 00:12:47,075
is a property not of

345
00:12:47,086 --> 00:12:50,055
the training set, but of the hypothesis and of the parameters.

346
00:12:51,063 --> 00:12:52,089
So long as we've

347
00:12:53,007 --> 00:12:54,041
given my parameter vector theta,

348
00:12:55,037 --> 00:12:56,076
that defines the decision

349
00:12:57,014 --> 00:12:58,026
boundary which is the circle.

350
00:12:59,021 --> 00:13:02,015
But the training set is not what we use to define decision boundary.

351
00:13:03,004 --> 00:13:06,000
The training set may be used to fit the parameters theta.

352
00:13:06,054 --> 00:13:07,074
We'll talk about how to do that later.

353
00:13:08,061 --> 00:13:09,072
But once you have the

354
00:13:09,080 --> 00:13:13,010
parameters theta, that is what defines the decision boundary.

355
00:13:13,062 --> 00:13:15,064
Let me put back the training set

356
00:13:16,039 --> 00:13:17,008
just for visualization.

357
00:13:18,051 --> 00:13:20,059
And finally, let's look at a more complex example.

358
00:13:22,032 --> 00:13:23,017
So can we come up

359
00:13:23,029 --> 00:13:25,039
with even more complex decision boundaries and this?

360
00:13:26,052 --> 00:13:28,008
If I have even higher

361
00:13:28,041 --> 00:13:30,074
order polynomial terms, so things

362
00:13:31,012 --> 00:13:34,028
like X1 squared, X1

363
00:13:34,047 --> 00:13:36,001
squared X2, X1 squared

364
00:13:36,060 --> 00:13:37,054
X2 squared, and so on.

365
00:13:37,076 --> 00:13:38,071
If I have much higher order

366
00:13:38,098 --> 00:13:41,019
polynomials, then it's possible

367
00:13:41,055 --> 00:13:42,061
to show that you can get

368
00:13:42,085 --> 00:13:44,033
even more complex decision boundaries and

369
00:13:45,022 --> 00:13:46,084
logistic regression can be

370
00:13:46,092 --> 00:13:48,008
used to find the zero boundaries

371
00:13:48,050 --> 00:13:49,097
that may, for example, be

372
00:13:50,009 --> 00:13:51,087
an ellipse like that, or

373
00:13:52,008 --> 00:13:53,037
maybe with a different setting of

374
00:13:53,048 --> 00:13:55,033
the parameters, maybe you

375
00:13:55,041 --> 00:13:57,064
can get instead a different decision boundary that

376
00:13:57,084 --> 00:13:59,050
may even look like, you know, some funny

377
00:13:59,076 --> 00:14:03,092
shape like that.

378
00:14:04,008 --> 00:14:05,029
Or for even more complex examples

379
00:14:06,033 --> 00:14:07,050
you can also get decision boundaries

380
00:14:08,095 --> 00:14:10,012
that can look like, you know,

381
00:14:10,038 --> 00:14:11,062
more complex shapes like that.

382
00:14:12,003 --> 00:14:13,014
Where everything in here you

383
00:14:13,033 --> 00:14:14,092
predict Y equals 1, and

384
00:14:15,039 --> 00:14:16,087
everything outside you predict Y equals 0.

385
00:14:17,051 --> 00:14:18,090
So these higher order polynomial

386
00:14:19,055 --> 00:14:21,083
features you can get very complex decision boundaries.

387
00:14:23,007 --> 00:14:24,071
So with these visualizations, I

388
00:14:24,076 --> 00:14:26,000
hope that gives you a

389
00:14:26,013 --> 00:14:27,089
what's the range of hypothesis

390
00:14:28,060 --> 00:14:30,023
functions we can represent using

391
00:14:30,064 --> 00:14:33,012
the representation that we have for logistic regression.

392
00:14:34,087 --> 00:14:36,059
Now that we know what H of X can represent.

393
00:14:37,069 --> 00:14:38,082
What I'd like to do next in

394
00:14:38,095 --> 00:14:40,033
the following video is talk

395
00:14:40,052 --> 00:14:43,025
about how to automatically choose the parameters theta.

396
00:14:44,011 --> 00:14:45,037
So that given a training

397
00:14:45,054 --> 00:14:48,036
set we can automatically fit the parameters to our data.
