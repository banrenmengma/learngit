
1
00:00:01,037 --> 00:00:02,041
In the last video, we talked

2
00:00:02,074 --> 00:00:04,020
about the recommender system problem,

3
00:00:05,003 --> 00:00:06,026
where for example, you may

4
00:00:06,037 --> 00:00:07,080
have a set of movies and you

5
00:00:07,094 --> 00:00:09,014
may have a set of users,

6
00:00:09,081 --> 00:00:10,096
each of whom has rated

7
00:00:11,067 --> 00:00:13,016
some subset of the movies,

8
00:00:13,036 --> 00:00:14,033
rated the movies 1 to

9
00:00:14,050 --> 00:00:15,046
5 stars or 0 to 5

10
00:00:15,063 --> 00:00:16,082
stars, and what I would like

11
00:00:17,019 --> 00:00:18,017
to do, is look at

12
00:00:18,023 --> 00:00:19,071
these users and predict how

13
00:00:19,091 --> 00:00:22,053
they would have rated other movies that they have not yet rated.

14
00:00:23,053 --> 00:00:24,053
In this video, I would like

15
00:00:24,060 --> 00:00:25,094
to talk about our first approach

16
00:00:26,042 --> 00:00:28,019
to building a recommender system, this

17
00:00:28,035 --> 00:00:30,010
approach is called content based recommendations.

18
00:00:31,046 --> 00:00:32,068
Here's our data set from before,

19
00:00:33,031 --> 00:00:34,046
and just to remind you of a

20
00:00:34,054 --> 00:00:35,078
bit of notation, I was using

21
00:00:36,068 --> 00:00:37,086
Nu to denote the number

22
00:00:38,003 --> 00:00:39,010
of users, and so that's equal

23
00:00:39,028 --> 00:00:40,099
to 4, and Nm

24
00:00:41,099 --> 00:00:44,078
to denote the number of movies, I have five movies.

25
00:00:47,022 --> 00:00:48,014
So, how do I predict

26
00:00:48,096 --> 00:00:50,095
what these missing values would be?

27
00:00:52,049 --> 00:00:53,052
Let's suppose that for each

28
00:00:53,070 --> 00:00:55,050
of these movies, I have a

29
00:00:55,053 --> 00:00:57,046
set of features for them.

30
00:00:57,090 --> 00:00:58,099
In particular, lets say that

31
00:00:59,068 --> 00:01:00,085
for each of the movies I have two features,

32
00:01:01,092 --> 00:01:03,050
which I'm going to denote X1 and

33
00:01:04,007 --> 00:01:05,070
X2, where X1 measures the degree

34
00:01:06,012 --> 00:01:07,045
to which a movie is a

35
00:01:07,065 --> 00:01:09,026
romantic movie and X2 measures

36
00:01:09,081 --> 00:01:12,007
the degree to which a movie is an action movie.

37
00:01:12,084 --> 00:01:13,070
So if you take a movie

38
00:01:14,046 --> 00:01:16,048
Love at last, you know,

39
00:01:16,079 --> 00:01:17,095
0.9 rating on the

40
00:01:18,003 --> 00:01:19,018
romance scale, it is a

41
00:01:19,026 --> 00:01:20,084
highly romantic movie but zero on

42
00:01:20,092 --> 00:01:22,040
the action scale, so almost no

43
00:01:22,051 --> 00:01:24,039
action in that

44
00:01:24,054 --> 00:01:25,085
movie. Romance forever was 1.0,

45
00:01:26,023 --> 00:01:27,060
lot of romance and 0.01 action,

46
00:01:27,085 --> 00:01:29,079
I don't know maybe

47
00:01:30,070 --> 00:01:32,065
there's a minor car crash in

48
00:01:33,062 --> 00:01:35,057
that movie or something, so little bit of action.

49
00:01:35,060 --> 00:01:36,076
Skipping one let's do

50
00:01:37,085 --> 00:01:39,062
Swords vs,. karate, maybe that

51
00:01:39,087 --> 00:01:41,010
has a zero romance rating

52
00:01:41,051 --> 00:01:42,078
and no romance at all in that

53
00:01:43,025 --> 00:01:46,004
but plenty of action and you know, non-stop car chases.

54
00:01:46,029 --> 00:01:47,012
Maybe again there is

55
00:01:47,021 --> 00:01:48,039
tiny bit of romance in

56
00:01:48,050 --> 00:01:49,079
that movie, but mainly action,

57
00:01:50,045 --> 00:01:51,056
and and Cute puppies of

58
00:01:51,068 --> 00:01:52,073
love again but mainly a romance

59
00:01:53,051 --> 00:01:54,040
movie with no action at all.

60
00:01:55,098 --> 00:01:57,015
So if we have features

61
00:01:57,054 --> 00:01:59,021
like these then each movie

62
00:01:59,079 --> 00:02:01,051
can be represented with a feature vector.

63
00:02:02,037 --> 00:02:03,081
Let's take movie 1, so just

64
00:02:04,001 --> 00:02:06,020
call these movies you know, movies 1 2, 3, 4 and 5.

65
00:02:06,062 --> 00:02:08,018
For my first movie,

66
00:02:08,052 --> 00:02:09,081
Love at last, I have

67
00:02:10,016 --> 00:02:11,071
my two features, 0.9 and

68
00:02:12,018 --> 00:02:12,094
0, and so these are features

69
00:02:13,037 --> 00:02:16,016
X1 and X2, and

70
00:02:16,034 --> 00:02:17,027
let's add an extra feature

71
00:02:17,078 --> 00:02:18,078
as usual, which is my

72
00:02:19,034 --> 00:02:21,063
interceptor feature X0, which is equal to 1

73
00:02:22,068 --> 00:02:23,081
and so, putting these together,

74
00:02:24,069 --> 00:02:26,015
I would then have a feature X1,

75
00:02:26,096 --> 00:02:28,041
the superscript 1 denotes it's

76
00:02:28,050 --> 00:02:29,043
the feature vector for my first

77
00:02:29,077 --> 00:02:30,071
movie, and this feature

78
00:02:30,097 --> 00:02:32,052
vector is equal to one.

79
00:02:33,018 --> 00:02:34,087
The first one there is this interceptor,

80
00:02:35,074 --> 00:02:37,000
and then my two features 0.9, 0,

81
00:02:37,025 --> 00:02:39,033
like so.

82
00:02:40,037 --> 00:02:41,036
So, for Love at last, I

83
00:02:41,055 --> 00:02:43,046
would have a feature vector X1,

84
00:02:44,047 --> 00:02:46,021
for the movie Romance Forever, we

85
00:02:46,034 --> 00:02:47,050
have the separate feature vector

86
00:02:47,080 --> 00:02:49,031
X2 and so on, and

87
00:02:49,037 --> 00:02:50,078
for Swords vs. karate I would

88
00:02:51,050 --> 00:02:54,005
have a different feature vector x superscript 5.

89
00:02:56,015 --> 00:02:57,046
Also, consistent with our

90
00:02:57,068 --> 00:02:59,009
early notation that we were

91
00:02:59,030 --> 00:03:00,021
using, we're going to set N

92
00:03:00,049 --> 00:03:02,012
to be the number of features, not

93
00:03:02,036 --> 00:03:03,053
counting this X zero

94
00:03:03,081 --> 00:03:05,031
intercept term so n is

95
00:03:05,041 --> 00:03:06,059
equal to two because we have

96
00:03:06,078 --> 00:03:08,018
two features x1 and x2

97
00:03:08,088 --> 00:03:10,013
capturing the degree of romance

98
00:03:10,063 --> 00:03:11,097
and the degree of action in each

99
00:03:12,062 --> 00:03:14,027
movie. 
Now in order

100
00:03:14,056 --> 00:03:17,093
to make predictions, here is one thing we could do,

101
00:03:19,022 --> 00:03:20,097
which is that we could treat predicting

102
00:03:21,015 --> 00:03:22,034
the ratings of each user

103
00:03:23,025 --> 00:03:26,021
as a separate linear regression problem. So

104
00:03:26,043 --> 00:03:27,065
specifically lets say that for each

105
00:03:27,091 --> 00:03:29,016
user j we are going

106
00:03:29,027 --> 00:03:30,086
to learn a parameter vector theta

107
00:03:31,034 --> 00:03:33,003
J which would be in R3 in this case,

108
00:03:33,053 --> 00:03:35,072
more generally theta j would

109
00:03:35,094 --> 00:03:37,096
be in r n+1, where

110
00:03:38,034 --> 00:03:39,046
n is the number of features,

111
00:03:39,069 --> 00:03:42,016
not counting the intercept term, and we're going

112
00:03:42,043 --> 00:03:43,087
to predict user J as

113
00:03:44,005 --> 00:03:45,078
rating movie I, with just

114
00:03:46,000 --> 00:03:47,038
the inner product between the parameters

115
00:03:47,086 --> 00:03:50,059
vector theta and the features "XI".

116
00:03:51,083 --> 00:03:53,068
So, let's take a specific example.

117
00:03:55,012 --> 00:03:56,069
Let's take user one.

118
00:03:59,059 --> 00:04:01,012
So that would be Alice and

119
00:04:01,037 --> 00:04:02,069
associated with Alice would

120
00:04:02,083 --> 00:04:03,099
be some parameter vector,

121
00:04:04,081 --> 00:04:06,021
theta 1 and our

122
00:04:06,052 --> 00:04:07,061
second user Bob will be

123
00:04:07,071 --> 00:04:08,059
associated, with a different

124
00:04:08,096 --> 00:04:10,028
parameter vector theta 2.

125
00:04:10,080 --> 00:04:12,018
Carol will be associated with a

126
00:04:12,030 --> 00:04:13,036
different parameter vector theta

127
00:04:13,065 --> 00:04:14,078
3 and Dave a different

128
00:04:15,075 --> 00:04:17,067
parameter vector, theta 4. So

129
00:04:18,008 --> 00:04:18,099
lets say we want to make a

130
00:04:19,031 --> 00:04:21,004
prediction for what Alice will

131
00:04:21,024 --> 00:04:22,044
think of the movie, Cute

132
00:04:22,068 --> 00:04:24,063
puppies of love. Well that

133
00:04:24,081 --> 00:04:25,067
movie is going to have some

134
00:04:26,081 --> 00:04:29,018
parameter vector X3, where

135
00:04:29,041 --> 00:04:30,039
we have that X3 is going

136
00:04:30,043 --> 00:04:32,045
to be equal to 1

137
00:04:32,064 --> 00:04:34,057
which is my intercept term, and

138
00:04:34,080 --> 00:04:37,022
then 0.99, and then 0.

139
00:04:38,056 --> 00:04:39,068
And let's say for this

140
00:04:39,081 --> 00:04:41,004
example, let's say that you

141
00:04:41,018 --> 00:04:42,088
know we have somehow already gotten

142
00:04:43,029 --> 00:04:44,060
a parameter vector theta 1

143
00:04:44,082 --> 00:04:45,069
for Alice--we we will

144
00:04:45,085 --> 00:04:47,056
say later exactly how

145
00:04:47,080 --> 00:04:48,051
we come up with this parameter

146
00:04:48,060 --> 00:04:50,052
vector--but let's

147
00:04:50,070 --> 00:04:52,000
just say for now that you

148
00:04:52,014 --> 00:04:53,056
know some unspecified learning algorithm

149
00:04:54,004 --> 00:04:55,004
has learned the parameter vector

150
00:04:55,018 --> 00:04:56,097
theta 1 and it is

151
00:04:57,012 --> 00:04:59,025
equal to 0 5 0. And so

152
00:05:00,014 --> 00:05:02,000
our prediction for this

153
00:05:02,026 --> 00:05:04,012
entry is going to

154
00:05:04,025 --> 00:05:06,093
be equal to theta 1,

155
00:05:07,043 --> 00:05:08,075
that is Alice's parameter vector,

156
00:05:09,062 --> 00:05:11,044
transpose X3, that

157
00:05:11,062 --> 00:05:13,073
is the feature vector for

158
00:05:14,017 --> 00:05:16,005
the Cute Puppies of Love movie number 3.

159
00:05:16,025 --> 00:05:17,019
And so the inner

160
00:05:17,047 --> 00:05:18,047
product between these two vectors

161
00:05:19,091 --> 00:05:21,077
is going to be 5 x 0.99.

162
00:05:23,098 --> 00:05:26,033
Which is equal to 4.95.

163
00:05:27,036 --> 00:05:28,093
And so my prediction for value this

164
00:05:29,012 --> 00:05:30,093
over here is going to be 4.95.

165
00:05:31,097 --> 00:05:33,011
And maybe that seems like a

166
00:05:33,023 --> 00:05:34,066
reasonable value, if indeed

167
00:05:36,012 --> 00:05:37,082
this is my parameter vector theta 1.

168
00:05:38,094 --> 00:05:40,029
So all we doing here is

169
00:05:40,051 --> 00:05:42,070
we are applying a different copy of

170
00:05:42,093 --> 00:05:44,048
essentially linear regression for each

171
00:05:44,075 --> 00:05:46,001
user and we are saying

172
00:05:46,023 --> 00:05:47,061
that what Alice does, is

173
00:05:47,081 --> 00:05:48,087
Alice has seem some parameter vector

174
00:05:49,016 --> 00:05:50,039
theta 1 that she uses,

175
00:05:51,041 --> 00:05:52,037
that we use to predict

176
00:05:53,031 --> 00:05:54,076
her ratings as a

177
00:05:54,094 --> 00:05:56,018
function of how romantic and how

178
00:05:56,047 --> 00:05:57,054
action packed the movie is

179
00:05:58,020 --> 00:05:59,060
and Bob, and Carol, and

180
00:05:59,074 --> 00:06:01,000
Dave each of them have a

181
00:06:01,022 --> 00:06:03,017
different linear function of the

182
00:06:03,032 --> 00:06:04,069
romantic-ness and action-ness or the degree

183
00:06:05,022 --> 00:06:06,050
of romance and the degree of action

184
00:06:07,057 --> 00:06:08,002
in a movie,

185
00:06:08,081 --> 00:06:11,030
and that that is how we're going to predict their star ratings.

186
00:06:14,081 --> 00:06:16,032
More formally here is

187
00:06:16,061 --> 00:06:17,092
how we can write down the problem.

188
00:06:19,025 --> 00:06:20,031
Our notation is that RIJ

189
00:06:20,068 --> 00:06:21,060
is equal to one, if

190
00:06:21,068 --> 00:06:22,091
user J has rated movie I,

191
00:06:23,037 --> 00:06:24,062
and YIJ is the rating

192
00:06:25,085 --> 00:06:28,000
of that movie if that rating exists.

193
00:06:29,054 --> 00:06:30,051
That is if that user has actually

194
00:06:31,002 --> 00:06:32,082
rated that movie. And

195
00:06:33,032 --> 00:06:34,036
on the previous slide we also

196
00:06:34,064 --> 00:06:36,054
defined theta J which

197
00:06:36,074 --> 00:06:38,079
is a parameter for each user XI

198
00:06:39,014 --> 00:06:40,082
which is a feature vector for specific

199
00:06:41,022 --> 00:06:42,037
movie and for each user

200
00:06:42,085 --> 00:06:43,077
and each movie you would predict

201
00:06:44,030 --> 00:06:45,062
that rating, as follows.

202
00:06:47,023 --> 00:06:49,056
So let me introduce,

203
00:06:49,064 --> 00:06:51,060
just temporarily, introduce one extra

204
00:06:51,086 --> 00:06:53,052
bit of notation mj, we

205
00:06:53,075 --> 00:06:54,098
are gonna use mj to denote the

206
00:06:55,006 --> 00:06:56,013
number of users rated by movie

207
00:06:56,039 --> 00:06:57,035
j, we're gonna need this

208
00:06:57,057 --> 00:06:59,088
notation only for this slide. Now, in order to learn

209
00:07:00,016 --> 00:07:01,069
the parameter vector for

210
00:07:01,075 --> 00:07:03,072
theta j, well, how can we do so?

211
00:07:04,041 --> 00:07:06,037
This is basically a linear regression problem.

212
00:07:06,093 --> 00:07:07,098
So what we can do, is

213
00:07:08,029 --> 00:07:09,081
just choose a parameter vector, theta j,

214
00:07:10,051 --> 00:07:12,010
so the predicted value

215
00:07:12,056 --> 00:07:13,062
here are as close

216
00:07:13,098 --> 00:07:15,027
as possible to the values

217
00:07:15,080 --> 00:07:18,075
that we observed in our training set, the values we observed in our data.

218
00:07:19,089 --> 00:07:21,038
So, let's write that down.

219
00:07:22,029 --> 00:07:24,031
In order to learn the

220
00:07:24,037 --> 00:07:26,095
parameter vector theta j, let's minimize over

221
00:07:27,017 --> 00:07:28,050
my parameter vector theta j,

222
00:07:29,039 --> 00:07:30,036
of sum--

223
00:07:31,092 --> 00:07:32,086
and I want to sum

224
00:07:33,029 --> 00:07:34,089
over all movies that user

225
00:07:35,024 --> 00:07:36,093
j has rated--so we write this as sum

226
00:07:37,026 --> 00:07:38,029
over all values of i

227
00:07:39,010 --> 00:07:42,000
that is a colon rij equals 1.

228
00:07:43,087 --> 00:07:45,097
So the way to read this summation index is

229
00:07:46,037 --> 00:07:48,027
this is summation over all

230
00:07:48,047 --> 00:07:49,055
the values of i, so that

231
00:07:49,077 --> 00:07:51,018
r i j is equal to 1.

232
00:07:51,020 --> 00:07:52,047
So this is going to be summing over all the

233
00:07:52,056 --> 00:07:54,067
movies that user j has rated.

234
00:07:56,023 --> 00:07:57,000
And then I am going to

235
00:07:58,014 --> 00:07:59,091
compute theta j

236
00:08:01,081 --> 00:08:04,044
transpose xi so

237
00:08:04,061 --> 00:08:06,074
that's the prediction of user

238
00:08:07,002 --> 00:08:08,038
j's rating on movie i,

239
00:08:09,023 --> 00:08:10,095
minus y i j,

240
00:08:11,069 --> 00:08:13,069
so that's the actual observed rating squared,

241
00:08:15,018 --> 00:08:16,079
and then, let me just divide

242
00:08:17,025 --> 00:08:18,064
by the number of movies

243
00:08:19,004 --> 00:08:20,099
that user J, has

244
00:08:21,037 --> 00:08:23,091
actually rated, so just divide by 1 over 2MJ.

245
00:08:24,000 --> 00:08:25,045
And so this is

246
00:08:25,068 --> 00:08:27,062
just like the least squares regression,

247
00:08:28,020 --> 00:08:29,055
it's just like linear regression

248
00:08:30,017 --> 00:08:31,017
where we want to choose

249
00:08:31,031 --> 00:08:34,048
the parameter vector theta J, to minimize this type of squared error term.

250
00:08:34,050 --> 00:08:35,009
And if you want to, you can

251
00:08:36,033 --> 00:08:39,058
also add in a regularization term

252
00:08:39,098 --> 00:08:41,087
so plus lambda over 2m, and

253
00:08:43,077 --> 00:08:44,092
this is really 2MJ because,

254
00:08:45,041 --> 00:08:47,075
this as if we have MJ examples right?

255
00:08:47,091 --> 00:08:49,033
Because if user J has

256
00:08:49,064 --> 00:08:50,090
rated that many movies, it's

257
00:08:51,004 --> 00:08:53,034
sort of like we have that many data points with which to fit

258
00:08:53,067 --> 00:08:55,078
the parameters theta J. And then

259
00:08:56,064 --> 00:08:57,038
let me add in my usual

260
00:08:58,034 --> 00:09:00,025
regularization term here of

261
00:09:00,046 --> 00:09:02,052
theta J K squared.

262
00:09:03,011 --> 00:09:04,026
As usual this sum is from

263
00:09:04,084 --> 00:09:05,098
K equals 1 through N

264
00:09:06,033 --> 00:09:08,066
so here theta J is

265
00:09:08,087 --> 00:09:10,004
going to be an N plus

266
00:09:10,051 --> 00:09:12,039
1 dimensional vector where,

267
00:09:12,062 --> 00:09:14,062
in our early example, n was equal to two,

268
00:09:15,032 --> 00:09:17,009
but more generally, n is

269
00:09:17,025 --> 00:09:20,098
the number of features we have per movie.

270
00:09:21,073 --> 00:09:22,026
And so as usual we don't regularize over theta 0.

271
00:09:22,038 --> 00:09:23,071
We don't regularize over the

272
00:09:23,090 --> 00:09:24,075
biased term because the sum is

273
00:09:24,092 --> 00:09:28,059
from K1 through N. If

274
00:09:28,075 --> 00:09:30,042
you minimize this as

275
00:09:30,057 --> 00:09:31,077
a function of theta J you get a

276
00:09:31,089 --> 00:09:33,000
good solution, you get a

277
00:09:33,017 --> 00:09:35,033
pretty good estimate of a parameter vector theta j

278
00:09:36,049 --> 00:09:37,020
with which to make the predictions

279
00:09:37,094 --> 00:09:39,046
for user J's movie ratings.

280
00:09:40,082 --> 00:09:42,025
For recommender systems, we're going

281
00:09:42,051 --> 00:09:44,013
to change this notation a little

282
00:09:44,050 --> 00:09:46,012
bit. So to simplify the subsequent math,

283
00:09:46,069 --> 00:09:48,044
I'm actually going to get rid of this term MJ.

284
00:09:49,057 --> 00:09:50,072
So that's just a constant right

285
00:09:50,097 --> 00:09:52,013
so I can delete it without changing

286
00:09:53,000 --> 00:09:54,030
the value of theta J that

287
00:09:54,042 --> 00:09:55,084
I get out of this optimization,

288
00:09:56,000 --> 00:09:57,002
so if you imagine taking this

289
00:09:57,022 --> 00:09:58,085
whole equation, taking this

290
00:09:59,000 --> 00:10:00,028
whole expression and multiplying it by

291
00:10:00,087 --> 00:10:02,053
MJ and get rid of that constant, and when

292
00:10:02,095 --> 00:10:04,011
I minimize this I should still get

293
00:10:04,020 --> 00:10:06,059
the same value of theta J as before.

294
00:10:06,071 --> 00:10:07,077
So, just to repeat what

295
00:10:08,044 --> 00:10:10,005
we wrote on the previous slide, here

296
00:10:10,034 --> 00:10:12,025
is our optimization objective: In order

297
00:10:12,058 --> 00:10:13,062
to learn theta J, which is

298
00:10:13,099 --> 00:10:15,008
a parameter for user J,

299
00:10:15,078 --> 00:10:17,057
we're going to minimize over theta j

300
00:10:17,076 --> 00:10:19,082
this optimization objectives. So

301
00:10:20,010 --> 00:10:21,036
this is our usual squared

302
00:10:21,072 --> 00:10:24,083
error term and then this is our regularization term.

303
00:10:26,004 --> 00:10:27,040
Now of course in building

304
00:10:27,069 --> 00:10:28,078
a recommender system we don't

305
00:10:29,002 --> 00:10:29,079
just want to learn parameters

306
00:10:30,041 --> 00:10:31,050
for a single user, we want

307
00:10:31,064 --> 00:10:33,013
to learn parameters for all of

308
00:10:33,049 --> 00:10:35,063
our users, I have n subscript u

309
00:10:35,075 --> 00:10:36,073
users, so I want to

310
00:10:36,095 --> 00:10:38,091
learn all of these parameters and

311
00:10:39,005 --> 00:10:39,083
so what I'm going to do

312
00:10:40,013 --> 00:10:42,032
is take this minimization, take

313
00:10:42,050 --> 00:10:45,048
this optimization objective and just add an extra summation there.

314
00:10:45,079 --> 00:10:47,061
So, you know, this expression here

315
00:10:48,040 --> 00:10:49,020
with the one half on top again, so

316
00:10:49,024 --> 00:10:50,050
it's exactly the same

317
00:10:50,077 --> 00:10:52,051
as what we have on top except

318
00:10:52,095 --> 00:10:53,098
that now, instead of just

319
00:10:54,009 --> 00:10:55,066
doing this for a specific user theta

320
00:10:55,096 --> 00:10:57,026
J, I'm going to sum

321
00:10:57,067 --> 00:10:59,034
my objective over all of

322
00:10:59,049 --> 00:11:00,094
my users and then minimize

323
00:11:01,025 --> 00:11:03,070
this overall optimization objective.

324
00:11:04,032 --> 00:11:05,057
Minimize this overall cost function.

325
00:11:06,073 --> 00:11:09,020
And when I minimize this

326
00:11:09,037 --> 00:11:10,055
as a function of theta 1,

327
00:11:11,036 --> 00:11:12,039
theta 2, up to

328
00:11:12,060 --> 00:11:14,012
theta NU, I will

329
00:11:14,026 --> 00:11:15,075
get a separate parameter

330
00:11:16,002 --> 00:11:17,034
vector each user and

331
00:11:17,045 --> 00:11:18,072
I can then use that

332
00:11:19,009 --> 00:11:20,046
to make predictions for all of

333
00:11:20,052 --> 00:11:21,061
my users for all of

334
00:11:21,072 --> 00:11:23,014
my N subscript u users.

335
00:11:24,051 --> 00:11:26,055
So putting everything together, this

336
00:11:27,017 --> 00:11:28,073
was our optimization objective on

337
00:11:28,087 --> 00:11:29,094
top and to give

338
00:11:30,016 --> 00:11:31,007
this thing a name, I'll just call this

339
00:11:31,092 --> 00:11:33,048
J of theta 1,

340
00:11:33,062 --> 00:11:35,051
dot, dot, dot theta NU.

341
00:11:36,004 --> 00:11:37,027
So J as usual is my

342
00:11:37,059 --> 00:11:39,083
optimization objective which I'm trying to minimize.

343
00:11:41,033 --> 00:11:42,050
Next, in order to actually

344
00:11:42,087 --> 00:11:44,030
do the minimization, if you

345
00:11:44,050 --> 00:11:45,084
were to derive the gradient

346
00:11:46,014 --> 00:11:47,040
descent updates, these are

347
00:11:47,052 --> 00:11:48,072
the equations you would get,

348
00:11:49,089 --> 00:11:51,029
so you would take theta

349
00:11:51,075 --> 00:11:53,030
JK and subtract from

350
00:11:53,042 --> 00:11:56,019
it alpha, which is the learning rate, times these terms here on the right.

351
00:11:56,027 --> 00:11:57,053
So we have slightly different cases

352
00:11:58,015 --> 00:11:59,065
so when K equals 0 and when K is not

353
00:11:59,084 --> 00:12:01,046
equal to 0, because our regularization

354
00:12:01,096 --> 00:12:04,037
term here regularizes only the

355
00:12:04,090 --> 00:12:06,042
values of theta JK for

356
00:12:06,061 --> 00:12:07,069
K not equal to zero. So

357
00:12:07,083 --> 00:12:09,047
we don't regularize theta 0

358
00:12:10,009 --> 00:12:11,061
so the slightly different updates

359
00:12:12,026 --> 00:12:13,058
for k equals zero, and k not equal to 0.

360
00:12:14,067 --> 00:12:16,008
And this term, over

361
00:12:16,025 --> 00:12:18,009
here, for example is just a partial

362
00:12:18,051 --> 00:12:20,078
derivative with respect to your parameter,

363
00:12:21,009 --> 00:12:24,029
that of your

364
00:12:25,035 --> 00:12:28,026
optimization objective, right?

365
00:12:28,078 --> 00:12:30,027
And so, this is just

366
00:12:30,067 --> 00:12:33,000
gradient descent and I've

367
00:12:33,023 --> 00:12:35,044
already computed the derivatives and plugged them into here.

368
00:12:36,055 --> 00:12:39,058
If these gradient

369
00:12:40,057 --> 00:12:41,080
descent updates look a

370
00:12:41,098 --> 00:12:42,087
lot like what we had for

371
00:12:43,004 --> 00:12:44,070
linear regression, that's because these

372
00:12:44,087 --> 00:12:47,025
are essentially the same as linear regression.

373
00:12:48,019 --> 00:12:49,050
The only minor difference is that

374
00:12:49,077 --> 00:12:51,012
for linear regression we have

375
00:12:51,058 --> 00:12:52,060
these 1 over M terms

376
00:12:52,099 --> 00:12:54,071
- it's really 1

377
00:12:54,080 --> 00:12:56,076
over MJ - but

378
00:12:57,054 --> 00:12:59,023
because earlier when we were

379
00:12:59,037 --> 00:13:00,077
deriving the optimization objective

380
00:13:01,026 --> 00:13:03,053
we got rid of this, that's why we don't have this 1 over M term.

381
00:13:04,044 --> 00:13:05,087
But otherwise it's really sum over

382
00:13:06,008 --> 00:13:08,035
my training examples of, you

383
00:13:08,052 --> 00:13:09,088
know, the error times

384
00:13:10,023 --> 00:13:13,038
XK plus that regularization

385
00:13:14,089 --> 00:13:16,054
term contributes to the derivative.

386
00:13:18,012 --> 00:13:19,003
So if you are using

387
00:13:19,020 --> 00:13:20,036
gradient descent, here is how

388
00:13:20,067 --> 00:13:22,013
you can minimize the cost

389
00:13:22,044 --> 00:13:23,087
function j, to learn all

390
00:13:24,011 --> 00:13:25,049
the parameters, and using these

391
00:13:25,063 --> 00:13:26,098
formulas for the derivatives, if

392
00:13:27,009 --> 00:13:28,024
you want, you can also plug them

393
00:13:28,044 --> 00:13:29,071
into a more advanced optimization

394
00:13:30,028 --> 00:13:31,071
algorithm like cluster gradient or

395
00:13:31,080 --> 00:13:33,073
LBFGS or what have you, and use

396
00:13:33,094 --> 00:13:35,092
that to try to minimize the cost function J as well.

397
00:13:37,036 --> 00:13:38,045
So hopefully you now know

398
00:13:38,075 --> 00:13:40,050
how you can apply essentially a

399
00:13:41,000 --> 00:13:42,082
variation on linear regression in

400
00:13:42,095 --> 00:13:45,046
order to predict different movie ratings by different users.

401
00:13:46,035 --> 00:13:47,050
This particular algorithm is called

402
00:13:48,002 --> 00:13:49,092
a content based recommendations, or

403
00:13:50,003 --> 00:13:51,098
content based approach because we

404
00:13:52,012 --> 00:13:53,020
assume that we have available

405
00:13:53,064 --> 00:13:55,042
to us, features for the different movies.

406
00:13:56,014 --> 00:13:57,033
So we have features that

407
00:13:57,049 --> 00:13:58,061
capture what is the

408
00:13:58,070 --> 00:14:00,025
content of these movies. How romantic is this movie?

409
00:14:01,027 --> 00:14:03,004
How much action is in this movie?

410
00:14:03,042 --> 00:14:04,069
And we are really using features of the

411
00:14:04,077 --> 00:14:06,090
content of the movies to make our predictions.

412
00:14:08,035 --> 00:14:09,076
But for many movies we

413
00:14:09,091 --> 00:14:11,029
don't actually have such features,

414
00:14:11,082 --> 00:14:13,062
or it may be very difficult to get

415
00:14:13,085 --> 00:14:14,097
such features for all of

416
00:14:15,004 --> 00:14:16,015
our movies, for all

417
00:14:16,046 --> 00:14:17,079
of whatever items we are trying to sell.

418
00:14:18,087 --> 00:14:20,042
So in the next video, we'll

419
00:14:20,059 --> 00:14:21,052
start to talk about an approach

420
00:14:22,000 --> 00:14:23,028
to recommender systems that isn't

421
00:14:23,057 --> 00:14:24,071
content based and does not

422
00:14:24,098 --> 00:14:26,009
assume that we have

423
00:14:26,066 --> 00:14:28,041
someone else giving us all of these features,

424
00:14:28,087 --> 00:14:30,029
for all of the movies in our data set.
