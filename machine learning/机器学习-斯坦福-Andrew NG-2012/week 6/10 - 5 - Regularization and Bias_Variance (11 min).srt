
1
00:00:00,039 --> 00:00:02,043
You've seen how regularization can help

2
00:00:02,060 --> 00:00:04,066
prevent overfitting, but how

3
00:00:04,096 --> 00:00:06,023
does it affect the bias and

4
00:00:06,046 --> 00:00:08,007
variance of a learning algorithm?

5
00:00:08,063 --> 00:00:09,089
In this video, I like to

6
00:00:10,001 --> 00:00:11,017
go deeper into the issue

7
00:00:11,055 --> 00:00:13,030
of bias and variance, and

8
00:00:13,051 --> 00:00:14,044
talk about how it interacts

9
00:00:15,007 --> 00:00:15,088
with, and is effected by,

10
00:00:16,007 --> 00:00:18,046
the regularization of your learning algorithm.

11
00:00:22,017 --> 00:00:23,039
Suppose we fit a linear

12
00:00:23,069 --> 00:00:24,087
regression model with a very

13
00:00:25,025 --> 00:00:27,046
high order polynomial, but to

14
00:00:27,067 --> 00:00:28,067
prevent overfitting, we are

15
00:00:28,078 --> 00:00:30,087
going to use regularization as shown here.

16
00:00:31,055 --> 00:00:32,078
Suppose we're fitting a high

17
00:00:33,018 --> 00:00:34,068
order polynomial like that shown

18
00:00:35,011 --> 00:00:36,032
here, but to prevent

19
00:00:36,075 --> 00:00:37,077
overfitting, we're going to

20
00:00:37,090 --> 00:00:39,053
use regularization, like that shown

21
00:00:39,090 --> 00:00:41,007
here, so we have this regularization

22
00:00:41,088 --> 00:00:43,004
term to try to

23
00:00:43,039 --> 00:00:45,028
keep the values of the parameters small.

24
00:00:45,071 --> 00:00:47,039
And as usual, the regularization sums

25
00:00:47,077 --> 00:00:49,018
from j equals 1 to

26
00:00:49,028 --> 00:00:50,047
m rather than j equals 0

27
00:00:50,060 --> 00:00:53,013
to m.  Let's consider three cases.

28
00:00:53,074 --> 00:00:55,059
The first is the case of

29
00:00:55,065 --> 00:00:56,089
a very large value of the

30
00:00:56,096 --> 00:00:59,025
regularization parameter lambda, such

31
00:00:59,049 --> 00:01:00,064
as if lambda were

32
00:01:00,078 --> 00:01:01,060
equal to 10,000s of huge value.

33
00:01:01,078 --> 00:01:04,009
In this

34
00:01:04,037 --> 00:01:05,051
case, all of these

35
00:01:05,065 --> 00:01:07,025
parameters, theta 1, theta 2,

36
00:01:07,057 --> 00:01:08,031
theta 3 and so on will

37
00:01:08,048 --> 00:01:10,039
be heavily penalized and

38
00:01:10,056 --> 00:01:12,065
so, what ends up with most

39
00:01:13,010 --> 00:01:14,043
of these parameter values being close

40
00:01:14,079 --> 00:01:17,000
to 0 and the hypothesis will be

41
00:01:17,018 --> 00:01:17,093
roughly h or x

42
00:01:18,028 --> 00:01:19,098
just equal or approximately equal

43
00:01:20,026 --> 00:01:21,053
to theta 0, and so we

44
00:01:21,068 --> 00:01:23,056
end up a hypothesis that more

45
00:01:23,079 --> 00:01:25,025
or less looks like that. This is more or

46
00:01:25,037 --> 00:01:28,012
less a flat, constant straight line.

47
00:01:28,040 --> 00:01:30,031
And so this hypothesis has high

48
00:01:30,065 --> 00:01:32,062
bias and a value underfits this data set.

49
00:01:32,096 --> 00:01:34,051
So the horizontal straight

50
00:01:34,084 --> 00:01:35,081
line is just not a very

51
00:01:35,093 --> 00:01:38,009
good model for this data set.

52
00:01:38,070 --> 00:01:39,087
At the other extreme beam is if we have

53
00:01:40,025 --> 00:01:41,056
a very small value of

54
00:01:41,084 --> 00:01:43,031
lambda, such as if lambda

55
00:01:43,070 --> 00:01:45,062
were equal to 0.

56
00:01:45,071 --> 00:01:46,093
In that case, given that we're

57
00:01:47,007 --> 00:01:48,023
fitting a high order polynomial,

58
00:01:48,039 --> 00:01:49,068
this is a

59
00:01:49,093 --> 00:01:51,059
usual overfitting setting.

60
00:01:52,075 --> 00:01:53,098
In that case, given that we're

61
00:01:54,018 --> 00:01:55,023
fitting a high order polynomial,

62
00:01:56,017 --> 00:01:58,004
basically without regularization or with

63
00:01:58,023 --> 00:02:00,017
very minimal regularization, we end

64
00:02:00,034 --> 00:02:02,018
up with our usual high variance, overfitting

65
00:02:02,081 --> 00:02:03,090
setting, because basically if lambda is

66
00:02:04,062 --> 00:02:05,065
equal to zero, we are just

67
00:02:05,079 --> 00:02:08,031
fitting with our regularization so

68
00:02:08,043 --> 00:02:14,046
that overfits the hypothesis

69
00:02:15,069 --> 00:02:16,056
and is only if we have some

70
00:02:16,072 --> 00:02:18,071
intermediate value of lambda that is neither too large nor too small that we end up with parameters theta

71
00:02:19,021 --> 00:02:20,037
that we end up that give us a reasonable

72
00:02:20,077 --> 00:02:22,005
fit to this data.

73
00:02:22,088 --> 00:02:23,081
So how can we automatically

74
00:02:24,061 --> 00:02:26,008
choose a good value

75
00:02:26,058 --> 00:02:28,009
for the regularization parameter lambda?

76
00:02:29,009 --> 00:02:31,037
Just to reiterate, here is our model and here is our learning algorithm subjective.

77
00:02:33,066 --> 00:02:36,058
For the setting where we're using regularization, let me define

78
00:02:37,040 --> 00:02:39,053
j train of theta to be something different

79
00:02:40,040 --> 00:02:42,037
to be the optimization objective

80
00:02:43,016 --> 00:02:44,080
but without the regularization term.

81
00:02:45,053 --> 00:02:47,040
Previously, in earlier video

82
00:02:47,075 --> 00:02:48,066
when we are not using

83
00:02:49,003 --> 00:02:50,080
regularization, I define j train of theta to

84
00:02:51,065 --> 00:02:54,078
be the same as j of theta as the cost function but

85
00:02:55,003 --> 00:02:57,043
when we are using regularization with this extra lambda term

86
00:02:58,047 --> 00:03:00,084
we're going to

87
00:03:01,008 --> 00:03:02,022
define j train my training set error,

88
00:03:02,050 --> 00:03:03,061
to be just my sum of

89
00:03:03,083 --> 00:03:05,006
squared errors on the training

90
00:03:05,040 --> 00:03:06,090
set, or my average squared error

91
00:03:07,012 --> 00:03:10,006
on the training set without taking into account that regularization chart.

92
00:03:10,093 --> 00:03:12,025
And similarly, I'm then

93
00:03:12,040 --> 00:03:13,068
also going to define the

94
00:03:14,021 --> 00:03:16,016
cross-validation set error when the

95
00:03:16,027 --> 00:03:17,037
test set error, as before

96
00:03:17,083 --> 00:03:19,071
to be the average sum of squared errors

97
00:03:20,031 --> 00:03:21,099
on the cross-validation and the test sets.

98
00:03:23,024 --> 00:03:25,027
So just to summarize,

99
00:03:25,081 --> 00:03:27,006
my definitions of J train and

100
00:03:27,049 --> 00:03:28,040
J C V and J

101
00:03:28,062 --> 00:03:29,081
Test are just the

102
00:03:30,005 --> 00:03:31,000
average squared error, or one

103
00:03:31,040 --> 00:03:32,061
half of the average

104
00:03:32,099 --> 00:03:34,059
squared error on my training validation and

105
00:03:34,084 --> 00:03:36,077
test sets without the extra

106
00:03:38,031 --> 00:03:39,028
regularization chart.

107
00:03:39,036 --> 00:03:41,050
So, this is how we can automatically choose the regularization parameter lambda.

108
00:03:43,094 --> 00:03:45,059
What I usually do is may

109
00:03:45,071 --> 00:03:48,003
be have some range of values of lambda I want to try it.

110
00:03:48,021 --> 00:03:49,074
So I might be

111
00:03:49,087 --> 00:03:51,005
considering not using regularization,

112
00:03:52,043 --> 00:03:53,056
or here are a few values I might try.

113
00:03:53,078 --> 00:03:54,074
I might be considering along because

114
00:03:55,021 --> 00:03:57,038
of O1, O2 from O4 and so on.

115
00:03:57,097 --> 00:03:59,040
And you know, I usually step these

116
00:03:59,065 --> 00:04:02,011
up in multiples of

117
00:04:02,031 --> 00:04:04,084
two until some maybe larger value

118
00:04:04,096 --> 00:04:06,013
this in multiples of two you

119
00:04:06,037 --> 00:04:07,088
I actually end up with 10.24;

120
00:04:08,015 --> 00:04:10,069
it's ten exactly, but you

121
00:04:10,087 --> 00:04:12,012
know, this is close enough and

122
00:04:12,075 --> 00:04:14,021
the 35 decimal

123
00:04:14,050 --> 00:04:16,072
places won't affect your result that much.

124
00:04:19,082 --> 00:04:21,061
So, this gives me, maybe

125
00:04:22,032 --> 00:04:24,016
twelve different models, that I'm

126
00:04:24,030 --> 00:04:26,004
trying to select amongst, corresponding to

127
00:04:26,023 --> 00:04:27,089
12 different values of the

128
00:04:28,020 --> 00:04:34,012
regularization parameter lambda and

129
00:04:34,026 --> 00:04:35,039
of course, you can also go

130
00:04:35,060 --> 00:04:37,052
to values less than 0.01

131
00:04:37,061 --> 00:04:38,080
or values larger than 10,

132
00:04:38,089 --> 00:04:41,006
but I've just truncated it here for convenience.

133
00:04:46,039 --> 00:04:47,025
Given each of these 12

134
00:04:47,058 --> 00:04:48,074
models, what we can

135
00:04:48,097 --> 00:04:49,076
do is then the following:

136
00:04:50,080 --> 00:04:52,010
we take this first

137
00:04:52,048 --> 00:04:53,085
model with lambda equals 0,

138
00:04:54,005 --> 00:04:56,011
and minimize my cos

139
00:04:56,038 --> 00:04:58,055
function j of theta and this

140
00:04:58,077 --> 00:05:00,031
would give me some parameter vector theta

141
00:05:00,085 --> 00:05:02,000
and similar to the earlier video,

142
00:05:02,019 --> 00:05:04,006
let me just denote this as

143
00:05:05,055 --> 00:05:06,064
theta superscript 1.

144
00:05:08,057 --> 00:05:09,043
And then I can take my

145
00:05:09,062 --> 00:05:11,020
second model, with lambda

146
00:05:11,068 --> 00:05:13,022
set to 0.01 and

147
00:05:13,085 --> 00:05:15,081
minimize my cos function, now

148
00:05:15,093 --> 00:05:17,056
using lambda equals 0.01

149
00:05:17,066 --> 00:05:18,076
of course, to get some

150
00:05:18,095 --> 00:05:19,098
different parameter vector theta,

151
00:05:20,052 --> 00:05:21,042
we need to know that theta 2,

152
00:05:21,055 --> 00:05:22,068
and for that I

153
00:05:22,093 --> 00:05:24,020
end up with theta 3

154
00:05:24,041 --> 00:05:25,027
so that this is correct for my

155
00:05:25,035 --> 00:05:27,008
third model, and so on,

156
00:05:27,062 --> 00:05:28,098
until for for my final model

157
00:05:29,044 --> 00:05:32,005
with lambda set to 10,

158
00:05:32,005 --> 00:05:35,014
or 10.24, or I end up with this theta 12.

159
00:05:36,033 --> 00:05:37,081
Next I can take

160
00:05:38,005 --> 00:05:39,070
all of these hypotheses, all of

161
00:05:39,079 --> 00:05:41,085
these parameters, and use

162
00:05:42,016 --> 00:05:44,019
my cross-validation set to evaluate them.

163
00:05:44,093 --> 00:05:46,043
So I can look at my

164
00:05:47,012 --> 00:05:48,042
first model, my second

165
00:05:48,076 --> 00:05:49,037
model, fits with these different values

166
00:05:49,039 --> 00:06:00,029
of the regularization parameter and

167
00:06:00,043 --> 00:06:01,031
evaluate them on my cross-validation

168
00:06:01,056 --> 00:06:02,014
set - basically measure the average squared error of each of these parameter

169
00:06:02,024 --> 00:06:03,091
vectors theta on my cross-validation set.

170
00:06:04,025 --> 00:06:05,080
And I would then pick whichever one

171
00:06:05,095 --> 00:06:07,039
of these 12 models gives me

172
00:06:07,056 --> 00:06:10,005
the lowest error on the cross-validation set.

173
00:06:11,005 --> 00:06:11,079
And let's say, for the sake

174
00:06:12,006 --> 00:06:13,066
of this example, that I

175
00:06:13,094 --> 00:06:15,056
end up picking theta 5,

176
00:06:15,064 --> 00:06:18,025
the fifth order polynomial, because

177
00:06:18,064 --> 00:06:21,024
that has the Noah's cross-validation error.

178
00:06:22,000 --> 00:06:24,022
Having done that, finally, what

179
00:06:24,038 --> 00:06:25,022
I would do if I want

180
00:06:25,049 --> 00:06:26,062
to report a test set error

181
00:06:27,037 --> 00:06:28,068
is to take the parameter theta

182
00:06:29,000 --> 00:06:30,088
5 that I've

183
00:06:31,004 --> 00:06:32,055
selected and look at

184
00:06:32,067 --> 00:06:34,070
how well it does on my test set.

185
00:06:34,083 --> 00:06:36,031
And once again here is as

186
00:06:36,048 --> 00:06:37,067
if we fit this parameter

187
00:06:38,023 --> 00:06:40,043
theta to my cross-validation

188
00:06:41,026 --> 00:06:42,045
set, which is why I

189
00:06:42,066 --> 00:06:43,093
am saving aside a separate

190
00:06:44,042 --> 00:06:45,081
test set that I

191
00:06:45,086 --> 00:06:47,006
am going to use to get

192
00:06:47,035 --> 00:06:48,047
a better estimate of how

193
00:06:48,073 --> 00:06:49,093
well my a parameter vector

194
00:06:50,018 --> 00:06:51,068
theta will generalize to previously unseen examples.

195
00:06:54,012 --> 00:06:55,087
So that's model selection applied

196
00:06:56,025 --> 00:06:58,031
to selecting the regularization parameter

197
00:06:59,025 --> 00:07:00,035
lambda. The last thing

198
00:07:00,049 --> 00:07:01,051
I'd like to do in this

199
00:07:01,076 --> 00:07:02,088
video, is get a

200
00:07:02,097 --> 00:07:05,007
better understanding of how

201
00:07:05,064 --> 00:07:07,033
cross-validation and training error

202
00:07:07,068 --> 00:07:10,042
vary as we as

203
00:07:10,052 --> 00:07:12,082
we vary the regularization parameter lambda.

204
00:07:13,045 --> 00:07:15,006
And so just a reminder, that

205
00:07:15,036 --> 00:07:16,075
was our original cosine function j of

206
00:07:16,083 --> 00:07:18,023
theta, but for this

207
00:07:18,039 --> 00:07:19,035
purpose we're going to define

208
00:07:20,044 --> 00:07:21,082
training error without using

209
00:07:22,024 --> 00:07:24,018
the regularization parameter, and cross-validation

210
00:07:24,086 --> 00:07:26,014
error without using the

211
00:07:26,036 --> 00:07:28,081
regularization parameter and what I'd like

212
00:07:29,020 --> 00:07:30,076
to do is plot this J train

213
00:07:31,075 --> 00:07:34,042
and plot this Jcv, meaning just

214
00:07:34,069 --> 00:07:35,081
how well does my

215
00:07:35,092 --> 00:07:38,025
hypothesis do for on

216
00:07:38,057 --> 00:07:39,075
the training set and how well

217
00:07:39,092 --> 00:07:41,027
does my hypothesis do on the

218
00:07:41,033 --> 00:07:43,025
cross-validation set as I

219
00:07:43,031 --> 00:07:45,023
vary my regularization parameter

220
00:07:45,069 --> 00:07:49,017
lambda so as

221
00:07:49,031 --> 00:07:51,074
we saw earlier, if lambda

222
00:07:52,006 --> 00:07:53,073
is small, then we're

223
00:07:53,092 --> 00:07:56,031
not using much regularization and

224
00:07:56,076 --> 00:07:58,086
we run a larger risk of overfitting.

225
00:07:59,094 --> 00:08:01,068
Where as if lambda is

226
00:08:01,093 --> 00:08:03,008
large, that is if we

227
00:08:03,031 --> 00:08:04,020
were on the right part

228
00:08:05,018 --> 00:08:07,039
of this horizontal axis, then

229
00:08:07,068 --> 00:08:08,076
with a large value of lambda

230
00:08:09,056 --> 00:08:12,006
we run the high risk of having a bias problem.

231
00:08:13,004 --> 00:08:14,064
So if you plot J train

232
00:08:15,027 --> 00:08:16,089
and Jcv, what you

233
00:08:16,098 --> 00:08:18,073
find is that for small

234
00:08:19,010 --> 00:08:21,017
values of lambda you can

235
00:08:22,000 --> 00:08:23,004
fit the training set relatively

236
00:08:23,063 --> 00:08:24,068
well because you're not regularizing.

237
00:08:25,060 --> 00:08:26,088
So, for small values of

238
00:08:26,099 --> 00:08:28,075
lambda, the regularization term basically

239
00:08:28,095 --> 00:08:30,010
goes away and you're just

240
00:08:30,042 --> 00:08:32,046
minimizing pretty much your squared error.

241
00:08:32,087 --> 00:08:34,049
So when lambda is small, you

242
00:08:34,062 --> 00:08:35,058
end up with a small value

243
00:08:36,016 --> 00:08:37,078
for J train, whereas if

244
00:08:37,089 --> 00:08:39,017
lambda is large, then you

245
00:08:39,074 --> 00:08:42,048
have a high bias problem and you might not fit your training set so well.

246
00:08:42,063 --> 00:08:43,079
So you end up with a value up there.

247
00:08:44,054 --> 00:08:48,079
So, J train of

248
00:08:48,092 --> 00:08:50,012
theta will tend to

249
00:08:50,032 --> 00:08:52,028
increase when lambda increases

250
00:08:53,004 --> 00:08:54,072
because a large value of

251
00:08:54,091 --> 00:08:55,085
lambda corresponds a high bias

252
00:08:56,039 --> 00:08:57,039
where you might not even fit your

253
00:08:57,059 --> 00:08:59,015
training set well, whereas a

254
00:08:59,028 --> 00:09:01,037
small value of lambda corresponds to,

255
00:09:01,064 --> 00:09:03,050
if you can you know freely

256
00:09:03,085 --> 00:09:06,069
fit to very high degree polynomials, your data, let's say.

257
00:09:06,091 --> 00:09:10,086
As for the cross-validation error, we end up with a figure like this.

258
00:09:12,008 --> 00:09:13,060
Where, over here on

259
00:09:13,092 --> 00:09:15,046
the right, if we

260
00:09:15,052 --> 00:09:16,047
have a large value of lambda,

261
00:09:17,044 --> 00:09:18,060
we may end up underfitting.

262
00:09:19,089 --> 00:09:21,027
And so, this is the bias regime

263
00:09:22,095 --> 00:09:25,075
whereas and cross

264
00:09:26,002 --> 00:09:27,067
validation error will be

265
00:09:27,091 --> 00:09:29,005
high and let me just leave

266
00:09:29,025 --> 00:09:31,075
all that. So, that's Jcv of theta because with

267
00:09:32,026 --> 00:09:33,044
high bias we won't be fitting.

268
00:09:34,042 --> 00:09:36,058
We won't be doing well on the cross-validation set.

269
00:09:38,004 --> 00:09:41,000
Whereas here on the left, this is the high-variance regime.

270
00:09:42,012 --> 00:09:43,062
Where if we have two smaller

271
00:09:44,001 --> 00:09:45,090
value of then we

272
00:09:46,007 --> 00:09:47,019
may be overfitting the data

273
00:09:47,087 --> 00:09:49,013
and so by over fitting the

274
00:09:49,023 --> 00:09:51,032
data then it a cross validation error

275
00:09:51,071 --> 00:09:52,061
will also be high.

276
00:09:53,070 --> 00:09:55,037
And so, this is what the

277
00:09:56,062 --> 00:09:58,026
cross-validation error and what

278
00:09:58,050 --> 00:09:59,086
the training error may look

279
00:10:00,012 --> 00:10:01,040
like on a training set

280
00:10:01,082 --> 00:10:04,026
as we vary the parameter

281
00:10:04,095 --> 00:10:06,091
lambda, as we vary the regularization parameter lambda.

282
00:10:07,011 --> 00:10:08,022
And so, once again, it will

283
00:10:08,042 --> 00:10:10,010
often be some intermediate value

284
00:10:10,078 --> 00:10:13,022
of lambda that you know, subsequent just right

285
00:10:13,072 --> 00:10:14,099
or that works best in

286
00:10:15,012 --> 00:10:16,047
terms of having a small

287
00:10:16,076 --> 00:10:19,071
cross-validation error or a small test set error.

288
00:10:19,091 --> 00:10:20,098
And whereas the curves I've drawn

289
00:10:21,029 --> 00:10:23,062
here are somewhat cartoonish and somewhat idealized.

290
00:10:24,064 --> 00:10:25,066
So on a real data set

291
00:10:26,021 --> 00:10:27,039
the pros you get may

292
00:10:27,050 --> 00:10:28,047
end up looking a little bit more

293
00:10:28,069 --> 00:10:30,058
messy and just a little bit more noisy than this.

294
00:10:31,053 --> 00:10:32,063
For some data sets you will

295
00:10:33,017 --> 00:10:34,045
really see these poor

296
00:10:34,074 --> 00:10:36,017
source of trends and

297
00:10:36,045 --> 00:10:37,034
by looking at the plot

298
00:10:37,089 --> 00:10:38,092
of the whole or cross validation

299
00:10:39,082 --> 00:10:41,046
error, you can either

300
00:10:41,060 --> 00:10:43,037
manually, automatically try to

301
00:10:43,067 --> 00:10:45,010
select a point that minimizes

302
00:10:45,054 --> 00:10:48,059
the cross-validation error and

303
00:10:48,087 --> 00:10:50,060
select the value of lambda corresponding

304
00:10:51,027 --> 00:10:52,077
to low cross-validation error.

305
00:10:53,055 --> 00:10:54,078
When I'm trying to pick the

306
00:10:54,091 --> 00:10:56,087
regularization parameter lambda

307
00:10:57,020 --> 00:10:59,029
for a learning algorithm, often I

308
00:10:59,041 --> 00:11:00,051
find that plotting a figure

309
00:11:00,079 --> 00:11:02,047
like this one showed here, helps

310
00:11:02,075 --> 00:11:04,051
me understand better what's going

311
00:11:04,077 --> 00:11:06,032
on and helps me verify that

312
00:11:06,087 --> 00:11:08,013
I am indeed picking a good

313
00:11:08,032 --> 00:11:09,066
value for the regularization parameter

314
00:11:10,051 --> 00:11:12,032
lambda. So hopefully that

315
00:11:12,051 --> 00:11:14,015
gives you more insight into regularization

316
00:11:15,064 --> 00:11:16,088
and it's effects on the bias

317
00:11:17,039 --> 00:11:18,047
and variance of the learning algorithm.

318
00:11:19,097 --> 00:11:21,050
By know you've seen bias and

319
00:11:21,066 --> 00:11:23,040
variance from a lot of different perspectives.

320
00:11:24,017 --> 00:11:25,047
And what I'd like to do

321
00:11:25,070 --> 00:11:27,000
in the next video is take

322
00:11:27,023 --> 00:11:28,011
a lot of the insights

323
00:11:28,027 --> 00:11:30,007
that we've gone through and build

324
00:11:30,032 --> 00:11:31,021
on them to put together

325
00:11:31,091 --> 00:11:33,076
a diagnostic that's called learning

326
00:11:34,004 --> 00:11:35,010
curves, which is a

327
00:11:35,014 --> 00:11:36,029
tool that I often use

328
00:11:36,072 --> 00:11:37,091
to try to diagnose if a

329
00:11:38,019 --> 00:11:39,062
learning algorithm may be suffering

330
00:11:40,003 --> 00:11:41,033
from a bias problem or a

331
00:11:41,055 --> 00:11:42,095
variance problem or a little bit of both.
