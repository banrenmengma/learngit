
1
00:00:00,009 --> 00:00:02,004
In this video, I'd like to tell you about learning curves.

2
00:00:03,031 --> 00:00:05,084
Learning curves is often a very useful thing to plot.

3
00:00:06,071 --> 00:00:08,016
If either you wanted to sanity check

4
00:00:08,042 --> 00:00:09,058
that your algorithm is working correctly,

5
00:00:10,040 --> 00:00:12,073
or if you want to improve the performance of the algorithm.

6
00:00:13,094 --> 00:00:15,019
And learning curves is a

7
00:00:15,031 --> 00:00:16,041
tool that I actually use

8
00:00:16,082 --> 00:00:17,092
very often to try to

9
00:00:18,028 --> 00:00:20,003
diagnose if a physical learning algorithm may be

10
00:00:20,017 --> 00:00:23,021
suffering from bias, sort of variance problem or a bit of both.

11
00:00:27,017 --> 00:00:28,007
Here's what a learning curve is.

12
00:00:28,082 --> 00:00:30,055
To plot a learning curve, what

13
00:00:30,069 --> 00:00:31,076
I usually do is plot

14
00:00:32,021 --> 00:00:33,095
j train which is, say,

15
00:00:35,003 --> 00:00:36,004
average squared error on my training

16
00:00:36,043 --> 00:00:39,009
set or Jcv which is

17
00:00:39,034 --> 00:00:41,013
the average squared error on my cross validation set.

18
00:00:41,059 --> 00:00:42,089
And I'm going to plot

19
00:00:43,014 --> 00:00:44,015
that as a function

20
00:00:44,050 --> 00:00:46,038
of m, that is as a function

21
00:00:47,022 --> 00:00:51,025
of the number of training examples I have.

22
00:00:51,095 --> 00:00:53,042
And so m is usually a constant like maybe I just have, you know, a 100

23
00:00:53,064 --> 00:00:55,021
training examples but what I'm

24
00:00:55,032 --> 00:00:57,067
going to do is artificially with

25
00:00:57,085 --> 00:00:59,028
use my training set exercise. So, I

26
00:00:59,050 --> 00:01:01,046
deliberately limit myself to using only,

27
00:01:01,084 --> 00:01:03,043
say, 10 or 20 or

28
00:01:03,065 --> 00:01:06,004
30 or 40 training examples and

29
00:01:06,017 --> 00:01:07,060
plot what the training error is and

30
00:01:07,073 --> 00:01:09,064
what the cross validation is for this

31
00:01:10,004 --> 00:01:12,026
smallest training set exercises.
So

32
00:01:12,062 --> 00:01:14,009
let's see what these plots may look

33
00:01:14,026 --> 00:01:15,053
like. Suppose I have only

34
00:01:15,073 --> 00:01:17,020
one training example like that

35
00:01:17,039 --> 00:01:18,045
shown in this this first example

36
00:01:18,085 --> 00:01:19,096
here and let's say I'm fitting a quadratic function. Well, I

37
00:01:20,025 --> 00:01:22,031


38
00:01:22,046 --> 00:01:24,048
have only one training example. I'm

39
00:01:25,004 --> 00:01:26,009
going to be able to fit it perfectly

40
00:01:26,065 --> 00:01:28,059
right? You know, just fit the quadratic function. I'm

41
00:01:28,076 --> 00:01:30,000
going to have 0

42
00:01:30,015 --> 00:01:32,023
error on the one training example. If I

43
00:01:32,056 --> 00:01:34,017
have two training examples. Well the quadratic function can also fit that very well. So,

44
00:01:34,023 --> 00:01:36,059


45
00:01:37,004 --> 00:01:38,054
even if I am using regularization,

46
00:01:38,075 --> 00:01:40,021
I can probably fit this quite well.

47
00:01:41,007 --> 00:01:41,096
And if I am using no neural regularization,

48
00:01:42,003 --> 00:01:45,020
I'm going to fit this perfectly and

49
00:01:45,043 --> 00:01:46,040
if I have three training examples

50
00:01:47,026 --> 00:01:48,037
again. Yeah, I can fit a quadratic

51
00:01:48,065 --> 00:01:51,031
function perfectly so if

52
00:01:51,054 --> 00:01:52,059
m equals 1 or m equals 2 or m equals 3,

53
00:01:52,062 --> 00:01:54,045


54
00:01:54,084 --> 00:01:56,076
my training error

55
00:01:57,034 --> 00:01:58,087
on my training set is

56
00:01:59,010 --> 00:02:01,018
going to be 0 assuming I'm

57
00:02:01,021 --> 00:02:02,076
not using regularization or it may

58
00:02:03,015 --> 00:02:04,029
slightly large in 0 if

59
00:02:04,056 --> 00:02:06,040
I'm using regularization and

60
00:02:06,050 --> 00:02:07,034
by the way if I have

61
00:02:07,073 --> 00:02:08,097
a large training set and I'm artificially

62
00:02:09,093 --> 00:02:11,003
restricting the size of my

63
00:02:11,012 --> 00:02:13,008
training set in order to J train.

64
00:02:13,083 --> 00:02:14,077
Here if I set

65
00:02:15,011 --> 00:02:16,071
M equals 3, say, and I

66
00:02:17,003 --> 00:02:18,028
train on only three examples,

67
00:02:19,027 --> 00:02:21,003
then, for this figure I

68
00:02:21,011 --> 00:02:22,043
am going to measure my training error

69
00:02:22,083 --> 00:02:24,044
only on the three examples that

70
00:02:24,055 --> 00:02:25,058
actually fit my data too

71
00:02:27,015 --> 00:02:28,012
and so even I have to say

72
00:02:28,028 --> 00:02:31,015
a 100 training examples but if I want to plot what my

73
00:02:31,043 --> 00:02:32,062
training error is the m equals 3. What I'm going to do

74
00:02:32,084 --> 00:02:34,000


75
00:02:34,027 --> 00:02:35,019
is to measure the

76
00:02:35,034 --> 00:02:36,065
training error on the

77
00:02:36,075 --> 00:02:39,087
three examples that I've actually fit to my hypothesis 2.

78
00:02:41,028 --> 00:02:42,090
And not all the other examples that I have

79
00:02:43,000 --> 00:02:44,093
deliberately omitted from the training

80
00:02:45,013 --> 00:02:46,075
process. So just to summarize what we've

81
00:02:46,096 --> 00:02:48,046
seen is that if the training set

82
00:02:48,081 --> 00:02:50,056
size is small then the

83
00:02:50,062 --> 00:02:52,062
training error is going to be small as well.

84
00:02:52,096 --> 00:02:53,090
Because you know, we have a

85
00:02:53,093 --> 00:02:55,015
small training set is

86
00:02:55,034 --> 00:02:56,078
going to be very easy to

87
00:02:56,090 --> 00:02:58,008
fit your training set

88
00:02:58,071 --> 00:02:59,049
very well may be even

89
00:02:59,078 --> 00:03:02,096
perfectly now say

90
00:03:03,018 --> 00:03:04,046
we have m equals 4 for example. Well then

91
00:03:04,068 --> 00:03:06,080
a quadratic function can be

92
00:03:06,091 --> 00:03:07,090
a longer fit this data set

93
00:03:08,009 --> 00:03:09,068
perfectly and if I

94
00:03:09,078 --> 00:03:11,034
have m equals 5 then you

95
00:03:11,046 --> 00:03:13,083
know, maybe quadratic function will fit to stay there so

96
00:03:14,009 --> 00:03:15,093
so, then as my training set gets larger.

97
00:03:16,097 --> 00:03:18,046
It becomes harder and harder to

98
00:03:18,062 --> 00:03:19,086
ensure that I can

99
00:03:20,006 --> 00:03:21,081
find the quadratic function that process through

100
00:03:21,096 --> 00:03:25,046
all my examples perfectly. So

101
00:03:25,084 --> 00:03:27,030
in fact as the training set size

102
00:03:27,068 --> 00:03:28,077
grows what you find

103
00:03:29,030 --> 00:03:30,096
is that my average training error

104
00:03:31,031 --> 00:03:33,008
actually increases and so if you plot

105
00:03:33,050 --> 00:03:34,065
this figure what you find

106
00:03:35,021 --> 00:03:36,086
is that the training set

107
00:03:37,012 --> 00:03:38,052
error that is the average

108
00:03:38,093 --> 00:03:40,065
error on your hypothesis grows

109
00:03:41,030 --> 00:03:44,072
as m grows and just to repeat when the intuition is that when

110
00:03:45,002 --> 00:03:46,019
m is small when you have very

111
00:03:46,050 --> 00:03:48,006
few training examples. It's pretty

112
00:03:48,034 --> 00:03:49,041
easy to fit every single

113
00:03:49,078 --> 00:03:51,034
one of your training examples perfectly and

114
00:03:51,061 --> 00:03:52,084
so your error is going

115
00:03:52,093 --> 00:03:54,053
to be small whereas

116
00:03:54,071 --> 00:03:56,009
when m is larger then gets

117
00:03:56,046 --> 00:03:57,090
harder all the training

118
00:03:58,021 --> 00:03:59,090
examples perfectly and so

119
00:04:00,043 --> 00:04:01,083
your training set error becomes

120
00:04:02,037 --> 00:04:05,084
more larger now, how about the cross validation error.

121
00:04:06,071 --> 00:04:08,046
Well, the cross validation is

122
00:04:08,059 --> 00:04:10,009
my error on this cross

123
00:04:10,034 --> 00:04:12,065
validation set that I haven't seen and

124
00:04:12,087 --> 00:04:14,059
so, you know, when I have

125
00:04:14,071 --> 00:04:15,090
a very small training set, I'm

126
00:04:16,007 --> 00:04:16,088
not going to generalize well, just

127
00:04:17,001 --> 00:04:19,061
not going to do well on that.

128
00:04:19,085 --> 00:04:21,022
So, right, this hypothesis here doesn't

129
00:04:21,062 --> 00:04:22,072
look like a good one, and

130
00:04:23,001 --> 00:04:23,097
it's only when I get

131
00:04:24,005 --> 00:04:25,026
a larger training set that,

132
00:04:25,050 --> 00:04:26,037
you know, I'm starting to get

133
00:04:26,088 --> 00:04:28,010
hypotheses that maybe fit

134
00:04:28,048 --> 00:04:30,081
the data somewhat better.

135
00:04:31,037 --> 00:04:32,005
So your cross validation error and

136
00:04:32,025 --> 00:04:35,064
your test set error will tend

137
00:04:35,088 --> 00:04:37,016
to decrease as your training

138
00:04:37,047 --> 00:04:39,014
set size increases because the

139
00:04:39,025 --> 00:04:40,069
more data you have, the better

140
00:04:40,099 --> 00:04:43,041
you do at generalizing to new examples.

141
00:04:44,000 --> 00:04:46,073
So, just the more data you have, the better the hypothesis you fit.

142
00:04:47,056 --> 00:04:48,056
So if you plot j train,

143
00:04:49,042 --> 00:04:51,067
and Jcv this is the sort of thing that you get.

144
00:04:52,049 --> 00:04:53,055
Now let's look at what

145
00:04:53,076 --> 00:04:54,093
the learning curves may look like

146
00:04:55,036 --> 00:04:56,055
if we have either high

147
00:04:56,093 --> 00:04:58,020
bias or high variance problems.

148
00:04:58,092 --> 00:05:00,052
Suppose your hypothesis has high

149
00:05:00,082 --> 00:05:02,014
bias and to explain this

150
00:05:02,037 --> 00:05:03,077
I'm going to use a, set an

151
00:05:03,093 --> 00:05:05,025
example, of fitting a straight

152
00:05:05,043 --> 00:05:06,050
line to data that, you

153
00:05:06,076 --> 00:05:08,024
know, can't really be fit well by a straight line.

154
00:05:09,054 --> 00:05:12,032
So we end up with a hypotheses that maybe looks like that.

155
00:05:13,091 --> 00:05:15,044
Now let's think what would

156
00:05:15,075 --> 00:05:16,083
happen if we were to increase

157
00:05:17,047 --> 00:05:18,087
the training set size. So if

158
00:05:19,016 --> 00:05:20,048
instead of five examples like

159
00:05:20,058 --> 00:05:22,039
what I've drawn there, imagine that

160
00:05:22,056 --> 00:05:24,007
we have a lot more training examples.

161
00:05:25,027 --> 00:05:27,023
Well what happens, if you fit a straight line to this.

162
00:05:27,098 --> 00:05:29,069
What you find is that, you

163
00:05:30,004 --> 00:05:31,036
end up with you know, pretty much the same straight line.

164
00:05:31,068 --> 00:05:32,093
I mean a straight line that

165
00:05:33,052 --> 00:05:35,011
just cannot fit this

166
00:05:35,026 --> 00:05:37,031
data and getting a ton more data, well

167
00:05:37,088 --> 00:05:39,045
the straight line isn't going to change that much.

168
00:05:40,023 --> 00:05:41,039
This is the best possible straight-line

169
00:05:41,083 --> 00:05:42,076
fit to this data, but the

170
00:05:42,088 --> 00:05:44,016
straight line just can't fit this

171
00:05:44,031 --> 00:05:45,062
data set that well. So,

172
00:05:45,087 --> 00:05:47,042
if you plot across validation error,

173
00:05:49,025 --> 00:05:50,017
this is what it will look like.

174
00:05:51,031 --> 00:05:54,047
Option on the left, if you have already a miniscule training set size like you know,

175
00:05:55,041 --> 00:05:57,070
maybe just one training example and is not going to do well.

176
00:05:58,055 --> 00:05:59,047
But by the time you have

177
00:05:59,066 --> 00:06:00,075
reached a certain number of training

178
00:06:00,093 --> 00:06:02,035
examples, you have almost

179
00:06:02,081 --> 00:06:04,000
fit the best possible straight

180
00:06:04,019 --> 00:06:05,039
line, and even if

181
00:06:05,049 --> 00:06:06,025
you end up with a much

182
00:06:06,048 --> 00:06:07,079
larger training set size, a

183
00:06:07,097 --> 00:06:09,017
much larger value of m,

184
00:06:10,000 --> 00:06:12,004
you know, you're basically getting the same straight line,

185
00:06:12,037 --> 00:06:14,018
and so, the cross-validation error

186
00:06:14,048 --> 00:06:15,042
- let me label that -

187
00:06:15,064 --> 00:06:17,004
or test set error or

188
00:06:17,013 --> 00:06:18,066
plateau out, or flatten out

189
00:06:18,099 --> 00:06:20,048
pretty soon, once you reached

190
00:06:20,091 --> 00:06:22,092
beyond a certain the number

191
00:06:23,026 --> 00:06:24,069
of training examples, unless you

192
00:06:25,012 --> 00:06:27,048
pretty much fit the best possible straight line.

193
00:06:28,038 --> 00:06:29,054
And how about training error?

194
00:06:30,012 --> 00:06:33,005
Well, the training error will again be small.

195
00:06:34,062 --> 00:06:36,027
And what you find

196
00:06:36,075 --> 00:06:38,007
in the high bias case is

197
00:06:38,020 --> 00:06:40,076
that the training error will end

198
00:06:41,000 --> 00:06:42,050
up close to the cross

199
00:06:42,082 --> 00:06:44,069
validation error, because you

200
00:06:44,081 --> 00:06:46,037
have so few parameters and so

201
00:06:46,058 --> 00:06:48,006
much data, at least when m is large.

202
00:06:48,089 --> 00:06:49,083
The performance on the training

203
00:06:50,022 --> 00:06:52,050
set and the cross validation set will be very similar.

204
00:06:53,080 --> 00:06:54,075
And so, this is what your

205
00:06:54,087 --> 00:06:56,045
learning curves will look like,

206
00:06:56,076 --> 00:06:58,085
if you have an algorithm that has high bias.

207
00:07:00,022 --> 00:07:01,047
And finally, the problem with

208
00:07:01,062 --> 00:07:03,025
high bias is reflected in

209
00:07:03,044 --> 00:07:04,093
the fact that both the

210
00:07:05,057 --> 00:07:07,035
cross validation error and the

211
00:07:07,042 --> 00:07:09,012
training error are high,

212
00:07:09,056 --> 00:07:10,043
and so you end up with

213
00:07:10,064 --> 00:07:12,004
a relatively high value of

214
00:07:12,027 --> 00:07:14,025
both Jcv and the j train.

215
00:07:15,037 --> 00:07:16,081
This also implies something very

216
00:07:17,012 --> 00:07:18,051
interesting, which is that,

217
00:07:18,080 --> 00:07:19,099
if a learning algorithm has high

218
00:07:20,036 --> 00:07:22,025
bias, as we

219
00:07:22,038 --> 00:07:23,043
get more and more training examples,

220
00:07:24,006 --> 00:07:25,010
that is, as we move to

221
00:07:25,020 --> 00:07:26,060
the right of this figure, we'll

222
00:07:26,074 --> 00:07:27,087
notice that the cross

223
00:07:28,022 --> 00:07:29,043
validation error isn't going

224
00:07:29,074 --> 00:07:31,001
down much, it's basically fattened

225
00:07:31,056 --> 00:07:32,081
up, and so if

226
00:07:32,094 --> 00:07:35,001
learning algorithms are really suffering from high bias.

227
00:07:36,063 --> 00:07:38,019
Getting more training data by

228
00:07:38,037 --> 00:07:39,070
itself will actually not help

229
00:07:40,018 --> 00:07:41,057
that much,and as our figure

230
00:07:41,075 --> 00:07:43,012
example in the figure

231
00:07:43,020 --> 00:07:45,067
on the right, here we had only five training.

232
00:07:46,006 --> 00:07:47,097
examples, and we fill certain straight line.

233
00:07:48,055 --> 00:07:49,026
And when we had a ton

234
00:07:49,054 --> 00:07:50,073
more training data, we still

235
00:07:51,004 --> 00:07:52,070
end up with roughly the same straight line.

236
00:07:53,019 --> 00:07:54,029
And so if the learning algorithm

237
00:07:54,043 --> 00:07:57,008
has high bias give me a lot more training data.

238
00:07:57,064 --> 00:07:59,006
That doesn't actually help you

239
00:07:59,082 --> 00:08:01,029
get a much lower cross validation

240
00:08:01,088 --> 00:08:02,088
error or test set error.

241
00:08:03,073 --> 00:08:04,094
So knowing if your learning

242
00:08:05,025 --> 00:08:06,060
algorithm is suffering from high

243
00:08:06,077 --> 00:08:07,062
bias seems like a useful

244
00:08:08,010 --> 00:08:09,050
thing to know because this can

245
00:08:09,063 --> 00:08:11,013
prevent you from wasting a

246
00:08:11,029 --> 00:08:12,051
lot of time collecting more training

247
00:08:12,092 --> 00:08:15,043
data where it might just not end up being helpful.

248
00:08:16,019 --> 00:08:17,006
Next let us look at the

249
00:08:17,013 --> 00:08:18,052
setting of a learning algorithm

250
00:08:19,047 --> 00:08:20,033
that may have high variance.

251
00:08:21,058 --> 00:08:22,087
Let us just look at the

252
00:08:23,055 --> 00:08:24,025
training error in a around if

253
00:08:25,012 --> 00:08:26,035
you have very smart training

254
00:08:26,068 --> 00:08:28,073
set like five training examples shown on

255
00:08:29,012 --> 00:08:30,072
the figure on the right and

256
00:08:31,014 --> 00:08:32,016
if we're fitting say a

257
00:08:32,020 --> 00:08:33,004
very high order polynomial,

258
00:08:34,037 --> 00:08:36,052
and I've written a hundredth degree polynomial which

259
00:08:37,009 --> 00:08:38,075
really no one uses, but just an illustration.

260
00:08:39,091 --> 00:08:41,046
And if we're using a

261
00:08:41,054 --> 00:08:43,015
fairly small value of lambda,

262
00:08:43,079 --> 00:08:44,091
maybe not zero, but a fairly

263
00:08:45,007 --> 00:08:46,083
small value of lambda, then

264
00:08:47,003 --> 00:08:47,098
we'll end up, you know,

265
00:08:48,019 --> 00:08:50,059
fitting this data very well that with

266
00:08:50,086 --> 00:08:53,038
a function that overfits this.

267
00:08:54,037 --> 00:08:55,063
So, if the training

268
00:08:55,099 --> 00:08:57,082
set size is small, our training

269
00:08:58,032 --> 00:08:59,052
error, that is, j train

270
00:09:00,002 --> 00:09:01,080
of theta will be small.

271
00:09:03,012 --> 00:09:04,033
And as this training set size increases

272
00:09:04,094 --> 00:09:05,087
a bit, you know, we may

273
00:09:06,000 --> 00:09:07,015
still be overfitting this

274
00:09:07,033 --> 00:09:08,080
data a little bit but

275
00:09:09,077 --> 00:09:11,087
it also becomes slightly harder to

276
00:09:12,001 --> 00:09:12,097
fit this data set perfectly,

277
00:09:13,094 --> 00:09:15,013
and so, as the training set size

278
00:09:15,035 --> 00:09:16,080
increases, we'll find that

279
00:09:16,096 --> 00:09:19,038
j train increases, because

280
00:09:19,084 --> 00:09:21,003
it is just a little harder to fit

281
00:09:21,025 --> 00:09:22,072
the training set perfectly when we have

282
00:09:22,088 --> 00:09:25,070
more examples, but the training set error will still be pretty low.

283
00:09:26,052 --> 00:09:28,060
Now, how about the cross validation error?

284
00:09:29,022 --> 00:09:30,059
Well, in high variance

285
00:09:31,003 --> 00:09:32,075
setting, a hypothesis is

286
00:09:32,098 --> 00:09:34,019
overfitting and so the

287
00:09:34,028 --> 00:09:35,067
cross validation error will remain

288
00:09:36,012 --> 00:09:37,064
high, even as we

289
00:09:37,075 --> 00:09:38,092
get you know, a moderate number

290
00:09:39,025 --> 00:09:40,051
of training examples and, so

291
00:09:41,016 --> 00:09:42,095
maybe, the cross validation

292
00:09:43,073 --> 00:09:45,051
error may look like that.

293
00:09:45,065 --> 00:09:47,072
And the indicative diagnostic that we

294
00:09:47,083 --> 00:09:49,020
have a high variance problem,

295
00:09:50,021 --> 00:09:51,049
is the fact that there's

296
00:09:51,072 --> 00:09:54,000
this large gap between

297
00:09:54,034 --> 00:09:56,044
the training error and the cross validation error.

298
00:09:57,044 --> 00:09:58,017
And looking at this figure.

299
00:09:58,072 --> 00:10:00,016
If we think about adding more

300
00:10:00,044 --> 00:10:01,080
training data, that is, taking

301
00:10:02,011 --> 00:10:03,065
this figure and extrapolating to

302
00:10:03,078 --> 00:10:05,022
the right, we can kind

303
00:10:05,033 --> 00:10:06,083
of tell that, you know the

304
00:10:07,002 --> 00:10:08,012
two curves, the blue curve

305
00:10:08,048 --> 00:10:10,048
and the magenta curve, are converging to each other.

306
00:10:11,041 --> 00:10:12,036
And so, if we were to

307
00:10:12,051 --> 00:10:13,084
extrapolate this figure to

308
00:10:13,098 --> 00:10:21,023
the right, then it

309
00:10:21,036 --> 00:10:23,000
seems it likely that the

310
00:10:23,016 --> 00:10:24,012
training error will keep on

311
00:10:24,026 --> 00:10:25,074
going up and the

312
00:10:27,012 --> 00:10:29,003
cross-validation error would keep on going down.

313
00:10:30,000 --> 00:10:32,034
And the thing we really care about is the cross-validation error

314
00:10:33,000 --> 00:10:35,014
or the test set error, right?

315
00:10:35,029 --> 00:10:36,046
So in this sort

316
00:10:36,073 --> 00:10:37,085
of figure, we can tell that

317
00:10:38,023 --> 00:10:39,041
if we keep on adding training

318
00:10:39,082 --> 00:10:40,092
examples and extrapolate to the

319
00:10:41,004 --> 00:10:42,064
right, well our cross validation

320
00:10:43,028 --> 00:10:44,061
error will keep on coming down.

321
00:10:45,012 --> 00:10:46,009
And, so, in the high

322
00:10:46,033 --> 00:10:47,098
variance setting, getting more

323
00:10:48,017 --> 00:10:49,054
training data is, indeed,

324
00:10:50,016 --> 00:10:51,024
likely to help.

325
00:10:51,051 --> 00:10:52,080
And so again, this seems like a

326
00:10:53,005 --> 00:10:54,017
useful thing to know if your

327
00:10:54,033 --> 00:10:55,083
learning algorithm is suffering

328
00:10:56,014 --> 00:10:57,046
from a high variance problem, because

329
00:10:57,080 --> 00:10:59,014
that tells you, for example that it

330
00:10:59,022 --> 00:11:00,010
may be be worth your while

331
00:11:00,067 --> 00:11:02,042
to see if you can go and get some more training data.

332
00:11:03,070 --> 00:11:04,091
Now, on the previous slide

333
00:11:05,033 --> 00:11:06,045
and this slide, I've drawn fairly

334
00:11:06,097 --> 00:11:08,050
clean fairly idealized curves.

335
00:11:08,089 --> 00:11:10,004
If you plot these curves for

336
00:11:10,016 --> 00:11:11,097
an actual learning algorithm, sometimes

337
00:11:12,050 --> 00:11:13,090
you will actually see, you know, pretty

338
00:11:14,055 --> 00:11:15,089
much curves, like what I've drawn here.

339
00:11:16,060 --> 00:11:17,073
Although, sometimes you see curves

340
00:11:18,014 --> 00:11:19,015
that are a little bit noisier and

341
00:11:19,023 --> 00:11:20,082
a little bit messier than this.

342
00:11:21,009 --> 00:11:22,044
But plotting learning curves like

343
00:11:22,062 --> 00:11:23,085
these can often tell

344
00:11:24,012 --> 00:11:25,046
you, can often help you

345
00:11:25,057 --> 00:11:26,064
figure out if your learning algorithm is

346
00:11:26,095 --> 00:11:29,008
suffering from bias, or variance or even a little bit of both.

347
00:11:29,016 --> 00:11:31,002
So when I'm

348
00:11:31,020 --> 00:11:32,070
trying to improve the performance of

349
00:11:32,075 --> 00:11:34,005
a learning algorithm, one thing

350
00:11:34,025 --> 00:11:35,072
that I'll almost always do

351
00:11:35,096 --> 00:11:37,044
is plot these learning

352
00:11:37,097 --> 00:11:39,046
curves, and usually this will

353
00:11:39,049 --> 00:11:41,071
give you a better sense of whether there is a bias or variance problem.

354
00:11:44,027 --> 00:11:45,017
And in the next video

355
00:11:45,041 --> 00:11:46,044
we'll see how this can

356
00:11:46,064 --> 00:11:48,037
help suggest specific actions is

357
00:11:48,045 --> 00:11:49,058
to take, or to not take,

358
00:11:50,025 --> 00:11:53,025
in order to try to improve the performance of your learning algorithm.
