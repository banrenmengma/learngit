
1
00:00:00,000 --> 00:00:04,067
Our first learning algorithm will be
linear regression. In this video, you'll see

2
00:00:04,067 --> 00:00:09,023
what the model looks like and more
importantly you'll see what the overall

3
00:00:09,023 --> 00:00:14,080
process of supervised learning looks like. Let's
use some motivating example of predicting

4
00:00:14,080 --> 00:00:20,003
housing prices. We're going to use a data
set of housing prices from the city of

5
00:00:20,003 --> 00:00:25,020
Portland, Oregon. And here we're gonna
plot my data set of a number of houses

6
00:00:25,020 --> 00:00:30,083
that were different sizes that were sold
for a range of different prices. Let's say

7
00:00:30,083 --> 00:00:35,087
that given this data set, you have a
friend that's trying to sell a house and

8
00:00:35,087 --> 00:00:41,023
let's see if friend's house is size of
1250 square feet and you want to tell them

9
00:00:41,023 --> 00:00:46,045
how much they might be able to sell the
house for. Well one thing you could do is

10
00:00:46,064 --> 00:00:53,003
fit a model. Maybe fit a straight line
to this data. Something like that and, based

11
00:00:53,003 --> 00:00:59,016
on that, maybe you could tell your friend
that let's say maybe he can sell the

12
00:00:59,016 --> 00:01:03,057
house for around $220,000.
So this is an example of a

13
00:01:03,057 --> 00:01:08,083
supervised learning algorithm. And it's
supervised learning because we're given

14
00:01:08,083 --> 00:01:14,028
the "right answer" for each of
our examples. Namely we're told what was

15
00:01:14,028 --> 00:01:19,035
the actual house, what was the actual
price of each of the houses in our data

16
00:01:19,035 --> 00:01:24,044
set were sold for and moreover, this is
an example of a regression problem where

17
00:01:24,044 --> 00:01:29,054
the term regression refers to the fact
that we are predicting a real-valued output

18
00:01:29,054 --> 00:01:34,058
namely the price. And just to remind you
the other most common type of supervised

19
00:01:34,058 --> 00:01:39,000
learning problem is called the
classification problem where we predict

20
00:01:39,000 --> 00:01:45,020
discrete-valued outputs such as if we are
looking at cancer tumors and trying to

21
00:01:45,020 --> 00:01:52,003
decide if a tumor is malignant or benign.
So that's a zero one value [inaudible]. More

22
00:01:52,003 --> 00:01:57,008
formally, in supervised learning, we have
a data set and this data set is called a

23
00:01:57,008 --> 00:02:02,001
training set. So for housing prices
example, we have a training set of

24
00:02:02,001 --> 00:02:07,038
different housing prices and our job is to
learn from this data how to predict prices

25
00:02:07,038 --> 00:02:11,090
of the houses. Let's define some notation
that we're using throughout this course.

26
00:02:11,090 --> 00:02:16,009
We're going to define quite a lot of
symbols. It's okay if you don't remember

27
00:02:16,009 --> 00:02:20,007
all the symbols right now but as the
course progresses it will be useful

28
00:02:20,007 --> 00:02:24,026
[inaudible] notation. So I'm gonna use
lower case m throughout this course to

29
00:02:24,026 --> 00:02:28,089
denote the number of training examples. So
in this data set, if I have, you know,

30
00:02:28,089 --> 00:02:34,036
let's say 47 rows in this table. Then I
have 47 training examples and m equals 47.

31
00:02:34,036 --> 00:02:39,049
Let me use lowercase x  to denote the
input variables often also called the

32
00:02:39,049 --> 00:02:44,028
features. Without the x's here, we
[inaudible] input features and I'm gonna

33
00:02:44,028 --> 00:02:49,055
use y to denote my output variables or the
target variable which I'm going to

34
00:02:49,055 --> 00:02:54,055
predict and so that's the second
column here. [inaudible] notation, I'm

35
00:02:54,055 --> 00:03:05,074
going to use (x, y) to denote a single
training example. So, a single row in this

36
00:03:05,074 --> 00:03:12,006
table corresponds to a single training
example and to refer to a specific

37
00:03:12,006 --> 00:03:19,070
training example, I'm going to use this
notation x(i) comma gives me y(i) And, we're

38
00:03:25,032 --> 00:03:30,093
going to use this to refer to the ith
training example. So this superscript i

39
00:03:30,093 --> 00:03:37,086
over here, this is not exponentiation
right? This (x(i), y(i)), the superscript i in

40
00:03:37,086 --> 00:03:44,087
parenthesis that's just an index into my
training set and refers to the ith row in

41
00:03:44,087 --> 00:03:51,062
this table, okay? So this is not x to
the power of i, y to the power of i. Instead

42
00:03:51,062 --> 00:03:58,021
(x(i), y(i)) just refers to the ith row of this
table. So for example, x(1) refers to the

43
00:03:58,021 --> 00:04:04,097
input value for the first training example so
that's 2104. That's this x in the first

44
00:04:04,097 --> 00:04:11,068
row. x(2) will be equal to
1416 right? That's the second

45
00:04:11,068 --> 00:04:17,038
x and y(1) will be equal to 460.
The first, the y value for my first

46
00:04:17,038 --> 00:04:23,015
training example, that's what that (1)
refers to. So as mentioned, occasionally I'll ask you a

47
00:04:23,015 --> 00:04:28,034
question to let you check your
understanding and a few seconds in this

48
00:04:28,034 --> 00:04:34,004
video a multiple-choice question
will pop up in the video. When it does,

49
00:04:34,004 --> 00:04:40,036
please use your mouse to select what you
think is the right answer. What defined by

50
00:04:40,036 --> 00:04:45,012
the training set is. So here's how I
supervised the learning algorithm works.

51
00:04:45,012 --> 00:04:50,051
We saw that with the training set like our
training set of housing prices and we feed

52
00:04:50,051 --> 00:04:55,071
that to our learning algorithm. Is the job
of a learning algorithm to then output a

53
00:04:55,071 --> 00:05:00,010
function which by convention is
denoted lowercase h and h

54
00:05:00,010 --> 00:05:06,057
stands for hypothesis And what the job of
the hypothesis is, is, is a function that

55
00:05:06,057 --> 00:05:12,047
takes as input the size of a house like
maybe the size of the new house your friend's

56
00:05:12,047 --> 00:05:18,036
trying to sell so it takes in the value of
x and it tries to output the estimated

57
00:05:18,036 --> 00:05:31,062
value of y for the corresponding house.
So h is a function that maps from x's

58
00:05:31,062 --> 00:05:37,072
to y's. People often ask me, you
know, why is this function called

59
00:05:37,072 --> 00:05:42,012
hypothesis. Some of you may know the
meaning of the term hypothesis, from the

60
00:05:42,012 --> 00:05:46,074
dictionary or from science or whatever. It
turns out that in machine learning, this

61
00:05:46,074 --> 00:05:51,031
is a name that was used in the early days of
machine learning and it kinda stuck. 'Cause

62
00:05:51,031 --> 00:05:55,023
maybe not a great name for this sort of
function, for mapping from sizes of

63
00:05:55,023 --> 00:05:59,097
houses to the predictions, that you know....
I think the term hypothesis, maybe isn't

64
00:05:59,097 --> 00:06:04,054
the best possible name, but this is the
standard terminology that people use in

65
00:06:04,054 --> 00:06:09,036
machine learning. So don't worry too much
why people call it that. When

66
00:06:09,036 --> 00:06:14,033
designing a learning algorithm, the next
thing we need to decide is how do we

67
00:06:14,033 --> 00:06:20,054
represent this hypothesis h. For this and
the next few videos, I'm going to choose

68
00:06:20,054 --> 00:06:26,097
our initial choice , for representing the
hypothesis, will be the following. We're going to

69
00:06:26,097 --> 00:06:33,000
represent h as follows. And we will write this as
h<u>theta(x) equals theta<u>0</u></u>

70
00:06:33,000 --> 00:06:39,025
plus theta<u>1 of x. And as a shorthand,
sometimes instead of writing, you</u>

71
00:06:39,025 --> 00:06:45,044
know, h<u>theta(x), sometimes
there's a shorthand, which is written h(x).</u>

72
00:06:45,044 --> 00:06:51,062
But more often I'll write it as a
subscript theta over there. And plotting

73
00:06:51,062 --> 00:06:58,020
to some pictures, all this means is that,
we are going to predict that y is a linear

74
00:06:58,020 --> 00:07:04,063
function of x. Right, so that's the
data set and what this function is doing,

75
00:07:04,063 --> 00:07:11,069
is predicting that y is some straight
line function of x. h(x) equals theta<u>0</u>

76
00:07:11,069 --> 00:07:18,044
plus theta<u>1 x, okay? And why a linear
function? Well, sometimes we'll want to</u>

77
00:07:18,044 --> 00:07:23,040
fit more complicated, perhaps non-linear
functions as well. But since this linear

78
00:07:23,040 --> 00:07:28,029
case is the simple building block, we will
start with this example first of fitting

79
00:07:28,029 --> 00:07:32,094
linear functions, and we will build on
this to eventually have more complex

80
00:07:32,094 --> 00:07:37,040
models, and more complex learning
algorithms. Let me also give this

81
00:07:37,040 --> 00:07:42,062
particular model a name. This model is
called linear regression or, this for

82
00:07:42,062 --> 00:07:48,027
example is, actually linear regression
with one variable, with the variable being

83
00:07:48,027 --> 00:07:53,091
x. Predicting all the prices as functions
of one variable X. And another name for

84
00:07:53,091 --> 00:07:58,085
this model is univariate linear
regression. And univariate is just a

85
00:07:58,085 --> 00:08:04,039
fancy way of saying one variable. So,
that's linear regression. In the next

86
00:08:04,039 --> 00:08:09,075
video we'll start to talk about just how
we go about implementing this model.
